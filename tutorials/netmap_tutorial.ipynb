{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45190821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data_nfs/og86asub/netmap/netmap-evaluation/')\n",
    "\n",
    "import scanpy as sc\n",
    "import time \n",
    "\n",
    "from netmap.src.utils.misc import write_config\n",
    "\n",
    "from netmap.src.model.negbinautoencoder import *\n",
    "import scanpy as sc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from captum.attr import GradientShap, LRP\n",
    "from netmap.src.model.inferrence_simple import *\n",
    "from netmap.src.utils.data_utils import attribution_to_anndata\n",
    "from netmap.src.model.pipeline import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "import os.path as op\n",
    "import os\n",
    "\n",
    "import anndata as ad\n",
    "from statsmodels.stats.nonparametric import rank_compare_2indep\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as scs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from captum.attr import *\n",
    "import pingouin as pingu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def create_model_zoo(data_tensor, n_models = 4, n_epochs = 500):\n",
    "    model_zoo = []\n",
    "    for _ in range(n_models):\n",
    "\n",
    "        data_train2, data_test2 = train_test_split(data_tensor,test_size=0.01, shuffle=True)\n",
    "\n",
    "        trained_model2 = NegativeBinomialAutoencoder(input_dim=data_tensor.shape[1], latent_dim=10, dropout_rate = 0.02)\n",
    "        trained_model2 = trained_model2.cuda()\n",
    "\n",
    "        optimizer2 = torch.optim.Adam(trained_model2.parameters(), lr=1e-4)\n",
    "\n",
    "        trained_model2 = train_autoencoder(\n",
    "                trained_model2,\n",
    "                data_train2.cuda(),\n",
    "                optimizer2,\n",
    "                num_epochs=n_epochs\n",
    "\n",
    "            )\n",
    "        model_zoo.append(trained_model2)\n",
    "    return model_zoo\n",
    "\n",
    "\n",
    "def create_model_zoo(data_tensor, n_models = 4, n_epochs = 500):\n",
    "    model_zoo = []\n",
    "    for _ in range(n_models):\n",
    "\n",
    "        data_train2, data_test2 = train_test_split(data_tensor,test_size=0.01, shuffle=True)\n",
    "\n",
    "        trained_model2 = ZINBAutoencoder(input_dim=data_tensor.shape[1], latent_dim=10, dropout_rate = 0.02)\n",
    "        trained_model2 = trained_model2.cuda()\n",
    "\n",
    "        optimizer2 = torch.optim.Adam(trained_model2.parameters(), lr=1e-4)\n",
    "\n",
    "        trained_model2 = train_autoencoder(\n",
    "                trained_model2,\n",
    "                data_train2.cuda(),\n",
    "                optimizer2,\n",
    "                num_epochs=n_epochs\n",
    "\n",
    "            )\n",
    "        model_zoo.append(trained_model2)\n",
    "    return model_zoo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_latent_true(model_zoo):\n",
    "    for mo in model_zoo:\n",
    "        mo.forward_mu_only = False\n",
    "        mo.forward_theta_only = False\n",
    "        mo.latent_only = True\n",
    "    return model_zoo\n",
    "\n",
    "\n",
    "def set_all_false(model_zoo):\n",
    "    for mo in model_zoo:\n",
    "        mo.forward_mu_only = False\n",
    "        mo.forward_theta_only = False\n",
    "        mo.latent_only = False\n",
    "    return model_zoo\n",
    "\n",
    "def shuffle_each_column_independently(tensor):\n",
    "    \"\"\"\n",
    "    Shuffles each column of a 2D PyTorch tensor independently.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): The input tensor.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A new tensor with each of its columns independently shuffled.\n",
    "    \"\"\"\n",
    "    if tensor.dim() != 2:\n",
    "        raise ValueError(\"Input tensor must be 2-dimensional to shuffle columns.\")\n",
    "\n",
    "    # Create an empty tensor of the same size to store the shuffled columns\n",
    "    shuffled_tensor = torch.empty_like(tensor)\n",
    "\n",
    "    # Iterate through each column, shuffle it, and place it in the new tensor\n",
    "    for i in range(tensor.size(1)):\n",
    "        column = tensor[:, i]\n",
    "        idx = torch.randperm(column.nelement())\n",
    "        shuffled_tensor[:, i] = column[idx]\n",
    "\n",
    "    return shuffled_tensor\n",
    "\n",
    "\n",
    "def attribution_one_target( \n",
    "        target_gene,\n",
    "        lrp_model,\n",
    "        input_data,\n",
    "        background,\n",
    "        xai_type='lrp-like',\n",
    "        randomize_background = False):\n",
    "    \n",
    "    attributions_list = []\n",
    "    for m in range(len(lrp_model)):\n",
    "        # Randomize backgorund for each round\n",
    "        if randomize_background:\n",
    "            background = shuffle_each_column_independently(background)\n",
    "\n",
    "        model = lrp_model[m]\n",
    "        #for _ in range(num_iterations):\n",
    "        if xai_type == 'lrp-like':\n",
    "            #print(input_data)\n",
    "            #print(target_gene)\n",
    "            attribution = model.attribute(input_data, target=target_gene)\n",
    "                \n",
    "        elif xai_type == 'shap-like':\n",
    "            attribution = model.attribute(input_data, baselines = background, target = target_gene)\n",
    "\n",
    "        attributions_list.append(attribution.detach().cpu().numpy())\n",
    "    return attributions_list\n",
    "\n",
    "def get_differential_edges(attribution_anndata, percentile = 10):\n",
    "    genelist = []\n",
    "    if len(np.unique(attribution_anndata.obs['leiden']))>1 :\n",
    "        for cat in np.unique(attribution_anndata.obs['leiden']):\n",
    "            statisi =rank_compare_2indep(x1=attribution_anndata.X[attribution_anndata.obs['leiden']==cat], x2= attribution_anndata.X[attribution_anndata.obs['leiden']!=cat])\n",
    "            sig_and_high = np.where((statisi.pvalue<(0.01/(attribution_anndata.X.shape[1]*attribution_anndata.X.shape[1])))  & (statisi.prob1>= 0.9))\n",
    "            genelist = genelist+ list(sig_and_high[0])\n",
    "\n",
    "    else:\n",
    "        # FALLBACk\n",
    "        m = np.abs(attribution_anndata.X).mean(axis=0)\n",
    "        # Get the indices of genes in the top 10%\n",
    "        top_10_percent_indices = np.where(m > np.percentile(m, 100-percentile))[0]\n",
    "\n",
    "        # Get the indices of genes in the bottom 10%\n",
    "        bottom_10_percent_indices = np.where(m < np.percentile(m, percentile))[0]\n",
    "\n",
    "        # Combine the two arrays of indices and sort them\n",
    "        genelist = np.unique(np.sort(\n",
    "            np.concatenate((top_10_percent_indices, bottom_10_percent_indices))\n",
    "        ))\n",
    "    return genelist\n",
    "\n",
    "def get_percentile_edges(attribution_anndata, percentile = 10):\n",
    "    # FALLBACk\n",
    "    m = attribution_anndata.X.mean(axis=0)\n",
    "    # Get the indices of genes in the top 10%\n",
    "    top_10_percent_indices = np.where(m > np.percentile(m, 100-percentile))[0]\n",
    "\n",
    "    # Get the indices of genes in the bottom 10%\n",
    "    bottom_10_percent_indices = np.where(m < np.percentile(m, percentile))[0]\n",
    "\n",
    "    # Combine the two arrays of indices and sort them\n",
    "    genelist = np.unique(np.sort(\n",
    "        np.concatenate((top_10_percent_indices, bottom_10_percent_indices))\n",
    "    ))\n",
    "    return genelist\n",
    "\n",
    "def get_edges(attribution_anndata, use_differential=False, percentile = 10):\n",
    "    if use_differential:\n",
    "        return get_differential_edges(attribution_anndata, percentile=percentile)\n",
    "    else:\n",
    "        return get_percentile_edges(attribution_anndata, percentile=percentile)\n",
    "    \n",
    "def get_explainer(model, explainer_type, raw=False):\n",
    "    if explainer_type in ['GuidedBackprop', 'Deconvolution']:\n",
    "        explainer_mode = 'lrp-like'\n",
    "    else:\n",
    "        explainer_mode = 'shap-like'\n",
    "    \n",
    "        \n",
    "    if explainer_type == 'GuidedBackprop': #fast\n",
    "        explainer = GuidedBackprop(model)\n",
    "    elif explainer_type == 'GradientShap': #fast\n",
    "        if raw:\n",
    "            explainer = GradientShap(model, multiply_by_inputs=False)\n",
    "        else:\n",
    "            explainer = GradientShap(model, multiply_by_inputs=True)\n",
    "\n",
    "    elif explainer_type == 'Deconvolution': #fast\n",
    "        explainer = Deconvolution(model)\n",
    "    else:\n",
    "        raise ValueError('no such method')\n",
    "        \n",
    "    return explainer, explainer_mode\n",
    "\n",
    "def compute_correlation_metric(data, cor_type):\n",
    "    # Compute gene correlation measure\n",
    "    #  'pingouin.pcorr', 'np.cov', 'np.corcoeff'\n",
    "    if cor_type ==  'pingouin.pcorr':\n",
    "        cov = pingu.pcorr(pd.DataFrame(data))\n",
    "    elif cor_type == 'np.cov':\n",
    "        cov = np.cov(data.T)\n",
    "    elif cor_type == 'np.corrcoeff':\n",
    "        cov = np.corrcoef(data.T)\n",
    "    elif cor_type == 'None':\n",
    "        cov = 1\n",
    "    else: \n",
    "        cov = 1\n",
    "    return cov\n",
    "\n",
    "def aggregate_attributions(attributions, strategy = 'mean'):\n",
    "    if strategy == 'mean':\n",
    "        return np.mean(attributions, axis = 0)\n",
    "    elif strategy == 'sum':\n",
    "        return np.sum(attributions, axis = 0)\n",
    "    elif strategy == 'median':\n",
    "        return np.median(attributions, axis = 0)\n",
    "    else:\n",
    "        # Default to mean aggregation\n",
    "        return np.mean(attributions, axis = 0)\n",
    "    \n",
    "\n",
    "    \n",
    "def wrapper(models, data_train_full_tensor, gene_names, config):\n",
    "\n",
    "    data = data_train_full_tensor.detach().cpu().numpy()\n",
    "    tms = []\n",
    "    name_list = []\n",
    "    target_names = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ings = {}\n",
    "    for trained_model in models:        \n",
    "        trained_model.forward_mu_only = True\n",
    "        explainer, xai_type = get_explainer(trained_model, config.xai_method, config.raw_attribution)\n",
    "        tms.append(explainer)\n",
    "\n",
    "    attributions = []\n",
    "    ## ATTRIBUTIONS\n",
    "    for g in tqdm(range(data_train_full_tensor.shape[1])):\n",
    "    #for g in range(2):\n",
    "\n",
    "        attributions_list = attribution_one_target(\n",
    "            g,\n",
    "            tms,\n",
    "            data_train_full_tensor,\n",
    "            data_train_full_tensor,\n",
    "            xai_type=xai_type,\n",
    "            randomize_background = True)\n",
    "        attributions.append(attributions_list)\n",
    "\n",
    "    \n",
    "\n",
    "    ## AGGREGATION: REPLACE LIST BY AGGREGATED DATA\n",
    "    for i in range(len(attributions)):\n",
    "        # CURRENTLY MEAN\n",
    "        attributions[i] = aggregate_attributions(attributions[i], strategy=config.aggregation_strategy )\n",
    "    \n",
    "    print(attributions)\n",
    "    ## PENALIZE:\n",
    "    if config.penalty != 'None':\n",
    "        penalty_matrix = compute_correlation_metric(data, cor_type=config.penalty)\n",
    "        for i in range(len(attributions)):\n",
    "            # CURRENTLY MEAN\n",
    "            attributions[i] = np.dot(attributions[i], (1-penalty_matrix))\n",
    "\n",
    "    print(attributions)\n",
    "    \n",
    "    ## CLUSTERING: CLUSTER EACH TARGET INDVIDUALLY\n",
    "    for i in range(len(attributions)):\n",
    " \n",
    "        attributions[i] = ad.AnnData(attributions[i])\n",
    "        print(attributions[i])\n",
    "        sc.pp.scale(attributions[i])\n",
    "        try:\n",
    "            sc.pp.pca(attributions[i],n_comps=50)\n",
    "        except:\n",
    "            try:\n",
    "                sc.pp.pca(attributions[i],n_comps=50 )\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "        sc.pp.neighbors(attributions[i], n_neighbors=15)\n",
    "        sc.tl.leiden(attributions[i], resolution=0.1)\n",
    "\n",
    "        clusterings[f'T_{gene_names[i]}'] = np.array(attributions[i].obs['leiden'])\n",
    "\n",
    "    \n",
    "    #EDGE SELECTION:\n",
    "    for i in range(len(attributions)):\n",
    "        edge_indices = get_edges(attributions[i], use_differential=config.use_differential, percentile=config.percentile)\n",
    "        name_list = name_list + list(gene_names[edge_indices])\n",
    "        target_names = target_names+[gene_names[i]]* len(edge_indices)\n",
    "        attributions[i] = attributions[i][:,edge_indices].X\n",
    "\n",
    "    attributions = np.hstack(attributions)\n",
    "    \n",
    "    index_list = [f\"{s}_{t}\" for (s, t) in zip(name_list, target_names)]\n",
    "    cou = pd.DataFrame({'index': index_list, 'source':name_list, 'target':target_names})\n",
    "    cou = cou.set_index('index')\n",
    "\n",
    "    clusterings = pd.DataFrame(clusterings)\n",
    "\n",
    "    grn_adata = attribution_to_anndata(attributions, var=cou, obs = clusterings)\n",
    "\n",
    "    return grn_adata\n",
    "\n",
    "def run_netmap(config, dataset_config):\n",
    "\n",
    "    print('Version 2')\n",
    "    start_total = time.monotonic()\n",
    "    \n",
    "    ## Load config and setup outputs\n",
    "    os.makedirs(config.output_directory, exist_ok=True)\n",
    "    sc.settings.figdir = config.output_directory\n",
    "    config.write_yaml(yaml_file=op.join(config.output_directory, 'config.yaml'))\n",
    "\n",
    "    ## load data\n",
    "    adata = sc.read_h5ad(config.input_data)\n",
    "    \n",
    "\n",
    "    ## Get the data matrix from the CustumAnndata obeject\n",
    "\n",
    "    gene_names = np.array(adata.var.index)\n",
    "    model_start = time.monotonic()\n",
    "\n",
    "    if config.layer == 'counts':\n",
    "        data_tensor = adata.layers['counts']\n",
    "    else:\n",
    "        data_tensor = adata.X\n",
    "\n",
    "    if scs.issparse(data_tensor):\n",
    "        data_tensor = torch.tensor(data_tensor.todense(), dtype=torch.float32)\n",
    "    else:\n",
    "        data_tensor = torch.tensor(data_tensor, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    print(data_tensor.shape)\n",
    "\n",
    "    model_zoo = create_model_zoo(data_tensor, n_models=config.n_models, n_epochs=500)\n",
    "    grn_adata = wrapper(model_zoo, data_tensor.cuda(), gene_names, config)\n",
    "\n",
    "    adob = adata.obs.reset_index()\n",
    "    grn_adata.obs['cell_id'] = np.array(adob['cell_id'])\n",
    "    grn_adata.obs['grn'] = np.array(adob['grn'])\n",
    "\n",
    "    \n",
    "    model_elapsed = time.monotonic()-model_start\n",
    "    grn_adata.write_h5ad(op.join(config.output_directory,config.adata_filename))\n",
    "\n",
    "    time_elapsed_total = time.monotonic()-start_total\n",
    "\n",
    "\n",
    "    res = {'time_elapsed_total': time_elapsed_total, 'time_elapsed_netmap': model_elapsed} \n",
    "    write_config(res, file=op.join(config.output_directory, 'results.yaml'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845176dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "sys.path.append('/data_nfs/og86asub/netmap/netmap-evaluation/')\n",
    "from netmap.src.utils.data_utils import *\n",
    "from netmap.src.utils.tf_utils import *\n",
    "from netmap.src.utils.netmap_config import NetmapConfig\n",
    "from netmap.src.model.negbinautoencoder import *\n",
    "from netmap.src.model.negbinautoencoder import train_autoencoder\n",
    "from netmap.src.model.inferrence_simple import *\n",
    "from netmap.src.model.pipeline import *\n",
    "\n",
    "from src.data_simulation.data_simulation_config import DataSimulationConfig\n",
    "\n",
    "\n",
    "import yaml\n",
    "def read_config(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "import os.path as op\n",
    "\n",
    "#config = NetmapConfig.read_yaml(\"/data_nfs/og86asub/netmap/netmap-evaluation/results/configurations/netmap/config/perturb_seq/\")\n",
    "dada = \"/data_nfs/og86asub/netmap/netmap-evaluation/results/configurations/data_simulation/config_easy/net_172_54892_net_131_54992_net_158_55084.config.yaml\"\n",
    "dataset_config = read_config(\"/data_nfs/og86asub/netmap/netmap-evaluation/results/configurations/data_simulation/config_easy/net_172_54892_net_131_54992_net_158_55084.config.yaml\")\n",
    "\n",
    "nets = [pd.read_csv(op.join(\"/data_nfs/og86asub/netmap/netmap-evaluation/data/clustered_network/\", filename), sep='\\t') for filename in dataset_config['edgelist']]\n",
    "common = [pd.read_csv(op.join(\"/data_nfs/og86asub/netmap/netmap-evaluation/data/clustered_network/\", filename), sep='\\t') for filename in dataset_config['common_edges']]\n",
    "\n",
    "    \n",
    "config = NetmapConfig.read_yaml('/data_nfs/og86asub/netmap/netmap-evaluation/results/netmap/config_22/config_easy/net_172_54892_net_131_54992_net_158_55084/config.yaml')\n",
    "dataset_config = DataSimulationConfig.read_yaml(dada)\n",
    "\n",
    "\n",
    "\n",
    "start_total = time.monotonic()\n",
    "\n",
    "## Load config and setup outputs\n",
    "os.makedirs(config.output_directory, exist_ok=True)\n",
    "sc.settings.figdir = config.output_directory\n",
    "config.write_yaml(yaml_file=op.join(config.output_directory, 'config.yaml'))\n",
    "\n",
    "## load data\n",
    "adata = sc.read_h5ad(config.input_data)\n",
    "\n",
    "\n",
    "## Get the data matrix from the CustumAnndata obeject\n",
    "gene_names = np.array(adata.var.index)\n",
    "model_start = time.monotonic()\n",
    "\n",
    "if config.layer == 'counts':\n",
    "    data_tensor = adata.layers['counts']\n",
    "else:\n",
    "    data_tensor = adata.X\n",
    "\n",
    "if scs.issparse(data_tensor):\n",
    "    data_tensor = torch.tensor(data_tensor.todense(), dtype=torch.float32)\n",
    "else:\n",
    "    data_tensor = torch.tensor(data_tensor, dtype=torch.float32)\n",
    "\n",
    "\n",
    "print(data_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b00a8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75918b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def create_pairwise_binary_mask(matrix_cells_x_genes, gene_list):\n",
    "    \"\"\"\n",
    "    Creates a dictionary of binary masks for each cell and pair of genes,\n",
    "    including both forward, reverse, and self-pairs (which are all zeros).\n",
    "\n",
    "    Args:\n",
    "        matrix_cells_x_genes (np.ndarray): A 2D numpy array where rows are cells\n",
    "                                          and columns are genes.\n",
    "        gene_list (list): A list of strings containing the names of the genes,\n",
    "                          in the same order as the columns in the matrix.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are gene pair strings (e.g., 'GeneA_GeneB')\n",
    "              and values are 1D numpy arrays representing the binary mask for that pair\n",
    "              across all cells.\n",
    "    \"\"\"\n",
    "    binary_matrix = (matrix_cells_x_genes > 0).astype(int)\n",
    "    num_cells, num_genes = binary_matrix.shape\n",
    "\n",
    "    if len(gene_list) != num_genes:\n",
    "        raise ValueError(\"The length of the gene_list must match the number of genes (columns) in the matrix.\")\n",
    "\n",
    "    pairwise_mask_dict = {}\n",
    "    zero_vector = np.zeros(num_cells, dtype=int)\n",
    "    for g_idx, gene_name in enumerate(gene_list):\n",
    "        key = f\"{gene_name}_{gene_name}\"\n",
    "        pairwise_mask_dict[key] = zero_vector\n",
    "\n",
    "    gene_pairs_indices = list(itertools.combinations(range(num_genes), 2))\n",
    "    for g1_idx, g2_idx in gene_pairs_indices:\n",
    "        mask = binary_matrix[:, g1_idx] * binary_matrix[:, g2_idx]\n",
    "        key_fwd = f\"{gene_list[g1_idx]}_{gene_list[g2_idx]}\"\n",
    "        pairwise_mask_dict[key_fwd] = mask\n",
    "        key_rev = f\"{gene_list[g2_idx]}_{gene_list[g1_idx]}\"\n",
    "        pairwise_mask_dict[key_rev] = mask\n",
    "\n",
    "    return pairwise_mask_dict\n",
    "\n",
    "def dict_to_dataframe(mask_dict, column_order_list):\n",
    "    \"\"\"\n",
    "    Converts a dictionary of binary masks into a pandas DataFrame,\n",
    "    respecting a specified column order.\n",
    "\n",
    "    Args:\n",
    "        mask_dict (dict): A dictionary where keys are gene pair strings and\n",
    "                          values are 1D numpy arrays (the masks).\n",
    "        column_order_list (list): A list of gene pair strings specifying the\n",
    "                                  desired order of the DataFrame columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with masks as columns, in the specified order.\n",
    "    \"\"\"\n",
    "    # 1. Create a dictionary with only the ordered columns\n",
    "    ordered_data = {col: mask_dict[col] for col in column_order_list if col in mask_dict}\n",
    "    \n",
    "    # 2. Check if all specified columns were found\n",
    "    if len(ordered_data) != len(column_order_list):\n",
    "        missing_columns = set(column_order_list) - set(ordered_data.keys())\n",
    "        print(f\"Warning: The following columns were not found in the mask dictionary: {missing_columns}\")\n",
    "\n",
    "    # 3. Create the DataFrame from the ordered dictionary\n",
    "    df = pd.DataFrame(ordered_data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "counts = pd.DataFrame(adata.X, columns= adata.var_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4c124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70476738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def flatten_upper_triangular_excluding_diagonal(matrix):\n",
    "    \"\"\"\n",
    "    Flattens the upper triangular part of a matrix, excluding the diagonal.\n",
    "\n",
    "    Args:\n",
    "        matrix: A list of lists or a NumPy array representing the matrix.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array containing the flattened upper triangular elements.\n",
    "    \"\"\"\n",
    "    # Convert the input to a NumPy array for efficient operations.\n",
    "    np_matrix = np.array(matrix)\n",
    "\n",
    "    # Get the upper triangular part of the matrix, excluding the diagonal.\n",
    "    # The 'k=1' argument specifies that the diagonal should not be included.\n",
    "    upper_triangle = np.triu(np_matrix, k=1)\n",
    "\n",
    "    # Flatten the resulting matrix.\n",
    "    flattened_matrix = upper_triangle.flatten()\n",
    "\n",
    "    # Filter out the zero values that were not part of the original matrix.\n",
    "    # We use a boolean mask to keep only non-zero elements.\n",
    "    result = flattened_matrix[flattened_matrix != 0]\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b04852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "from scipy.stats import spearmanr\n",
    "from statsmodels.formula.api import quantreg\n",
    "\n",
    "def get_hierarchical_clustering(adata):\n",
    "\n",
    "    corr_matrix, _ = spearmanr(adata.X, axis=0)\n",
    "\n",
    "    corr_matrix = np.corrcoef(adata.X.T)\n",
    "\n",
    "    corr_dist = 1 - corr_matrix\n",
    "    dist_linkage = hierarchy.average(corr_dist)\n",
    "\n",
    "    df = pd.DataFrame({'cophenet':hierarchy.cophenet(dist_linkage), 'corr':  flatten_upper_triangular_excluding_diagonal(corr_matrix)})\n",
    "\n",
    "    low_quantile_model = quantreg('corr ~ cophenet', df).fit(q=0.1)\n",
    "\n",
    "    # np.sort is used to ensure the line is drawn smoothly from left to right\n",
    "    x_sorted = np.sort(df['cophenet'])\n",
    "    y_predicted = low_quantile_model.predict({'cophenet': x_sorted})\n",
    "\n",
    "    return dist_linkage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929209c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f1190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def fit_regression_clusterings(corr_matrix, dist_linkage, threshold=2):\n",
    "\n",
    "    df = pd.DataFrame({'cophenet':hierarchy.cophenet(dist_linkage), 'corr':  flatten_upper_triangular_excluding_diagonal(corr_matrix)})\n",
    "    df =df[df.cophenet<threshold]\n",
    "    low_quantile_model = quantreg('corr ~ cophenet', df).fit(q=0.1)\n",
    "\n",
    "    # np.sort is used to ensure the line is drawn smoothly from left to right\n",
    "    df = df.sort_values('cophenet')\n",
    "    x_sorted = df['cophenet']\n",
    "\n",
    "    # model line\n",
    "    y_predicted = low_quantile_model.predict({'cophenet': x_sorted})\n",
    "    df['linear_model'] = y_predicted\n",
    "    return df\n",
    "\n",
    "def plot_regression(df):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.scatter(df['cophenet'], df['corr'], alpha=0.7, label='Data Points')\n",
    "\n",
    "    ax.plot(df['cophenet'], df['linear_model'], color='red', linewidth=2, label='10th Percentile Quantile Regression Line')\n",
    "    ax.axhline(y=0.6, color='r', linestyle='--', label='y = 0.6')\n",
    "    # Add labels and a legend for clarity\n",
    "    plt.title('Quantile Regression with correlation threshold')\n",
    "    plt.xlabel('Cophenet')\n",
    "    plt.ylabel('Correlation')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9ae09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cut_clustering_and_gene_mapping(corr_matrix, dist_linkage, threshold, variable_names):\n",
    "    clusters = hierarchy.fcluster(dist_linkage, t=threshold, criterion='distance')\n",
    "\n",
    "    cluster_to_genes = {}\n",
    "    for gene_name, cluster_id in zip(variable_names, clusters):\n",
    "        if cluster_id not in cluster_to_genes:\n",
    "            cluster_to_genes[cluster_id] = []\n",
    "        cluster_to_genes[cluster_id].append(gene_name)\n",
    "\n",
    "    high_average_similarity = {}\n",
    "    high_average_similarity_idx = {}\n",
    "\n",
    "    for cluster_id, gene_list in cluster_to_genes.items():\n",
    "        if len(gene_list) > 1:\n",
    "            # Get the sub-matrix of the correlation matrix for the genes in the cluster\n",
    "            gene_indices = [variable_names.get_loc(g) for g in gene_list]\n",
    "            cluster_corr_matrix = corr_matrix[np.ix_(gene_indices, gene_indices)]\n",
    "\n",
    "            # Calculate the average of the upper triangle (excluding the diagonal)\n",
    "            upper_triangle_indices = np.triu_indices_from(cluster_corr_matrix, k=1)\n",
    "            average_similarity = np.mean(cluster_corr_matrix[upper_triangle_indices])\n",
    "\n",
    "            #if average_similarity >= req_sim:\n",
    "            print(f\"Cluster {cluster_id}: {gene_list}\")\n",
    "            print(f\"  Average Similarity: {average_similarity:.4f}\")\n",
    "            print(\"-\" * 45)\n",
    "            high_average_similarity[cluster_id] = gene_list\n",
    "            high_average_similarity_idx[cluster_id] = gene_indices\n",
    "    return high_average_similarity_idx, high_average_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f82742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import random\n",
    "\n",
    "def get_max_identity_class_size(high_average_similarity_idx):\n",
    "    # order not important\n",
    "    identity_class_sizes_l = []\n",
    "    identity_class_sizes = {}\n",
    "    for k in high_average_similarity_idx.keys():\n",
    "        identity_class_sizes[k] = len(high_average_similarity_idx[k])\n",
    "        identity_class_sizes_l.append(len(high_average_similarity_idx[k]))\n",
    "    return np.max(identity_class_sizes_l)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_id_class_representative(data_dict: OrderedDict, keys):\n",
    "\n",
    "    elements = []\n",
    "    for key in keys:\n",
    "        value = data_dict[key]\n",
    "        \n",
    "        # Check if the 'unused' list is empty\n",
    "        if not value['unused']:\n",
    "            value['unused'].extend(value['used'])\n",
    "            value['used'].clear()\n",
    "            random.shuffle(value['unused'])\n",
    "\n",
    "        random_index = random.randint(0, len(value['unused']) - 1)\n",
    "        element = value['unused'].pop(random_index)\n",
    "        value['used'].append(element)\n",
    "        elements.append(element)\n",
    "\n",
    "    return elements, data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c18e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_assembly_dict(high_average_similarity_idx):\n",
    "    id_class_dictionary = {}\n",
    "    all_genes_in_identity_class = []\n",
    "    for k in  high_average_similarity_idx:\n",
    "        id_class_dictionary[k] = {'unused': high_average_similarity_idx[k].copy(), 'used' : []}\n",
    "        all_genes_in_identity_class = all_genes_in_identity_class+high_average_similarity_idx[k]\n",
    "    return id_class_dictionary, all_genes_in_identity_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f6fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import random\n",
    "\n",
    "def get_id_class_representative(data_dict: OrderedDict, keys) -> OrderedDict:\n",
    "\n",
    "    elements = []\n",
    "    for key in keys:\n",
    "        value = data_dict[key]\n",
    "        \n",
    "        # Check if the 'unused' list is empty\n",
    "        if not value['unused']:\n",
    "            value['unused'].extend(value['used'])\n",
    "            value['used'].clear()\n",
    "            random.shuffle(value['unused'])\n",
    "\n",
    "        random_index = random.randint(0, len(value['unused']) - 1)\n",
    "        element = value['unused'].pop(random_index)\n",
    "        value['used'].append(element)\n",
    "        elements.append(element)\n",
    "\n",
    "    return elements, data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f1c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = np.corrcoef(adata.X.T)\n",
    "\n",
    "corr_dist = 1 - corr_matrix\n",
    "dist_linkage = hierarchy.average(corr_dist)\n",
    "df = fit_regression_clusterings(corr_matrix, dist_linkage, threshold=2)\n",
    "plot_regression(df)\n",
    "cluster_mapping, cluster_mapping_genes = cut_clustering_and_gene_mapping(corr_matrix, dist_linkage, 0.75, adata.var_names)\n",
    "max_id_classes = get_max_identity_class_size(cluster_mapping)\n",
    "\n",
    "\n",
    "id_class_dictionary, all_genes_in_id_class = create_dataset_assembly_dict(cluster_mapping)\n",
    "other_genes = set(range(len(adata.var_names))) - set(all_genes_in_id_class)\n",
    "keys = list(cluster_mapping.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9332bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd2e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model_zoo(data_tensor, id_class_dictionary, keys, other_genes, n_models = 4, n_epochs = 500):\n",
    "    model_zoo = []\n",
    "    variables_selected = []\n",
    "    representatives = []\n",
    "    for _ in range(n_models):\n",
    "\n",
    "        elements, id_class_dictionary = get_id_class_representative(id_class_dictionary, keys)\n",
    "\n",
    "        current_data_selection = list(other_genes)+list(elements)\n",
    "        current_data = data_tensor[:,current_data_selection]\n",
    "        data_train2, data_test2 = train_test_split(current_data,test_size=0.01, shuffle=True)\n",
    "\n",
    "        trained_model2 = ZINBAutoencoder(input_dim=current_data.shape[1], latent_dim=10, dropout_rate = 0.02)\n",
    "        trained_model2 = trained_model2.cuda()\n",
    "\n",
    "        optimizer2 = torch.optim.Adam(trained_model2.parameters(), lr=1e-4)\n",
    "\n",
    "        trained_model2 = train_autoencoder(\n",
    "                trained_model2,\n",
    "                data_train2.cuda(),\n",
    "                optimizer2,\n",
    "                num_epochs=n_epochs\n",
    "\n",
    "            )\n",
    "        model_zoo.append(trained_model2)\n",
    "        variables_selected.append(current_data_selection)\n",
    "        representatives.append(elements)\n",
    "    return model_zoo, variables_selected, representatives\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "1d4592dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 37.0872\n",
      "Epoch 11/500, Loss: 5.0993\n",
      "Epoch 21/500, Loss: 4.2926\n",
      "Epoch 31/500, Loss: 3.9876\n",
      "Epoch 41/500, Loss: 3.8438\n",
      "Epoch 51/500, Loss: 3.7589\n",
      "Epoch 61/500, Loss: 3.6955\n",
      "Epoch 71/500, Loss: 3.6509\n",
      "Epoch 81/500, Loss: 3.6097\n",
      "Epoch 91/500, Loss: 3.5774\n",
      "Epoch 101/500, Loss: 3.5484\n",
      "Epoch 111/500, Loss: 3.5207\n",
      "Epoch 121/500, Loss: 3.4984\n",
      "Epoch 131/500, Loss: 3.4772\n",
      "Epoch 141/500, Loss: 3.4640\n",
      "Epoch 151/500, Loss: 3.4501\n",
      "Epoch 161/500, Loss: 3.4365\n",
      "Epoch 171/500, Loss: 3.4241\n",
      "Epoch 181/500, Loss: 3.4091\n",
      "Epoch 191/500, Loss: 3.4000\n",
      "Epoch 201/500, Loss: 3.3915\n",
      "Epoch 211/500, Loss: 3.3833\n",
      "Epoch 221/500, Loss: 3.3737\n",
      "Epoch 231/500, Loss: 3.3669\n",
      "Epoch 241/500, Loss: 3.3593\n",
      "Epoch 251/500, Loss: 3.3565\n",
      "Epoch 261/500, Loss: 3.3490\n",
      "Epoch 271/500, Loss: 3.3421\n",
      "Epoch 281/500, Loss: 3.3380\n",
      "Epoch 291/500, Loss: 3.3316\n",
      "Epoch 301/500, Loss: 3.3257\n",
      "Epoch 311/500, Loss: 3.3204\n",
      "Epoch 321/500, Loss: 3.3129\n",
      "Epoch 331/500, Loss: 3.3090\n",
      "Epoch 341/500, Loss: 3.3052\n",
      "Epoch 351/500, Loss: 3.3010\n",
      "Epoch 361/500, Loss: 3.2959\n",
      "Epoch 371/500, Loss: 3.2906\n",
      "Epoch 381/500, Loss: 3.2878\n",
      "Epoch 391/500, Loss: 3.2862\n",
      "Epoch 401/500, Loss: 3.2818\n",
      "Epoch 411/500, Loss: 3.2778\n",
      "Epoch 421/500, Loss: 3.2763\n",
      "Epoch 431/500, Loss: 3.2721\n",
      "Epoch 441/500, Loss: 3.2696\n",
      "Epoch 451/500, Loss: 3.2664\n",
      "Epoch 461/500, Loss: 3.2626\n",
      "Epoch 471/500, Loss: 3.2602\n",
      "Epoch 481/500, Loss: 3.2574\n",
      "Epoch 491/500, Loss: 3.2543\n",
      "Epoch 1/500, Loss: 36.7209\n",
      "Epoch 11/500, Loss: 5.3628\n",
      "Epoch 21/500, Loss: 4.6017\n",
      "Epoch 31/500, Loss: 4.1502\n",
      "Epoch 41/500, Loss: 3.9360\n",
      "Epoch 51/500, Loss: 3.8056\n",
      "Epoch 61/500, Loss: 3.7374\n",
      "Epoch 71/500, Loss: 3.6799\n",
      "Epoch 81/500, Loss: 3.6385\n",
      "Epoch 91/500, Loss: 3.6045\n",
      "Epoch 101/500, Loss: 3.5763\n",
      "Epoch 111/500, Loss: 3.5438\n",
      "Epoch 121/500, Loss: 3.5240\n",
      "Epoch 131/500, Loss: 3.5004\n",
      "Epoch 141/500, Loss: 3.4839\n",
      "Epoch 151/500, Loss: 3.4703\n",
      "Epoch 161/500, Loss: 3.4511\n",
      "Epoch 171/500, Loss: 3.4424\n",
      "Epoch 181/500, Loss: 3.4280\n",
      "Epoch 191/500, Loss: 3.4172\n",
      "Epoch 201/500, Loss: 3.4052\n",
      "Epoch 211/500, Loss: 3.3973\n",
      "Epoch 221/500, Loss: 3.3909\n",
      "Epoch 231/500, Loss: 3.3836\n",
      "Epoch 241/500, Loss: 3.3742\n",
      "Epoch 251/500, Loss: 3.3656\n",
      "Epoch 261/500, Loss: 3.3614\n",
      "Epoch 271/500, Loss: 3.3528\n",
      "Epoch 281/500, Loss: 3.3458\n",
      "Epoch 291/500, Loss: 3.3402\n",
      "Epoch 301/500, Loss: 3.3360\n",
      "Epoch 311/500, Loss: 3.3298\n",
      "Epoch 321/500, Loss: 3.3242\n",
      "Epoch 331/500, Loss: 3.3186\n",
      "Epoch 341/500, Loss: 3.3141\n",
      "Epoch 351/500, Loss: 3.3100\n",
      "Epoch 361/500, Loss: 3.3036\n",
      "Epoch 371/500, Loss: 3.3019\n",
      "Epoch 381/500, Loss: 3.2946\n",
      "Epoch 391/500, Loss: 3.2913\n",
      "Epoch 401/500, Loss: 3.2889\n",
      "Epoch 411/500, Loss: 3.2840\n",
      "Epoch 421/500, Loss: 3.2803\n",
      "Epoch 431/500, Loss: 3.2770\n",
      "Epoch 441/500, Loss: 3.2742\n",
      "Epoch 451/500, Loss: 3.2696\n",
      "Epoch 461/500, Loss: 3.2688\n",
      "Epoch 471/500, Loss: 3.2649\n",
      "Epoch 481/500, Loss: 3.2618\n",
      "Epoch 491/500, Loss: 3.2587\n",
      "Epoch 1/500, Loss: 37.6267\n",
      "Epoch 11/500, Loss: 5.1717\n",
      "Epoch 21/500, Loss: 4.1687\n",
      "Epoch 31/500, Loss: 3.8803\n",
      "Epoch 41/500, Loss: 3.7639\n",
      "Epoch 51/500, Loss: 3.6889\n",
      "Epoch 61/500, Loss: 3.6397\n",
      "Epoch 71/500, Loss: 3.5996\n",
      "Epoch 81/500, Loss: 3.5629\n",
      "Epoch 91/500, Loss: 3.5378\n",
      "Epoch 101/500, Loss: 3.5161\n",
      "Epoch 111/500, Loss: 3.4971\n",
      "Epoch 121/500, Loss: 3.4758\n",
      "Epoch 131/500, Loss: 3.4620\n",
      "Epoch 141/500, Loss: 3.4434\n",
      "Epoch 151/500, Loss: 3.4326\n",
      "Epoch 161/500, Loss: 3.4211\n",
      "Epoch 171/500, Loss: 3.4073\n",
      "Epoch 181/500, Loss: 3.4012\n",
      "Epoch 191/500, Loss: 3.3890\n",
      "Epoch 201/500, Loss: 3.3783\n",
      "Epoch 211/500, Loss: 3.3709\n",
      "Epoch 221/500, Loss: 3.3610\n",
      "Epoch 231/500, Loss: 3.3539\n",
      "Epoch 241/500, Loss: 3.3474\n",
      "Epoch 251/500, Loss: 3.3416\n",
      "Epoch 261/500, Loss: 3.3334\n",
      "Epoch 271/500, Loss: 3.3278\n",
      "Epoch 281/500, Loss: 3.3224\n",
      "Epoch 291/500, Loss: 3.3167\n",
      "Epoch 301/500, Loss: 3.3155\n",
      "Epoch 311/500, Loss: 3.3088\n",
      "Epoch 321/500, Loss: 3.3043\n",
      "Epoch 331/500, Loss: 3.3022\n",
      "Epoch 341/500, Loss: 3.2953\n",
      "Epoch 351/500, Loss: 3.2922\n",
      "Epoch 361/500, Loss: 3.2875\n",
      "Epoch 371/500, Loss: 3.2869\n",
      "Epoch 381/500, Loss: 3.2813\n",
      "Epoch 391/500, Loss: 3.2787\n",
      "Epoch 401/500, Loss: 3.2746\n",
      "Epoch 411/500, Loss: 3.2718\n",
      "Epoch 421/500, Loss: 3.2695\n",
      "Epoch 431/500, Loss: 3.2678\n",
      "Epoch 441/500, Loss: 3.2647\n",
      "Epoch 451/500, Loss: 3.2611\n",
      "Epoch 461/500, Loss: 3.2591\n",
      "Epoch 471/500, Loss: 3.2561\n",
      "Epoch 481/500, Loss: 3.2546\n",
      "Epoch 491/500, Loss: 3.2516\n",
      "Epoch 1/500, Loss: 33.0407\n",
      "Epoch 11/500, Loss: 5.0861\n",
      "Epoch 21/500, Loss: 4.1469\n",
      "Epoch 31/500, Loss: 3.8763\n",
      "Epoch 41/500, Loss: 3.7655\n",
      "Epoch 51/500, Loss: 3.6841\n",
      "Epoch 61/500, Loss: 3.6174\n",
      "Epoch 71/500, Loss: 3.5736\n",
      "Epoch 81/500, Loss: 3.5358\n",
      "Epoch 91/500, Loss: 3.5120\n",
      "Epoch 101/500, Loss: 3.4900\n",
      "Epoch 111/500, Loss: 3.4678\n",
      "Epoch 121/500, Loss: 3.4537\n",
      "Epoch 131/500, Loss: 3.4372\n",
      "Epoch 141/500, Loss: 3.4268\n",
      "Epoch 151/500, Loss: 3.4121\n",
      "Epoch 161/500, Loss: 3.4009\n",
      "Epoch 171/500, Loss: 3.3886\n",
      "Epoch 181/500, Loss: 3.3823\n",
      "Epoch 191/500, Loss: 3.3745\n",
      "Epoch 201/500, Loss: 3.3616\n",
      "Epoch 211/500, Loss: 3.3563\n",
      "Epoch 221/500, Loss: 3.3474\n",
      "Epoch 231/500, Loss: 3.3417\n",
      "Epoch 241/500, Loss: 3.3380\n",
      "Epoch 251/500, Loss: 3.3300\n",
      "Epoch 261/500, Loss: 3.3234\n",
      "Epoch 271/500, Loss: 3.3196\n",
      "Epoch 281/500, Loss: 3.3129\n",
      "Epoch 291/500, Loss: 3.3103\n",
      "Epoch 301/500, Loss: 3.3051\n",
      "Epoch 311/500, Loss: 3.3014\n",
      "Epoch 321/500, Loss: 3.2981\n",
      "Epoch 331/500, Loss: 3.2933\n",
      "Epoch 341/500, Loss: 3.2912\n",
      "Epoch 351/500, Loss: 3.2850\n",
      "Epoch 361/500, Loss: 3.2822\n",
      "Epoch 371/500, Loss: 3.2795\n",
      "Epoch 381/500, Loss: 3.2752\n",
      "Epoch 391/500, Loss: 3.2725\n",
      "Epoch 401/500, Loss: 3.2701\n",
      "Epoch 411/500, Loss: 3.2678\n",
      "Epoch 421/500, Loss: 3.2639\n",
      "Epoch 431/500, Loss: 3.2620\n",
      "Epoch 441/500, Loss: 3.2598\n",
      "Epoch 451/500, Loss: 3.2538\n",
      "Epoch 461/500, Loss: 3.2533\n",
      "Epoch 471/500, Loss: 3.2517\n",
      "Epoch 481/500, Loss: 3.2490\n",
      "Epoch 491/500, Loss: 3.2469\n",
      "Epoch 1/500, Loss: 40.7416\n",
      "Epoch 11/500, Loss: 5.1191\n",
      "Epoch 21/500, Loss: 4.1764\n",
      "Epoch 31/500, Loss: 3.8902\n",
      "Epoch 41/500, Loss: 3.7685\n",
      "Epoch 51/500, Loss: 3.6898\n",
      "Epoch 61/500, Loss: 3.6320\n",
      "Epoch 71/500, Loss: 3.5900\n",
      "Epoch 81/500, Loss: 3.5542\n",
      "Epoch 91/500, Loss: 3.5301\n",
      "Epoch 101/500, Loss: 3.5027\n",
      "Epoch 111/500, Loss: 3.4852\n",
      "Epoch 121/500, Loss: 3.4674\n",
      "Epoch 131/500, Loss: 3.4522\n",
      "Epoch 141/500, Loss: 3.4421\n",
      "Epoch 151/500, Loss: 3.4297\n",
      "Epoch 161/500, Loss: 3.4184\n",
      "Epoch 171/500, Loss: 3.4084\n",
      "Epoch 181/500, Loss: 3.3986\n",
      "Epoch 191/500, Loss: 3.3889\n",
      "Epoch 201/500, Loss: 3.3815\n",
      "Epoch 211/500, Loss: 3.3740\n",
      "Epoch 221/500, Loss: 3.3653\n",
      "Epoch 231/500, Loss: 3.3604\n",
      "Epoch 241/500, Loss: 3.3539\n",
      "Epoch 251/500, Loss: 3.3459\n",
      "Epoch 261/500, Loss: 3.3412\n",
      "Epoch 271/500, Loss: 3.3364\n",
      "Epoch 281/500, Loss: 3.3323\n",
      "Epoch 291/500, Loss: 3.3263\n",
      "Epoch 301/500, Loss: 3.3197\n",
      "Epoch 311/500, Loss: 3.3140\n",
      "Epoch 321/500, Loss: 3.3110\n",
      "Epoch 331/500, Loss: 3.3055\n",
      "Epoch 341/500, Loss: 3.3017\n",
      "Epoch 351/500, Loss: 3.3003\n",
      "Epoch 361/500, Loss: 3.2941\n",
      "Epoch 371/500, Loss: 3.2913\n",
      "Epoch 381/500, Loss: 3.2877\n",
      "Epoch 391/500, Loss: 3.2848\n",
      "Epoch 401/500, Loss: 3.2810\n",
      "Epoch 411/500, Loss: 3.2780\n",
      "Epoch 421/500, Loss: 3.2738\n",
      "Epoch 431/500, Loss: 3.2722\n",
      "Epoch 441/500, Loss: 3.2689\n",
      "Epoch 451/500, Loss: 3.2653\n",
      "Epoch 461/500, Loss: 3.2639\n",
      "Epoch 471/500, Loss: 3.2619\n",
      "Epoch 481/500, Loss: 3.2562\n",
      "Epoch 491/500, Loss: 3.2573\n",
      "Epoch 1/500, Loss: 35.8433\n",
      "Epoch 11/500, Loss: 4.9206\n",
      "Epoch 21/500, Loss: 4.2707\n",
      "Epoch 31/500, Loss: 3.9919\n",
      "Epoch 41/500, Loss: 3.8772\n",
      "Epoch 51/500, Loss: 3.7920\n",
      "Epoch 61/500, Loss: 3.7356\n",
      "Epoch 71/500, Loss: 3.6930\n",
      "Epoch 81/500, Loss: 3.6464\n",
      "Epoch 91/500, Loss: 3.6156\n",
      "Epoch 101/500, Loss: 3.5844\n",
      "Epoch 111/500, Loss: 3.5571\n",
      "Epoch 121/500, Loss: 3.5298\n",
      "Epoch 131/500, Loss: 3.5097\n",
      "Epoch 141/500, Loss: 3.4893\n",
      "Epoch 151/500, Loss: 3.4711\n",
      "Epoch 161/500, Loss: 3.4549\n",
      "Epoch 171/500, Loss: 3.4411\n",
      "Epoch 181/500, Loss: 3.4276\n",
      "Epoch 191/500, Loss: 3.4153\n",
      "Epoch 201/500, Loss: 3.4016\n",
      "Epoch 211/500, Loss: 3.3913\n",
      "Epoch 221/500, Loss: 3.3791\n",
      "Epoch 231/500, Loss: 3.3710\n",
      "Epoch 241/500, Loss: 3.3628\n",
      "Epoch 251/500, Loss: 3.3538\n",
      "Epoch 261/500, Loss: 3.3459\n",
      "Epoch 271/500, Loss: 3.3388\n",
      "Epoch 281/500, Loss: 3.3326\n",
      "Epoch 291/500, Loss: 3.3255\n",
      "Epoch 301/500, Loss: 3.3198\n",
      "Epoch 311/500, Loss: 3.3145\n",
      "Epoch 321/500, Loss: 3.3081\n",
      "Epoch 331/500, Loss: 3.3042\n",
      "Epoch 341/500, Loss: 3.2995\n",
      "Epoch 351/500, Loss: 3.2938\n",
      "Epoch 361/500, Loss: 3.2914\n",
      "Epoch 371/500, Loss: 3.2856\n",
      "Epoch 381/500, Loss: 3.2824\n",
      "Epoch 391/500, Loss: 3.2775\n",
      "Epoch 401/500, Loss: 3.2746\n",
      "Epoch 411/500, Loss: 3.2693\n",
      "Epoch 421/500, Loss: 3.2685\n",
      "Epoch 431/500, Loss: 3.2681\n",
      "Epoch 441/500, Loss: 3.2623\n",
      "Epoch 451/500, Loss: 3.2604\n",
      "Epoch 461/500, Loss: 3.2577\n",
      "Epoch 471/500, Loss: 3.2541\n",
      "Epoch 481/500, Loss: 3.2495\n",
      "Epoch 491/500, Loss: 3.2480\n",
      "Epoch 1/500, Loss: 43.7915\n",
      "Epoch 11/500, Loss: 4.8527\n",
      "Epoch 21/500, Loss: 4.1849\n",
      "Epoch 31/500, Loss: 3.9586\n",
      "Epoch 41/500, Loss: 3.8436\n",
      "Epoch 51/500, Loss: 3.7620\n",
      "Epoch 61/500, Loss: 3.7031\n",
      "Epoch 71/500, Loss: 3.6575\n",
      "Epoch 81/500, Loss: 3.6189\n",
      "Epoch 91/500, Loss: 3.5916\n",
      "Epoch 101/500, Loss: 3.5590\n",
      "Epoch 111/500, Loss: 3.5386\n",
      "Epoch 121/500, Loss: 3.5144\n",
      "Epoch 131/500, Loss: 3.4965\n",
      "Epoch 141/500, Loss: 3.4766\n",
      "Epoch 151/500, Loss: 3.4591\n",
      "Epoch 161/500, Loss: 3.4465\n",
      "Epoch 171/500, Loss: 3.4341\n",
      "Epoch 181/500, Loss: 3.4259\n",
      "Epoch 191/500, Loss: 3.4124\n",
      "Epoch 201/500, Loss: 3.4057\n",
      "Epoch 211/500, Loss: 3.3989\n",
      "Epoch 221/500, Loss: 3.3865\n",
      "Epoch 231/500, Loss: 3.3796\n",
      "Epoch 241/500, Loss: 3.3695\n",
      "Epoch 251/500, Loss: 3.3643\n",
      "Epoch 261/500, Loss: 3.3567\n",
      "Epoch 271/500, Loss: 3.3506\n",
      "Epoch 281/500, Loss: 3.3453\n",
      "Epoch 291/500, Loss: 3.3373\n",
      "Epoch 301/500, Loss: 3.3330\n",
      "Epoch 311/500, Loss: 3.3264\n",
      "Epoch 321/500, Loss: 3.3222\n",
      "Epoch 331/500, Loss: 3.3179\n",
      "Epoch 341/500, Loss: 3.3119\n",
      "Epoch 351/500, Loss: 3.3069\n",
      "Epoch 361/500, Loss: 3.3047\n",
      "Epoch 371/500, Loss: 3.3005\n",
      "Epoch 381/500, Loss: 3.2962\n",
      "Epoch 391/500, Loss: 3.2934\n",
      "Epoch 401/500, Loss: 3.2889\n",
      "Epoch 411/500, Loss: 3.2860\n",
      "Epoch 421/500, Loss: 3.2808\n",
      "Epoch 431/500, Loss: 3.2785\n",
      "Epoch 441/500, Loss: 3.2741\n",
      "Epoch 451/500, Loss: 3.2728\n",
      "Epoch 461/500, Loss: 3.2684\n",
      "Epoch 471/500, Loss: 3.2658\n",
      "Epoch 481/500, Loss: 3.2629\n",
      "Epoch 491/500, Loss: 3.2596\n",
      "Epoch 1/500, Loss: 40.1842\n",
      "Epoch 11/500, Loss: 5.2240\n",
      "Epoch 21/500, Loss: 4.4563\n",
      "Epoch 31/500, Loss: 4.0447\n",
      "Epoch 41/500, Loss: 3.8709\n",
      "Epoch 51/500, Loss: 3.7682\n",
      "Epoch 61/500, Loss: 3.7005\n",
      "Epoch 71/500, Loss: 3.6538\n",
      "Epoch 81/500, Loss: 3.6132\n",
      "Epoch 91/500, Loss: 3.5776\n",
      "Epoch 101/500, Loss: 3.5493\n",
      "Epoch 111/500, Loss: 3.5246\n",
      "Epoch 121/500, Loss: 3.5002\n",
      "Epoch 131/500, Loss: 3.4827\n",
      "Epoch 141/500, Loss: 3.4667\n",
      "Epoch 151/500, Loss: 3.4517\n",
      "Epoch 161/500, Loss: 3.4424\n",
      "Epoch 171/500, Loss: 3.4294\n",
      "Epoch 181/500, Loss: 3.4166\n",
      "Epoch 191/500, Loss: 3.4090\n",
      "Epoch 201/500, Loss: 3.3989\n",
      "Epoch 211/500, Loss: 3.3878\n",
      "Epoch 221/500, Loss: 3.3795\n",
      "Epoch 231/500, Loss: 3.3731\n",
      "Epoch 241/500, Loss: 3.3641\n",
      "Epoch 251/500, Loss: 3.3561\n",
      "Epoch 261/500, Loss: 3.3491\n",
      "Epoch 271/500, Loss: 3.3409\n",
      "Epoch 281/500, Loss: 3.3349\n",
      "Epoch 291/500, Loss: 3.3262\n",
      "Epoch 301/500, Loss: 3.3206\n",
      "Epoch 311/500, Loss: 3.3180\n",
      "Epoch 321/500, Loss: 3.3109\n",
      "Epoch 331/500, Loss: 3.3062\n",
      "Epoch 341/500, Loss: 3.3025\n",
      "Epoch 351/500, Loss: 3.2977\n",
      "Epoch 361/500, Loss: 3.2936\n",
      "Epoch 371/500, Loss: 3.2901\n",
      "Epoch 381/500, Loss: 3.2856\n",
      "Epoch 391/500, Loss: 3.2809\n",
      "Epoch 401/500, Loss: 3.2790\n",
      "Epoch 411/500, Loss: 3.2740\n",
      "Epoch 421/500, Loss: 3.2727\n",
      "Epoch 431/500, Loss: 3.2681\n",
      "Epoch 441/500, Loss: 3.2651\n",
      "Epoch 451/500, Loss: 3.2625\n",
      "Epoch 461/500, Loss: 3.2580\n",
      "Epoch 471/500, Loss: 3.2560\n",
      "Epoch 481/500, Loss: 3.2517\n",
      "Epoch 491/500, Loss: 3.2508\n",
      "Epoch 1/500, Loss: 37.8838\n",
      "Epoch 11/500, Loss: 5.4464\n",
      "Epoch 21/500, Loss: 4.5549\n",
      "Epoch 31/500, Loss: 4.1167\n",
      "Epoch 41/500, Loss: 3.9443\n",
      "Epoch 51/500, Loss: 3.8258\n",
      "Epoch 61/500, Loss: 3.7541\n",
      "Epoch 71/500, Loss: 3.6954\n",
      "Epoch 81/500, Loss: 3.6526\n",
      "Epoch 91/500, Loss: 3.6166\n",
      "Epoch 101/500, Loss: 3.5831\n",
      "Epoch 111/500, Loss: 3.5574\n",
      "Epoch 121/500, Loss: 3.5268\n",
      "Epoch 131/500, Loss: 3.5073\n",
      "Epoch 141/500, Loss: 3.4873\n",
      "Epoch 151/500, Loss: 3.4683\n",
      "Epoch 161/500, Loss: 3.4516\n",
      "Epoch 171/500, Loss: 3.4401\n",
      "Epoch 181/500, Loss: 3.4288\n",
      "Epoch 191/500, Loss: 3.4147\n",
      "Epoch 201/500, Loss: 3.3995\n",
      "Epoch 211/500, Loss: 3.3893\n",
      "Epoch 221/500, Loss: 3.3798\n",
      "Epoch 231/500, Loss: 3.3703\n",
      "Epoch 241/500, Loss: 3.3627\n",
      "Epoch 251/500, Loss: 3.3541\n",
      "Epoch 261/500, Loss: 3.3480\n",
      "Epoch 271/500, Loss: 3.3428\n",
      "Epoch 281/500, Loss: 3.3356\n",
      "Epoch 291/500, Loss: 3.3310\n",
      "Epoch 301/500, Loss: 3.3240\n",
      "Epoch 311/500, Loss: 3.3185\n",
      "Epoch 321/500, Loss: 3.3154\n",
      "Epoch 331/500, Loss: 3.3097\n",
      "Epoch 341/500, Loss: 3.3062\n",
      "Epoch 351/500, Loss: 3.3027\n",
      "Epoch 361/500, Loss: 3.2966\n",
      "Epoch 371/500, Loss: 3.2924\n",
      "Epoch 381/500, Loss: 3.2902\n",
      "Epoch 391/500, Loss: 3.2843\n",
      "Epoch 401/500, Loss: 3.2829\n",
      "Epoch 411/500, Loss: 3.2789\n",
      "Epoch 421/500, Loss: 3.2749\n",
      "Epoch 431/500, Loss: 3.2721\n",
      "Epoch 441/500, Loss: 3.2687\n",
      "Epoch 451/500, Loss: 3.2656\n",
      "Epoch 461/500, Loss: 3.2638\n",
      "Epoch 471/500, Loss: 3.2595\n",
      "Epoch 481/500, Loss: 3.2575\n",
      "Epoch 491/500, Loss: 3.2537\n",
      "Epoch 1/500, Loss: 32.8544\n",
      "Epoch 11/500, Loss: 5.2417\n",
      "Epoch 21/500, Loss: 4.3100\n",
      "Epoch 31/500, Loss: 3.9539\n",
      "Epoch 41/500, Loss: 3.8030\n",
      "Epoch 51/500, Loss: 3.7078\n",
      "Epoch 61/500, Loss: 3.6485\n",
      "Epoch 71/500, Loss: 3.6105\n",
      "Epoch 81/500, Loss: 3.5806\n",
      "Epoch 91/500, Loss: 3.5492\n",
      "Epoch 101/500, Loss: 3.5228\n",
      "Epoch 111/500, Loss: 3.4986\n",
      "Epoch 121/500, Loss: 3.4804\n",
      "Epoch 131/500, Loss: 3.4591\n",
      "Epoch 141/500, Loss: 3.4504\n",
      "Epoch 151/500, Loss: 3.4383\n",
      "Epoch 161/500, Loss: 3.4289\n",
      "Epoch 171/500, Loss: 3.4132\n",
      "Epoch 181/500, Loss: 3.4019\n",
      "Epoch 191/500, Loss: 3.3908\n",
      "Epoch 201/500, Loss: 3.3791\n",
      "Epoch 211/500, Loss: 3.3698\n",
      "Epoch 221/500, Loss: 3.3614\n",
      "Epoch 231/500, Loss: 3.3531\n",
      "Epoch 241/500, Loss: 3.3442\n",
      "Epoch 251/500, Loss: 3.3365\n",
      "Epoch 261/500, Loss: 3.3324\n",
      "Epoch 271/500, Loss: 3.3253\n",
      "Epoch 281/500, Loss: 3.3191\n",
      "Epoch 291/500, Loss: 3.3155\n",
      "Epoch 301/500, Loss: 3.3107\n",
      "Epoch 311/500, Loss: 3.3042\n",
      "Epoch 321/500, Loss: 3.3011\n",
      "Epoch 331/500, Loss: 3.2963\n",
      "Epoch 341/500, Loss: 3.2922\n",
      "Epoch 351/500, Loss: 3.2890\n",
      "Epoch 361/500, Loss: 3.2847\n",
      "Epoch 371/500, Loss: 3.2821\n",
      "Epoch 381/500, Loss: 3.2789\n",
      "Epoch 391/500, Loss: 3.2751\n",
      "Epoch 401/500, Loss: 3.2720\n",
      "Epoch 411/500, Loss: 3.2683\n",
      "Epoch 421/500, Loss: 3.2662\n",
      "Epoch 431/500, Loss: 3.2624\n",
      "Epoch 441/500, Loss: 3.2589\n",
      "Epoch 451/500, Loss: 3.2573\n",
      "Epoch 461/500, Loss: 3.2560\n",
      "Epoch 471/500, Loss: 3.2523\n",
      "Epoch 481/500, Loss: 3.2503\n",
      "Epoch 491/500, Loss: 3.2478\n",
      "Epoch 1/500, Loss: 52.6031\n",
      "Epoch 11/500, Loss: 5.3077\n",
      "Epoch 21/500, Loss: 4.5582\n",
      "Epoch 31/500, Loss: 4.1414\n",
      "Epoch 41/500, Loss: 3.9616\n",
      "Epoch 51/500, Loss: 3.8601\n",
      "Epoch 61/500, Loss: 3.7914\n",
      "Epoch 71/500, Loss: 3.7393\n",
      "Epoch 81/500, Loss: 3.6890\n",
      "Epoch 91/500, Loss: 3.6547\n",
      "Epoch 101/500, Loss: 3.6207\n",
      "Epoch 111/500, Loss: 3.5894\n",
      "Epoch 121/500, Loss: 3.5661\n",
      "Epoch 131/500, Loss: 3.5415\n",
      "Epoch 141/500, Loss: 3.5184\n",
      "Epoch 151/500, Loss: 3.4987\n",
      "Epoch 161/500, Loss: 3.4848\n",
      "Epoch 171/500, Loss: 3.4701\n",
      "Epoch 181/500, Loss: 3.4557\n",
      "Epoch 191/500, Loss: 3.4437\n",
      "Epoch 201/500, Loss: 3.4299\n",
      "Epoch 211/500, Loss: 3.4173\n",
      "Epoch 221/500, Loss: 3.4074\n",
      "Epoch 231/500, Loss: 3.4008\n",
      "Epoch 241/500, Loss: 3.3899\n",
      "Epoch 251/500, Loss: 3.3766\n",
      "Epoch 261/500, Loss: 3.3714\n",
      "Epoch 271/500, Loss: 3.3641\n",
      "Epoch 281/500, Loss: 3.3572\n",
      "Epoch 291/500, Loss: 3.3521\n",
      "Epoch 301/500, Loss: 3.3445\n",
      "Epoch 311/500, Loss: 3.3418\n",
      "Epoch 321/500, Loss: 3.3357\n",
      "Epoch 331/500, Loss: 3.3285\n",
      "Epoch 341/500, Loss: 3.3270\n",
      "Epoch 351/500, Loss: 3.3204\n",
      "Epoch 361/500, Loss: 3.3149\n",
      "Epoch 371/500, Loss: 3.3100\n",
      "Epoch 381/500, Loss: 3.3056\n",
      "Epoch 391/500, Loss: 3.3012\n",
      "Epoch 401/500, Loss: 3.2965\n",
      "Epoch 411/500, Loss: 3.2939\n",
      "Epoch 421/500, Loss: 3.2885\n",
      "Epoch 431/500, Loss: 3.2851\n",
      "Epoch 441/500, Loss: 3.2836\n",
      "Epoch 451/500, Loss: 3.2800\n",
      "Epoch 461/500, Loss: 3.2763\n",
      "Epoch 471/500, Loss: 3.2717\n",
      "Epoch 481/500, Loss: 3.2707\n",
      "Epoch 491/500, Loss: 3.2665\n",
      "Epoch 1/500, Loss: 37.0989\n",
      "Epoch 11/500, Loss: 5.2871\n",
      "Epoch 21/500, Loss: 4.4482\n",
      "Epoch 31/500, Loss: 4.0610\n",
      "Epoch 41/500, Loss: 3.8939\n",
      "Epoch 51/500, Loss: 3.7895\n",
      "Epoch 61/500, Loss: 3.7184\n",
      "Epoch 71/500, Loss: 3.6595\n",
      "Epoch 81/500, Loss: 3.6175\n",
      "Epoch 91/500, Loss: 3.5778\n",
      "Epoch 101/500, Loss: 3.5497\n",
      "Epoch 111/500, Loss: 3.5237\n",
      "Epoch 121/500, Loss: 3.5048\n",
      "Epoch 131/500, Loss: 3.4826\n",
      "Epoch 141/500, Loss: 3.4670\n",
      "Epoch 151/500, Loss: 3.4528\n",
      "Epoch 161/500, Loss: 3.4397\n",
      "Epoch 171/500, Loss: 3.4328\n",
      "Epoch 181/500, Loss: 3.4218\n",
      "Epoch 191/500, Loss: 3.4080\n",
      "Epoch 201/500, Loss: 3.3975\n",
      "Epoch 211/500, Loss: 3.3862\n",
      "Epoch 221/500, Loss: 3.3768\n",
      "Epoch 231/500, Loss: 3.3679\n",
      "Epoch 241/500, Loss: 3.3562\n",
      "Epoch 251/500, Loss: 3.3463\n",
      "Epoch 261/500, Loss: 3.3419\n",
      "Epoch 271/500, Loss: 3.3328\n",
      "Epoch 281/500, Loss: 3.3296\n",
      "Epoch 291/500, Loss: 3.3211\n",
      "Epoch 301/500, Loss: 3.3187\n",
      "Epoch 311/500, Loss: 3.3132\n",
      "Epoch 321/500, Loss: 3.3086\n",
      "Epoch 331/500, Loss: 3.3032\n",
      "Epoch 341/500, Loss: 3.2999\n",
      "Epoch 351/500, Loss: 3.2950\n",
      "Epoch 361/500, Loss: 3.2895\n",
      "Epoch 371/500, Loss: 3.2859\n",
      "Epoch 381/500, Loss: 3.2822\n",
      "Epoch 391/500, Loss: 3.2789\n",
      "Epoch 401/500, Loss: 3.2769\n",
      "Epoch 411/500, Loss: 3.2730\n",
      "Epoch 421/500, Loss: 3.2706\n",
      "Epoch 431/500, Loss: 3.2644\n",
      "Epoch 441/500, Loss: 3.2619\n",
      "Epoch 451/500, Loss: 3.2612\n",
      "Epoch 461/500, Loss: 3.2567\n",
      "Epoch 471/500, Loss: 3.2551\n",
      "Epoch 481/500, Loss: 3.2505\n",
      "Epoch 491/500, Loss: 3.2489\n",
      "Epoch 1/500, Loss: 34.3029\n",
      "Epoch 11/500, Loss: 5.0108\n",
      "Epoch 21/500, Loss: 4.1153\n",
      "Epoch 31/500, Loss: 3.8586\n",
      "Epoch 41/500, Loss: 3.7361\n",
      "Epoch 51/500, Loss: 3.6491\n",
      "Epoch 61/500, Loss: 3.5866\n",
      "Epoch 71/500, Loss: 3.5457\n",
      "Epoch 81/500, Loss: 3.5166\n",
      "Epoch 91/500, Loss: 3.4902\n",
      "Epoch 101/500, Loss: 3.4689\n",
      "Epoch 111/500, Loss: 3.4493\n",
      "Epoch 121/500, Loss: 3.4415\n",
      "Epoch 131/500, Loss: 3.4283\n",
      "Epoch 141/500, Loss: 3.4202\n",
      "Epoch 151/500, Loss: 3.4105\n",
      "Epoch 161/500, Loss: 3.4035\n",
      "Epoch 171/500, Loss: 3.3945\n",
      "Epoch 181/500, Loss: 3.3882\n",
      "Epoch 191/500, Loss: 3.3786\n",
      "Epoch 201/500, Loss: 3.3738\n",
      "Epoch 211/500, Loss: 3.3665\n",
      "Epoch 221/500, Loss: 3.3605\n",
      "Epoch 231/500, Loss: 3.3528\n",
      "Epoch 241/500, Loss: 3.3519\n",
      "Epoch 251/500, Loss: 3.3431\n",
      "Epoch 261/500, Loss: 3.3385\n",
      "Epoch 271/500, Loss: 3.3337\n",
      "Epoch 281/500, Loss: 3.3281\n",
      "Epoch 291/500, Loss: 3.3215\n",
      "Epoch 301/500, Loss: 3.3169\n",
      "Epoch 311/500, Loss: 3.3122\n",
      "Epoch 321/500, Loss: 3.3077\n",
      "Epoch 331/500, Loss: 3.3041\n",
      "Epoch 341/500, Loss: 3.3009\n",
      "Epoch 351/500, Loss: 3.2966\n",
      "Epoch 361/500, Loss: 3.2906\n",
      "Epoch 371/500, Loss: 3.2886\n",
      "Epoch 381/500, Loss: 3.2854\n",
      "Epoch 391/500, Loss: 3.2818\n",
      "Epoch 401/500, Loss: 3.2778\n",
      "Epoch 411/500, Loss: 3.2751\n",
      "Epoch 421/500, Loss: 3.2716\n",
      "Epoch 431/500, Loss: 3.2705\n",
      "Epoch 441/500, Loss: 3.2676\n",
      "Epoch 451/500, Loss: 3.2636\n",
      "Epoch 461/500, Loss: 3.2611\n",
      "Epoch 471/500, Loss: 3.2589\n",
      "Epoch 481/500, Loss: 3.2562\n",
      "Epoch 491/500, Loss: 3.2535\n",
      "Epoch 1/500, Loss: 41.2199\n",
      "Epoch 11/500, Loss: 5.0610\n",
      "Epoch 21/500, Loss: 4.2039\n",
      "Epoch 31/500, Loss: 3.9002\n",
      "Epoch 41/500, Loss: 3.7677\n",
      "Epoch 51/500, Loss: 3.6896\n",
      "Epoch 61/500, Loss: 3.6266\n",
      "Epoch 71/500, Loss: 3.5851\n",
      "Epoch 81/500, Loss: 3.5484\n",
      "Epoch 91/500, Loss: 3.5163\n",
      "Epoch 101/500, Loss: 3.4943\n",
      "Epoch 111/500, Loss: 3.4792\n",
      "Epoch 121/500, Loss: 3.4614\n",
      "Epoch 131/500, Loss: 3.4468\n",
      "Epoch 141/500, Loss: 3.4368\n",
      "Epoch 151/500, Loss: 3.4227\n",
      "Epoch 161/500, Loss: 3.4138\n",
      "Epoch 171/500, Loss: 3.4058\n",
      "Epoch 181/500, Loss: 3.3924\n",
      "Epoch 191/500, Loss: 3.3831\n",
      "Epoch 201/500, Loss: 3.3789\n",
      "Epoch 211/500, Loss: 3.3673\n",
      "Epoch 221/500, Loss: 3.3634\n",
      "Epoch 231/500, Loss: 3.3537\n",
      "Epoch 241/500, Loss: 3.3525\n",
      "Epoch 251/500, Loss: 3.3426\n",
      "Epoch 261/500, Loss: 3.3348\n",
      "Epoch 271/500, Loss: 3.3305\n",
      "Epoch 281/500, Loss: 3.3259\n",
      "Epoch 291/500, Loss: 3.3194\n",
      "Epoch 301/500, Loss: 3.3184\n",
      "Epoch 311/500, Loss: 3.3098\n",
      "Epoch 321/500, Loss: 3.3068\n",
      "Epoch 331/500, Loss: 3.3028\n",
      "Epoch 341/500, Loss: 3.2987\n",
      "Epoch 351/500, Loss: 3.2936\n",
      "Epoch 361/500, Loss: 3.2906\n",
      "Epoch 371/500, Loss: 3.2852\n",
      "Epoch 381/500, Loss: 3.2838\n",
      "Epoch 391/500, Loss: 3.2788\n",
      "Epoch 401/500, Loss: 3.2780\n",
      "Epoch 411/500, Loss: 3.2738\n",
      "Epoch 421/500, Loss: 3.2697\n",
      "Epoch 431/500, Loss: 3.2666\n",
      "Epoch 441/500, Loss: 3.2639\n",
      "Epoch 451/500, Loss: 3.2613\n",
      "Epoch 461/500, Loss: 3.2577\n",
      "Epoch 471/500, Loss: 3.2560\n",
      "Epoch 481/500, Loss: 3.2527\n",
      "Epoch 491/500, Loss: 3.2500\n",
      "Epoch 1/500, Loss: 38.0034\n",
      "Epoch 11/500, Loss: 5.1333\n",
      "Epoch 21/500, Loss: 4.2606\n",
      "Epoch 31/500, Loss: 3.9574\n",
      "Epoch 41/500, Loss: 3.8139\n",
      "Epoch 51/500, Loss: 3.7356\n",
      "Epoch 61/500, Loss: 3.6731\n",
      "Epoch 71/500, Loss: 3.6292\n",
      "Epoch 81/500, Loss: 3.5904\n",
      "Epoch 91/500, Loss: 3.5586\n",
      "Epoch 101/500, Loss: 3.5359\n",
      "Epoch 111/500, Loss: 3.5082\n",
      "Epoch 121/500, Loss: 3.4919\n",
      "Epoch 131/500, Loss: 3.4756\n",
      "Epoch 141/500, Loss: 3.4558\n",
      "Epoch 151/500, Loss: 3.4433\n",
      "Epoch 161/500, Loss: 3.4324\n",
      "Epoch 171/500, Loss: 3.4188\n",
      "Epoch 181/500, Loss: 3.4080\n",
      "Epoch 191/500, Loss: 3.3960\n",
      "Epoch 201/500, Loss: 3.3867\n",
      "Epoch 211/500, Loss: 3.3779\n",
      "Epoch 221/500, Loss: 3.3696\n",
      "Epoch 231/500, Loss: 3.3657\n",
      "Epoch 241/500, Loss: 3.3602\n",
      "Epoch 251/500, Loss: 3.3491\n",
      "Epoch 261/500, Loss: 3.3424\n",
      "Epoch 271/500, Loss: 3.3380\n",
      "Epoch 281/500, Loss: 3.3319\n",
      "Epoch 291/500, Loss: 3.3270\n",
      "Epoch 301/500, Loss: 3.3206\n",
      "Epoch 311/500, Loss: 3.3169\n",
      "Epoch 321/500, Loss: 3.3135\n",
      "Epoch 331/500, Loss: 3.3076\n",
      "Epoch 341/500, Loss: 3.3035\n",
      "Epoch 351/500, Loss: 3.3007\n",
      "Epoch 361/500, Loss: 3.2954\n",
      "Epoch 371/500, Loss: 3.2931\n",
      "Epoch 381/500, Loss: 3.2881\n",
      "Epoch 391/500, Loss: 3.2846\n",
      "Epoch 401/500, Loss: 3.2823\n",
      "Epoch 411/500, Loss: 3.2769\n",
      "Epoch 421/500, Loss: 3.2738\n",
      "Epoch 431/500, Loss: 3.2699\n",
      "Epoch 441/500, Loss: 3.2667\n",
      "Epoch 451/500, Loss: 3.2650\n",
      "Epoch 461/500, Loss: 3.2613\n",
      "Epoch 471/500, Loss: 3.2580\n",
      "Epoch 481/500, Loss: 3.2560\n",
      "Epoch 491/500, Loss: 3.2544\n",
      "Epoch 1/500, Loss: 35.1669\n",
      "Epoch 11/500, Loss: 4.8879\n",
      "Epoch 21/500, Loss: 4.2550\n",
      "Epoch 31/500, Loss: 3.9997\n",
      "Epoch 41/500, Loss: 3.8628\n",
      "Epoch 51/500, Loss: 3.7808\n",
      "Epoch 61/500, Loss: 3.7174\n",
      "Epoch 71/500, Loss: 3.6766\n",
      "Epoch 81/500, Loss: 3.6365\n",
      "Epoch 91/500, Loss: 3.6030\n",
      "Epoch 101/500, Loss: 3.5710\n",
      "Epoch 111/500, Loss: 3.5465\n",
      "Epoch 121/500, Loss: 3.5215\n",
      "Epoch 131/500, Loss: 3.5009\n",
      "Epoch 141/500, Loss: 3.4835\n",
      "Epoch 151/500, Loss: 3.4655\n",
      "Epoch 161/500, Loss: 3.4497\n",
      "Epoch 171/500, Loss: 3.4337\n",
      "Epoch 181/500, Loss: 3.4225\n",
      "Epoch 191/500, Loss: 3.4061\n",
      "Epoch 201/500, Loss: 3.3953\n",
      "Epoch 211/500, Loss: 3.3863\n",
      "Epoch 221/500, Loss: 3.3784\n",
      "Epoch 231/500, Loss: 3.3696\n",
      "Epoch 241/500, Loss: 3.3606\n",
      "Epoch 251/500, Loss: 3.3541\n",
      "Epoch 261/500, Loss: 3.3485\n",
      "Epoch 271/500, Loss: 3.3416\n",
      "Epoch 281/500, Loss: 3.3357\n",
      "Epoch 291/500, Loss: 3.3296\n",
      "Epoch 301/500, Loss: 3.3264\n",
      "Epoch 311/500, Loss: 3.3215\n",
      "Epoch 321/500, Loss: 3.3162\n",
      "Epoch 331/500, Loss: 3.3110\n",
      "Epoch 341/500, Loss: 3.3060\n",
      "Epoch 351/500, Loss: 3.3046\n",
      "Epoch 361/500, Loss: 3.3005\n",
      "Epoch 371/500, Loss: 3.2959\n",
      "Epoch 381/500, Loss: 3.2918\n",
      "Epoch 391/500, Loss: 3.2889\n",
      "Epoch 401/500, Loss: 3.2857\n",
      "Epoch 411/500, Loss: 3.2832\n",
      "Epoch 421/500, Loss: 3.2784\n",
      "Epoch 431/500, Loss: 3.2760\n",
      "Epoch 441/500, Loss: 3.2731\n",
      "Epoch 451/500, Loss: 3.2711\n",
      "Epoch 461/500, Loss: 3.2677\n",
      "Epoch 471/500, Loss: 3.2638\n",
      "Epoch 481/500, Loss: 3.2604\n",
      "Epoch 491/500, Loss: 3.2599\n",
      "Epoch 1/500, Loss: 32.1940\n",
      "Epoch 11/500, Loss: 4.5644\n",
      "Epoch 21/500, Loss: 3.9244\n",
      "Epoch 31/500, Loss: 3.7820\n",
      "Epoch 41/500, Loss: 3.7046\n",
      "Epoch 51/500, Loss: 3.6540\n",
      "Epoch 61/500, Loss: 3.6078\n",
      "Epoch 71/500, Loss: 3.5684\n",
      "Epoch 81/500, Loss: 3.5364\n",
      "Epoch 91/500, Loss: 3.5064\n",
      "Epoch 101/500, Loss: 3.4837\n",
      "Epoch 111/500, Loss: 3.4656\n",
      "Epoch 121/500, Loss: 3.4495\n",
      "Epoch 131/500, Loss: 3.4371\n",
      "Epoch 141/500, Loss: 3.4222\n",
      "Epoch 151/500, Loss: 3.4104\n",
      "Epoch 161/500, Loss: 3.4005\n",
      "Epoch 171/500, Loss: 3.3915\n",
      "Epoch 181/500, Loss: 3.3833\n",
      "Epoch 191/500, Loss: 3.3745\n",
      "Epoch 201/500, Loss: 3.3641\n",
      "Epoch 211/500, Loss: 3.3568\n",
      "Epoch 221/500, Loss: 3.3495\n",
      "Epoch 231/500, Loss: 3.3409\n",
      "Epoch 241/500, Loss: 3.3372\n",
      "Epoch 251/500, Loss: 3.3287\n",
      "Epoch 261/500, Loss: 3.3248\n",
      "Epoch 271/500, Loss: 3.3179\n",
      "Epoch 281/500, Loss: 3.3113\n",
      "Epoch 291/500, Loss: 3.3091\n",
      "Epoch 301/500, Loss: 3.3043\n",
      "Epoch 311/500, Loss: 3.2990\n",
      "Epoch 321/500, Loss: 3.2942\n",
      "Epoch 331/500, Loss: 3.2910\n",
      "Epoch 341/500, Loss: 3.2870\n",
      "Epoch 351/500, Loss: 3.2838\n",
      "Epoch 361/500, Loss: 3.2795\n",
      "Epoch 371/500, Loss: 3.2762\n",
      "Epoch 381/500, Loss: 3.2725\n",
      "Epoch 391/500, Loss: 3.2693\n",
      "Epoch 401/500, Loss: 3.2658\n",
      "Epoch 411/500, Loss: 3.2625\n",
      "Epoch 421/500, Loss: 3.2610\n",
      "Epoch 431/500, Loss: 3.2573\n",
      "Epoch 441/500, Loss: 3.2550\n",
      "Epoch 451/500, Loss: 3.2517\n",
      "Epoch 461/500, Loss: 3.2478\n",
      "Epoch 471/500, Loss: 3.2471\n",
      "Epoch 481/500, Loss: 3.2436\n",
      "Epoch 491/500, Loss: 3.2413\n",
      "Epoch 1/500, Loss: 39.5081\n",
      "Epoch 11/500, Loss: 5.2356\n",
      "Epoch 21/500, Loss: 4.3674\n",
      "Epoch 31/500, Loss: 3.9755\n",
      "Epoch 41/500, Loss: 3.8199\n",
      "Epoch 51/500, Loss: 3.7324\n",
      "Epoch 61/500, Loss: 3.6784\n",
      "Epoch 71/500, Loss: 3.6286\n",
      "Epoch 81/500, Loss: 3.5904\n",
      "Epoch 91/500, Loss: 3.5603\n",
      "Epoch 101/500, Loss: 3.5269\n",
      "Epoch 111/500, Loss: 3.5030\n",
      "Epoch 121/500, Loss: 3.4825\n",
      "Epoch 131/500, Loss: 3.4652\n",
      "Epoch 141/500, Loss: 3.4495\n",
      "Epoch 151/500, Loss: 3.4381\n",
      "Epoch 161/500, Loss: 3.4227\n",
      "Epoch 171/500, Loss: 3.4113\n",
      "Epoch 181/500, Loss: 3.4019\n",
      "Epoch 191/500, Loss: 3.3887\n",
      "Epoch 201/500, Loss: 3.3786\n",
      "Epoch 211/500, Loss: 3.3660\n",
      "Epoch 221/500, Loss: 3.3582\n",
      "Epoch 231/500, Loss: 3.3564\n",
      "Epoch 241/500, Loss: 3.3477\n",
      "Epoch 251/500, Loss: 3.3398\n",
      "Epoch 261/500, Loss: 3.3353\n",
      "Epoch 271/500, Loss: 3.3310\n",
      "Epoch 281/500, Loss: 3.3240\n",
      "Epoch 291/500, Loss: 3.3190\n",
      "Epoch 301/500, Loss: 3.3155\n",
      "Epoch 311/500, Loss: 3.3096\n",
      "Epoch 321/500, Loss: 3.3071\n",
      "Epoch 331/500, Loss: 3.3020\n",
      "Epoch 341/500, Loss: 3.2983\n",
      "Epoch 351/500, Loss: 3.2939\n",
      "Epoch 361/500, Loss: 3.2882\n",
      "Epoch 371/500, Loss: 3.2886\n",
      "Epoch 381/500, Loss: 3.2845\n",
      "Epoch 391/500, Loss: 3.2779\n",
      "Epoch 401/500, Loss: 3.2730\n",
      "Epoch 411/500, Loss: 3.2717\n",
      "Epoch 421/500, Loss: 3.2667\n",
      "Epoch 431/500, Loss: 3.2646\n",
      "Epoch 441/500, Loss: 3.2616\n",
      "Epoch 451/500, Loss: 3.2600\n",
      "Epoch 461/500, Loss: 3.2553\n",
      "Epoch 471/500, Loss: 3.2540\n",
      "Epoch 481/500, Loss: 3.2517\n",
      "Epoch 491/500, Loss: 3.2483\n",
      "Epoch 1/500, Loss: 42.6611\n",
      "Epoch 11/500, Loss: 5.0195\n",
      "Epoch 21/500, Loss: 4.0872\n",
      "Epoch 31/500, Loss: 3.8512\n",
      "Epoch 41/500, Loss: 3.7439\n",
      "Epoch 51/500, Loss: 3.6763\n",
      "Epoch 61/500, Loss: 3.6317\n",
      "Epoch 71/500, Loss: 3.5985\n",
      "Epoch 81/500, Loss: 3.5656\n",
      "Epoch 91/500, Loss: 3.5373\n",
      "Epoch 101/500, Loss: 3.5173\n",
      "Epoch 111/500, Loss: 3.4972\n",
      "Epoch 121/500, Loss: 3.4804\n",
      "Epoch 131/500, Loss: 3.4619\n",
      "Epoch 141/500, Loss: 3.4525\n",
      "Epoch 151/500, Loss: 3.4376\n",
      "Epoch 161/500, Loss: 3.4280\n",
      "Epoch 171/500, Loss: 3.4161\n",
      "Epoch 181/500, Loss: 3.4068\n",
      "Epoch 191/500, Loss: 3.3939\n",
      "Epoch 201/500, Loss: 3.3864\n",
      "Epoch 211/500, Loss: 3.3757\n",
      "Epoch 221/500, Loss: 3.3686\n",
      "Epoch 231/500, Loss: 3.3618\n",
      "Epoch 241/500, Loss: 3.3544\n",
      "Epoch 251/500, Loss: 3.3504\n",
      "Epoch 261/500, Loss: 3.3417\n",
      "Epoch 271/500, Loss: 3.3372\n",
      "Epoch 281/500, Loss: 3.3314\n",
      "Epoch 291/500, Loss: 3.3262\n",
      "Epoch 301/500, Loss: 3.3217\n",
      "Epoch 311/500, Loss: 3.3168\n",
      "Epoch 321/500, Loss: 3.3134\n",
      "Epoch 331/500, Loss: 3.3089\n",
      "Epoch 341/500, Loss: 3.3052\n",
      "Epoch 351/500, Loss: 3.3002\n",
      "Epoch 361/500, Loss: 3.2962\n",
      "Epoch 371/500, Loss: 3.2931\n",
      "Epoch 381/500, Loss: 3.2892\n",
      "Epoch 391/500, Loss: 3.2867\n",
      "Epoch 401/500, Loss: 3.2816\n",
      "Epoch 411/500, Loss: 3.2799\n",
      "Epoch 421/500, Loss: 3.2767\n",
      "Epoch 431/500, Loss: 3.2733\n",
      "Epoch 441/500, Loss: 3.2696\n",
      "Epoch 451/500, Loss: 3.2671\n",
      "Epoch 461/500, Loss: 3.2640\n",
      "Epoch 471/500, Loss: 3.2621\n",
      "Epoch 481/500, Loss: 3.2600\n",
      "Epoch 491/500, Loss: 3.2584\n",
      "Epoch 1/500, Loss: 41.8989\n",
      "Epoch 11/500, Loss: 5.0576\n",
      "Epoch 21/500, Loss: 4.2930\n",
      "Epoch 31/500, Loss: 4.0544\n",
      "Epoch 41/500, Loss: 3.9175\n",
      "Epoch 51/500, Loss: 3.8410\n",
      "Epoch 61/500, Loss: 3.7736\n",
      "Epoch 71/500, Loss: 3.7117\n",
      "Epoch 81/500, Loss: 3.6682\n",
      "Epoch 91/500, Loss: 3.6344\n",
      "Epoch 101/500, Loss: 3.5958\n",
      "Epoch 111/500, Loss: 3.5693\n",
      "Epoch 121/500, Loss: 3.5435\n",
      "Epoch 131/500, Loss: 3.5229\n",
      "Epoch 141/500, Loss: 3.5078\n",
      "Epoch 151/500, Loss: 3.4914\n",
      "Epoch 161/500, Loss: 3.4753\n",
      "Epoch 171/500, Loss: 3.4639\n",
      "Epoch 181/500, Loss: 3.4468\n",
      "Epoch 191/500, Loss: 3.4296\n",
      "Epoch 201/500, Loss: 3.4203\n",
      "Epoch 211/500, Loss: 3.4053\n",
      "Epoch 221/500, Loss: 3.3978\n",
      "Epoch 231/500, Loss: 3.3896\n",
      "Epoch 241/500, Loss: 3.3788\n",
      "Epoch 251/500, Loss: 3.3694\n",
      "Epoch 261/500, Loss: 3.3622\n",
      "Epoch 271/500, Loss: 3.3570\n",
      "Epoch 281/500, Loss: 3.3490\n",
      "Epoch 291/500, Loss: 3.3433\n",
      "Epoch 301/500, Loss: 3.3355\n",
      "Epoch 311/500, Loss: 3.3313\n",
      "Epoch 321/500, Loss: 3.3259\n",
      "Epoch 331/500, Loss: 3.3193\n",
      "Epoch 341/500, Loss: 3.3141\n",
      "Epoch 351/500, Loss: 3.3078\n",
      "Epoch 361/500, Loss: 3.3061\n",
      "Epoch 371/500, Loss: 3.2994\n",
      "Epoch 381/500, Loss: 3.2963\n",
      "Epoch 391/500, Loss: 3.2899\n",
      "Epoch 401/500, Loss: 3.2866\n",
      "Epoch 411/500, Loss: 3.2844\n",
      "Epoch 421/500, Loss: 3.2803\n",
      "Epoch 431/500, Loss: 3.2748\n",
      "Epoch 441/500, Loss: 3.2728\n",
      "Epoch 451/500, Loss: 3.2697\n",
      "Epoch 461/500, Loss: 3.2663\n",
      "Epoch 471/500, Loss: 3.2629\n",
      "Epoch 481/500, Loss: 3.2601\n",
      "Epoch 491/500, Loss: 3.2568\n",
      "Epoch 1/500, Loss: 35.0678\n",
      "Epoch 11/500, Loss: 5.1438\n",
      "Epoch 21/500, Loss: 4.3942\n",
      "Epoch 31/500, Loss: 3.9924\n",
      "Epoch 41/500, Loss: 3.8226\n",
      "Epoch 51/500, Loss: 3.7396\n",
      "Epoch 61/500, Loss: 3.6805\n",
      "Epoch 71/500, Loss: 3.6239\n",
      "Epoch 81/500, Loss: 3.5908\n",
      "Epoch 91/500, Loss: 3.5575\n",
      "Epoch 101/500, Loss: 3.5337\n",
      "Epoch 111/500, Loss: 3.5092\n",
      "Epoch 121/500, Loss: 3.4856\n",
      "Epoch 131/500, Loss: 3.4659\n",
      "Epoch 141/500, Loss: 3.4487\n",
      "Epoch 151/500, Loss: 3.4312\n",
      "Epoch 161/500, Loss: 3.4194\n",
      "Epoch 171/500, Loss: 3.4081\n",
      "Epoch 181/500, Loss: 3.4000\n",
      "Epoch 191/500, Loss: 3.3944\n",
      "Epoch 201/500, Loss: 3.3861\n",
      "Epoch 211/500, Loss: 3.3757\n",
      "Epoch 221/500, Loss: 3.3694\n",
      "Epoch 231/500, Loss: 3.3617\n",
      "Epoch 241/500, Loss: 3.3547\n",
      "Epoch 251/500, Loss: 3.3466\n",
      "Epoch 261/500, Loss: 3.3427\n",
      "Epoch 271/500, Loss: 3.3353\n",
      "Epoch 281/500, Loss: 3.3306\n",
      "Epoch 291/500, Loss: 3.3265\n",
      "Epoch 301/500, Loss: 3.3190\n",
      "Epoch 311/500, Loss: 3.3155\n",
      "Epoch 321/500, Loss: 3.3107\n",
      "Epoch 331/500, Loss: 3.3064\n",
      "Epoch 341/500, Loss: 3.3017\n",
      "Epoch 351/500, Loss: 3.2975\n",
      "Epoch 361/500, Loss: 3.2929\n",
      "Epoch 371/500, Loss: 3.2893\n",
      "Epoch 381/500, Loss: 3.2863\n",
      "Epoch 391/500, Loss: 3.2816\n",
      "Epoch 401/500, Loss: 3.2785\n",
      "Epoch 411/500, Loss: 3.2741\n",
      "Epoch 421/500, Loss: 3.2713\n",
      "Epoch 431/500, Loss: 3.2670\n",
      "Epoch 441/500, Loss: 3.2670\n",
      "Epoch 451/500, Loss: 3.2616\n",
      "Epoch 461/500, Loss: 3.2610\n",
      "Epoch 471/500, Loss: 3.2569\n",
      "Epoch 481/500, Loss: 3.2548\n",
      "Epoch 491/500, Loss: 3.2525\n",
      "Epoch 1/500, Loss: 46.8003\n",
      "Epoch 11/500, Loss: 5.2106\n",
      "Epoch 21/500, Loss: 4.4119\n",
      "Epoch 31/500, Loss: 4.0402\n",
      "Epoch 41/500, Loss: 3.8886\n",
      "Epoch 51/500, Loss: 3.8048\n",
      "Epoch 61/500, Loss: 3.7481\n",
      "Epoch 71/500, Loss: 3.7050\n",
      "Epoch 81/500, Loss: 3.6610\n",
      "Epoch 91/500, Loss: 3.6315\n",
      "Epoch 101/500, Loss: 3.6044\n",
      "Epoch 111/500, Loss: 3.5744\n",
      "Epoch 121/500, Loss: 3.5456\n",
      "Epoch 131/500, Loss: 3.5234\n",
      "Epoch 141/500, Loss: 3.5036\n",
      "Epoch 151/500, Loss: 3.4851\n",
      "Epoch 161/500, Loss: 3.4670\n",
      "Epoch 171/500, Loss: 3.4533\n",
      "Epoch 181/500, Loss: 3.4423\n",
      "Epoch 191/500, Loss: 3.4291\n",
      "Epoch 201/500, Loss: 3.4176\n",
      "Epoch 211/500, Loss: 3.4050\n",
      "Epoch 221/500, Loss: 3.3989\n",
      "Epoch 231/500, Loss: 3.3894\n",
      "Epoch 241/500, Loss: 3.3813\n",
      "Epoch 251/500, Loss: 3.3714\n",
      "Epoch 261/500, Loss: 3.3644\n",
      "Epoch 271/500, Loss: 3.3576\n",
      "Epoch 281/500, Loss: 3.3491\n",
      "Epoch 291/500, Loss: 3.3448\n",
      "Epoch 301/500, Loss: 3.3371\n",
      "Epoch 311/500, Loss: 3.3322\n",
      "Epoch 321/500, Loss: 3.3268\n",
      "Epoch 331/500, Loss: 3.3219\n",
      "Epoch 341/500, Loss: 3.3160\n",
      "Epoch 351/500, Loss: 3.3122\n",
      "Epoch 361/500, Loss: 3.3082\n",
      "Epoch 371/500, Loss: 3.3029\n",
      "Epoch 381/500, Loss: 3.3000\n",
      "Epoch 391/500, Loss: 3.2964\n",
      "Epoch 401/500, Loss: 3.2934\n",
      "Epoch 411/500, Loss: 3.2899\n",
      "Epoch 421/500, Loss: 3.2858\n",
      "Epoch 431/500, Loss: 3.2840\n",
      "Epoch 441/500, Loss: 3.2798\n",
      "Epoch 451/500, Loss: 3.2769\n",
      "Epoch 461/500, Loss: 3.2743\n",
      "Epoch 471/500, Loss: 3.2707\n",
      "Epoch 481/500, Loss: 3.2700\n",
      "Epoch 491/500, Loss: 3.2641\n",
      "Epoch 1/500, Loss: 39.1560\n",
      "Epoch 11/500, Loss: 5.1779\n",
      "Epoch 21/500, Loss: 4.2887\n",
      "Epoch 31/500, Loss: 3.9675\n",
      "Epoch 41/500, Loss: 3.8332\n",
      "Epoch 51/500, Loss: 3.7586\n",
      "Epoch 61/500, Loss: 3.6980\n",
      "Epoch 71/500, Loss: 3.6507\n",
      "Epoch 81/500, Loss: 3.6108\n",
      "Epoch 91/500, Loss: 3.5756\n",
      "Epoch 101/500, Loss: 3.5511\n",
      "Epoch 111/500, Loss: 3.5287\n",
      "Epoch 121/500, Loss: 3.5107\n",
      "Epoch 131/500, Loss: 3.4925\n",
      "Epoch 141/500, Loss: 3.4780\n",
      "Epoch 151/500, Loss: 3.4656\n",
      "Epoch 161/500, Loss: 3.4509\n",
      "Epoch 171/500, Loss: 3.4327\n",
      "Epoch 181/500, Loss: 3.4224\n",
      "Epoch 191/500, Loss: 3.4114\n",
      "Epoch 201/500, Loss: 3.4038\n",
      "Epoch 211/500, Loss: 3.3921\n",
      "Epoch 221/500, Loss: 3.3826\n",
      "Epoch 231/500, Loss: 3.3766\n",
      "Epoch 241/500, Loss: 3.3693\n",
      "Epoch 251/500, Loss: 3.3641\n",
      "Epoch 261/500, Loss: 3.3545\n",
      "Epoch 271/500, Loss: 3.3464\n",
      "Epoch 281/500, Loss: 3.3405\n",
      "Epoch 291/500, Loss: 3.3362\n",
      "Epoch 301/500, Loss: 3.3304\n",
      "Epoch 311/500, Loss: 3.3233\n",
      "Epoch 321/500, Loss: 3.3165\n",
      "Epoch 331/500, Loss: 3.3133\n",
      "Epoch 341/500, Loss: 3.3079\n",
      "Epoch 351/500, Loss: 3.3052\n",
      "Epoch 361/500, Loss: 3.2999\n",
      "Epoch 371/500, Loss: 3.2948\n",
      "Epoch 381/500, Loss: 3.2920\n",
      "Epoch 391/500, Loss: 3.2875\n",
      "Epoch 401/500, Loss: 3.2846\n",
      "Epoch 411/500, Loss: 3.2810\n",
      "Epoch 421/500, Loss: 3.2768\n",
      "Epoch 431/500, Loss: 3.2733\n",
      "Epoch 441/500, Loss: 3.2704\n",
      "Epoch 451/500, Loss: 3.2688\n",
      "Epoch 461/500, Loss: 3.2655\n",
      "Epoch 471/500, Loss: 3.2622\n",
      "Epoch 481/500, Loss: 3.2591\n",
      "Epoch 491/500, Loss: 3.2550\n",
      "Epoch 1/500, Loss: 39.5011\n",
      "Epoch 11/500, Loss: 5.1512\n",
      "Epoch 21/500, Loss: 4.4009\n",
      "Epoch 31/500, Loss: 4.0756\n",
      "Epoch 41/500, Loss: 3.9043\n",
      "Epoch 51/500, Loss: 3.8028\n",
      "Epoch 61/500, Loss: 3.7372\n",
      "Epoch 71/500, Loss: 3.6828\n",
      "Epoch 81/500, Loss: 3.6423\n",
      "Epoch 91/500, Loss: 3.6080\n",
      "Epoch 101/500, Loss: 3.5801\n",
      "Epoch 111/500, Loss: 3.5532\n",
      "Epoch 121/500, Loss: 3.5273\n",
      "Epoch 131/500, Loss: 3.5075\n",
      "Epoch 141/500, Loss: 3.4902\n",
      "Epoch 151/500, Loss: 3.4717\n",
      "Epoch 161/500, Loss: 3.4562\n",
      "Epoch 171/500, Loss: 3.4402\n",
      "Epoch 181/500, Loss: 3.4244\n",
      "Epoch 191/500, Loss: 3.4168\n",
      "Epoch 201/500, Loss: 3.4015\n",
      "Epoch 211/500, Loss: 3.3867\n",
      "Epoch 221/500, Loss: 3.3780\n",
      "Epoch 231/500, Loss: 3.3731\n",
      "Epoch 241/500, Loss: 3.3625\n",
      "Epoch 251/500, Loss: 3.3532\n",
      "Epoch 261/500, Loss: 3.3473\n",
      "Epoch 271/500, Loss: 3.3410\n",
      "Epoch 281/500, Loss: 3.3346\n",
      "Epoch 291/500, Loss: 3.3270\n",
      "Epoch 301/500, Loss: 3.3211\n",
      "Epoch 311/500, Loss: 3.3170\n",
      "Epoch 321/500, Loss: 3.3092\n",
      "Epoch 331/500, Loss: 3.3055\n",
      "Epoch 341/500, Loss: 3.3005\n",
      "Epoch 351/500, Loss: 3.2947\n",
      "Epoch 361/500, Loss: 3.2899\n",
      "Epoch 371/500, Loss: 3.2872\n",
      "Epoch 381/500, Loss: 3.2834\n",
      "Epoch 391/500, Loss: 3.2794\n",
      "Epoch 401/500, Loss: 3.2761\n",
      "Epoch 411/500, Loss: 3.2699\n",
      "Epoch 421/500, Loss: 3.2682\n",
      "Epoch 431/500, Loss: 3.2650\n",
      "Epoch 441/500, Loss: 3.2626\n",
      "Epoch 451/500, Loss: 3.2591\n",
      "Epoch 461/500, Loss: 3.2553\n",
      "Epoch 471/500, Loss: 3.2522\n",
      "Epoch 481/500, Loss: 3.2487\n",
      "Epoch 491/500, Loss: 3.2496\n",
      "Epoch 1/500, Loss: 41.9873\n",
      "Epoch 11/500, Loss: 5.0947\n",
      "Epoch 21/500, Loss: 4.2804\n",
      "Epoch 31/500, Loss: 3.9859\n",
      "Epoch 41/500, Loss: 3.8624\n",
      "Epoch 51/500, Loss: 3.7811\n",
      "Epoch 61/500, Loss: 3.7183\n",
      "Epoch 71/500, Loss: 3.6666\n",
      "Epoch 81/500, Loss: 3.6270\n",
      "Epoch 91/500, Loss: 3.5908\n",
      "Epoch 101/500, Loss: 3.5604\n",
      "Epoch 111/500, Loss: 3.5375\n",
      "Epoch 121/500, Loss: 3.5164\n",
      "Epoch 131/500, Loss: 3.4910\n",
      "Epoch 141/500, Loss: 3.4750\n",
      "Epoch 151/500, Loss: 3.4626\n",
      "Epoch 161/500, Loss: 3.4451\n",
      "Epoch 171/500, Loss: 3.4282\n",
      "Epoch 181/500, Loss: 3.4145\n",
      "Epoch 191/500, Loss: 3.4043\n",
      "Epoch 201/500, Loss: 3.3948\n",
      "Epoch 211/500, Loss: 3.3856\n",
      "Epoch 221/500, Loss: 3.3752\n",
      "Epoch 231/500, Loss: 3.3651\n",
      "Epoch 241/500, Loss: 3.3589\n",
      "Epoch 251/500, Loss: 3.3518\n",
      "Epoch 261/500, Loss: 3.3441\n",
      "Epoch 271/500, Loss: 3.3385\n",
      "Epoch 281/500, Loss: 3.3332\n",
      "Epoch 291/500, Loss: 3.3271\n",
      "Epoch 301/500, Loss: 3.3206\n",
      "Epoch 311/500, Loss: 3.3165\n",
      "Epoch 321/500, Loss: 3.3122\n",
      "Epoch 331/500, Loss: 3.3056\n",
      "Epoch 341/500, Loss: 3.3020\n",
      "Epoch 351/500, Loss: 3.2982\n",
      "Epoch 361/500, Loss: 3.2940\n",
      "Epoch 371/500, Loss: 3.2888\n",
      "Epoch 381/500, Loss: 3.2850\n",
      "Epoch 391/500, Loss: 3.2812\n",
      "Epoch 401/500, Loss: 3.2791\n",
      "Epoch 411/500, Loss: 3.2736\n",
      "Epoch 421/500, Loss: 3.2699\n",
      "Epoch 431/500, Loss: 3.2675\n",
      "Epoch 441/500, Loss: 3.2645\n",
      "Epoch 451/500, Loss: 3.2601\n",
      "Epoch 461/500, Loss: 3.2577\n",
      "Epoch 471/500, Loss: 3.2556\n",
      "Epoch 481/500, Loss: 3.2530\n",
      "Epoch 491/500, Loss: 3.2493\n",
      "Epoch 1/500, Loss: 46.0822\n",
      "Epoch 11/500, Loss: 5.2603\n",
      "Epoch 21/500, Loss: 4.4250\n",
      "Epoch 31/500, Loss: 4.0722\n",
      "Epoch 41/500, Loss: 3.9156\n",
      "Epoch 51/500, Loss: 3.8141\n",
      "Epoch 61/500, Loss: 3.7378\n",
      "Epoch 71/500, Loss: 3.6867\n",
      "Epoch 81/500, Loss: 3.6381\n",
      "Epoch 91/500, Loss: 3.6028\n",
      "Epoch 101/500, Loss: 3.5712\n",
      "Epoch 111/500, Loss: 3.5447\n",
      "Epoch 121/500, Loss: 3.5197\n",
      "Epoch 131/500, Loss: 3.4948\n",
      "Epoch 141/500, Loss: 3.4824\n",
      "Epoch 151/500, Loss: 3.4621\n",
      "Epoch 161/500, Loss: 3.4523\n",
      "Epoch 171/500, Loss: 3.4395\n",
      "Epoch 181/500, Loss: 3.4303\n",
      "Epoch 191/500, Loss: 3.4218\n",
      "Epoch 201/500, Loss: 3.4084\n",
      "Epoch 211/500, Loss: 3.3985\n",
      "Epoch 221/500, Loss: 3.3895\n",
      "Epoch 231/500, Loss: 3.3835\n",
      "Epoch 241/500, Loss: 3.3752\n",
      "Epoch 251/500, Loss: 3.3676\n",
      "Epoch 261/500, Loss: 3.3603\n",
      "Epoch 271/500, Loss: 3.3531\n",
      "Epoch 281/500, Loss: 3.3464\n",
      "Epoch 291/500, Loss: 3.3387\n",
      "Epoch 301/500, Loss: 3.3369\n",
      "Epoch 311/500, Loss: 3.3290\n",
      "Epoch 321/500, Loss: 3.3249\n",
      "Epoch 331/500, Loss: 3.3219\n",
      "Epoch 341/500, Loss: 3.3152\n",
      "Epoch 351/500, Loss: 3.3113\n",
      "Epoch 361/500, Loss: 3.3091\n",
      "Epoch 371/500, Loss: 3.3033\n",
      "Epoch 381/500, Loss: 3.2997\n",
      "Epoch 391/500, Loss: 3.2950\n",
      "Epoch 401/500, Loss: 3.2910\n",
      "Epoch 411/500, Loss: 3.2866\n",
      "Epoch 421/500, Loss: 3.2855\n",
      "Epoch 431/500, Loss: 3.2817\n",
      "Epoch 441/500, Loss: 3.2793\n",
      "Epoch 451/500, Loss: 3.2753\n",
      "Epoch 461/500, Loss: 3.2711\n",
      "Epoch 471/500, Loss: 3.2701\n",
      "Epoch 481/500, Loss: 3.2666\n",
      "Epoch 491/500, Loss: 3.2654\n",
      "Epoch 1/500, Loss: 40.5101\n",
      "Epoch 11/500, Loss: 5.1587\n",
      "Epoch 21/500, Loss: 4.3164\n",
      "Epoch 31/500, Loss: 3.9879\n",
      "Epoch 41/500, Loss: 3.8481\n",
      "Epoch 51/500, Loss: 3.7648\n",
      "Epoch 61/500, Loss: 3.7024\n",
      "Epoch 71/500, Loss: 3.6605\n",
      "Epoch 81/500, Loss: 3.6125\n",
      "Epoch 91/500, Loss: 3.5760\n",
      "Epoch 101/500, Loss: 3.5452\n",
      "Epoch 111/500, Loss: 3.5212\n",
      "Epoch 121/500, Loss: 3.5022\n",
      "Epoch 131/500, Loss: 3.4897\n",
      "Epoch 141/500, Loss: 3.4736\n",
      "Epoch 151/500, Loss: 3.4556\n",
      "Epoch 161/500, Loss: 3.4408\n",
      "Epoch 171/500, Loss: 3.4304\n",
      "Epoch 181/500, Loss: 3.4232\n",
      "Epoch 191/500, Loss: 3.4163\n",
      "Epoch 201/500, Loss: 3.4047\n",
      "Epoch 211/500, Loss: 3.3947\n",
      "Epoch 221/500, Loss: 3.3893\n",
      "Epoch 231/500, Loss: 3.3851\n",
      "Epoch 241/500, Loss: 3.3732\n",
      "Epoch 251/500, Loss: 3.3713\n",
      "Epoch 261/500, Loss: 3.3624\n",
      "Epoch 271/500, Loss: 3.3556\n",
      "Epoch 281/500, Loss: 3.3510\n",
      "Epoch 291/500, Loss: 3.3453\n",
      "Epoch 301/500, Loss: 3.3380\n",
      "Epoch 311/500, Loss: 3.3370\n",
      "Epoch 321/500, Loss: 3.3274\n",
      "Epoch 331/500, Loss: 3.3242\n",
      "Epoch 341/500, Loss: 3.3190\n",
      "Epoch 351/500, Loss: 3.3126\n",
      "Epoch 361/500, Loss: 3.3079\n",
      "Epoch 371/500, Loss: 3.3064\n",
      "Epoch 381/500, Loss: 3.2988\n",
      "Epoch 391/500, Loss: 3.2952\n",
      "Epoch 401/500, Loss: 3.2902\n",
      "Epoch 411/500, Loss: 3.2875\n",
      "Epoch 421/500, Loss: 3.2854\n",
      "Epoch 431/500, Loss: 3.2812\n",
      "Epoch 441/500, Loss: 3.2764\n",
      "Epoch 451/500, Loss: 3.2742\n",
      "Epoch 461/500, Loss: 3.2707\n",
      "Epoch 471/500, Loss: 3.2676\n",
      "Epoch 481/500, Loss: 3.2637\n",
      "Epoch 491/500, Loss: 3.2617\n",
      "Epoch 1/500, Loss: 39.8571\n",
      "Epoch 11/500, Loss: 5.1580\n",
      "Epoch 21/500, Loss: 4.3610\n",
      "Epoch 31/500, Loss: 4.0119\n",
      "Epoch 41/500, Loss: 3.8568\n",
      "Epoch 51/500, Loss: 3.7599\n",
      "Epoch 61/500, Loss: 3.6864\n",
      "Epoch 71/500, Loss: 3.6406\n",
      "Epoch 81/500, Loss: 3.5977\n",
      "Epoch 91/500, Loss: 3.5670\n",
      "Epoch 101/500, Loss: 3.5342\n",
      "Epoch 111/500, Loss: 3.5102\n",
      "Epoch 121/500, Loss: 3.4828\n",
      "Epoch 131/500, Loss: 3.4640\n",
      "Epoch 141/500, Loss: 3.4484\n",
      "Epoch 151/500, Loss: 3.4344\n",
      "Epoch 161/500, Loss: 3.4188\n",
      "Epoch 171/500, Loss: 3.4049\n",
      "Epoch 181/500, Loss: 3.3969\n",
      "Epoch 191/500, Loss: 3.3847\n",
      "Epoch 201/500, Loss: 3.3754\n",
      "Epoch 211/500, Loss: 3.3682\n",
      "Epoch 221/500, Loss: 3.3629\n",
      "Epoch 231/500, Loss: 3.3558\n",
      "Epoch 241/500, Loss: 3.3469\n",
      "Epoch 251/500, Loss: 3.3412\n",
      "Epoch 261/500, Loss: 3.3374\n",
      "Epoch 271/500, Loss: 3.3293\n",
      "Epoch 281/500, Loss: 3.3250\n",
      "Epoch 291/500, Loss: 3.3202\n",
      "Epoch 301/500, Loss: 3.3145\n",
      "Epoch 311/500, Loss: 3.3121\n",
      "Epoch 321/500, Loss: 3.3061\n",
      "Epoch 331/500, Loss: 3.3023\n",
      "Epoch 341/500, Loss: 3.2973\n",
      "Epoch 351/500, Loss: 3.2929\n",
      "Epoch 361/500, Loss: 3.2894\n",
      "Epoch 371/500, Loss: 3.2855\n",
      "Epoch 381/500, Loss: 3.2802\n",
      "Epoch 391/500, Loss: 3.2771\n",
      "Epoch 401/500, Loss: 3.2743\n",
      "Epoch 411/500, Loss: 3.2717\n",
      "Epoch 421/500, Loss: 3.2674\n",
      "Epoch 431/500, Loss: 3.2647\n",
      "Epoch 441/500, Loss: 3.2629\n",
      "Epoch 451/500, Loss: 3.2610\n",
      "Epoch 461/500, Loss: 3.2572\n",
      "Epoch 471/500, Loss: 3.2542\n",
      "Epoch 481/500, Loss: 3.2516\n",
      "Epoch 491/500, Loss: 3.2502\n",
      "Epoch 1/500, Loss: 39.1794\n",
      "Epoch 11/500, Loss: 5.1142\n",
      "Epoch 21/500, Loss: 4.2783\n",
      "Epoch 31/500, Loss: 3.9482\n",
      "Epoch 41/500, Loss: 3.8210\n",
      "Epoch 51/500, Loss: 3.7391\n",
      "Epoch 61/500, Loss: 3.6806\n",
      "Epoch 71/500, Loss: 3.6259\n",
      "Epoch 81/500, Loss: 3.5875\n",
      "Epoch 91/500, Loss: 3.5557\n",
      "Epoch 101/500, Loss: 3.5337\n",
      "Epoch 111/500, Loss: 3.5064\n",
      "Epoch 121/500, Loss: 3.4865\n",
      "Epoch 131/500, Loss: 3.4717\n",
      "Epoch 141/500, Loss: 3.4565\n",
      "Epoch 151/500, Loss: 3.4426\n",
      "Epoch 161/500, Loss: 3.4318\n",
      "Epoch 171/500, Loss: 3.4200\n",
      "Epoch 181/500, Loss: 3.4092\n",
      "Epoch 191/500, Loss: 3.3999\n",
      "Epoch 201/500, Loss: 3.3903\n",
      "Epoch 211/500, Loss: 3.3889\n",
      "Epoch 221/500, Loss: 3.3730\n",
      "Epoch 231/500, Loss: 3.3648\n",
      "Epoch 241/500, Loss: 3.3616\n",
      "Epoch 251/500, Loss: 3.3530\n",
      "Epoch 261/500, Loss: 3.3464\n",
      "Epoch 271/500, Loss: 3.3411\n",
      "Epoch 281/500, Loss: 3.3369\n",
      "Epoch 291/500, Loss: 3.3276\n",
      "Epoch 301/500, Loss: 3.3236\n",
      "Epoch 311/500, Loss: 3.3187\n",
      "Epoch 321/500, Loss: 3.3145\n",
      "Epoch 331/500, Loss: 3.3090\n",
      "Epoch 341/500, Loss: 3.3034\n",
      "Epoch 351/500, Loss: 3.3017\n",
      "Epoch 361/500, Loss: 3.2954\n",
      "Epoch 371/500, Loss: 3.2903\n",
      "Epoch 381/500, Loss: 3.2880\n",
      "Epoch 391/500, Loss: 3.2845\n",
      "Epoch 401/500, Loss: 3.2799\n",
      "Epoch 411/500, Loss: 3.2773\n",
      "Epoch 421/500, Loss: 3.2738\n",
      "Epoch 431/500, Loss: 3.2710\n",
      "Epoch 441/500, Loss: 3.2664\n",
      "Epoch 451/500, Loss: 3.2646\n",
      "Epoch 461/500, Loss: 3.2611\n",
      "Epoch 471/500, Loss: 3.2592\n",
      "Epoch 481/500, Loss: 3.2551\n",
      "Epoch 491/500, Loss: 3.2527\n",
      "Epoch 1/500, Loss: 41.2056\n",
      "Epoch 11/500, Loss: 5.0657\n",
      "Epoch 21/500, Loss: 4.2209\n",
      "Epoch 31/500, Loss: 3.9118\n",
      "Epoch 41/500, Loss: 3.7721\n",
      "Epoch 51/500, Loss: 3.6920\n",
      "Epoch 61/500, Loss: 3.6398\n",
      "Epoch 71/500, Loss: 3.5988\n",
      "Epoch 81/500, Loss: 3.5601\n",
      "Epoch 91/500, Loss: 3.5320\n",
      "Epoch 101/500, Loss: 3.5103\n",
      "Epoch 111/500, Loss: 3.4868\n",
      "Epoch 121/500, Loss: 3.4691\n",
      "Epoch 131/500, Loss: 3.4506\n",
      "Epoch 141/500, Loss: 3.4340\n",
      "Epoch 151/500, Loss: 3.4238\n",
      "Epoch 161/500, Loss: 3.4110\n",
      "Epoch 171/500, Loss: 3.4035\n",
      "Epoch 181/500, Loss: 3.3911\n",
      "Epoch 191/500, Loss: 3.3853\n",
      "Epoch 201/500, Loss: 3.3808\n",
      "Epoch 211/500, Loss: 3.3726\n",
      "Epoch 221/500, Loss: 3.3639\n",
      "Epoch 231/500, Loss: 3.3578\n",
      "Epoch 241/500, Loss: 3.3518\n",
      "Epoch 251/500, Loss: 3.3455\n",
      "Epoch 261/500, Loss: 3.3439\n",
      "Epoch 271/500, Loss: 3.3356\n",
      "Epoch 281/500, Loss: 3.3288\n",
      "Epoch 291/500, Loss: 3.3260\n",
      "Epoch 301/500, Loss: 3.3237\n",
      "Epoch 311/500, Loss: 3.3168\n",
      "Epoch 321/500, Loss: 3.3130\n",
      "Epoch 331/500, Loss: 3.3082\n",
      "Epoch 341/500, Loss: 3.3046\n",
      "Epoch 351/500, Loss: 3.3019\n",
      "Epoch 361/500, Loss: 3.2970\n",
      "Epoch 371/500, Loss: 3.2929\n",
      "Epoch 381/500, Loss: 3.2908\n",
      "Epoch 391/500, Loss: 3.2879\n",
      "Epoch 401/500, Loss: 3.2834\n",
      "Epoch 411/500, Loss: 3.2811\n",
      "Epoch 421/500, Loss: 3.2794\n",
      "Epoch 431/500, Loss: 3.2746\n",
      "Epoch 441/500, Loss: 3.2708\n",
      "Epoch 451/500, Loss: 3.2690\n",
      "Epoch 461/500, Loss: 3.2663\n",
      "Epoch 471/500, Loss: 3.2647\n",
      "Epoch 481/500, Loss: 3.2601\n",
      "Epoch 491/500, Loss: 3.2589\n",
      "Epoch 1/500, Loss: 38.5263\n",
      "Epoch 11/500, Loss: 5.2310\n",
      "Epoch 21/500, Loss: 4.4738\n",
      "Epoch 31/500, Loss: 4.0953\n",
      "Epoch 41/500, Loss: 3.9108\n",
      "Epoch 51/500, Loss: 3.8087\n",
      "Epoch 61/500, Loss: 3.7376\n",
      "Epoch 71/500, Loss: 3.6913\n",
      "Epoch 81/500, Loss: 3.6439\n",
      "Epoch 91/500, Loss: 3.6096\n",
      "Epoch 101/500, Loss: 3.5771\n",
      "Epoch 111/500, Loss: 3.5467\n",
      "Epoch 121/500, Loss: 3.5212\n",
      "Epoch 131/500, Loss: 3.5018\n",
      "Epoch 141/500, Loss: 3.4834\n",
      "Epoch 151/500, Loss: 3.4672\n",
      "Epoch 161/500, Loss: 3.4536\n",
      "Epoch 171/500, Loss: 3.4359\n",
      "Epoch 181/500, Loss: 3.4227\n",
      "Epoch 191/500, Loss: 3.4089\n",
      "Epoch 201/500, Loss: 3.3990\n",
      "Epoch 211/500, Loss: 3.3916\n",
      "Epoch 221/500, Loss: 3.3823\n",
      "Epoch 231/500, Loss: 3.3713\n",
      "Epoch 241/500, Loss: 3.3673\n",
      "Epoch 251/500, Loss: 3.3557\n",
      "Epoch 261/500, Loss: 3.3493\n",
      "Epoch 271/500, Loss: 3.3399\n",
      "Epoch 281/500, Loss: 3.3366\n",
      "Epoch 291/500, Loss: 3.3309\n",
      "Epoch 301/500, Loss: 3.3241\n",
      "Epoch 311/500, Loss: 3.3189\n",
      "Epoch 321/500, Loss: 3.3127\n",
      "Epoch 331/500, Loss: 3.3077\n",
      "Epoch 341/500, Loss: 3.3014\n",
      "Epoch 351/500, Loss: 3.2961\n",
      "Epoch 361/500, Loss: 3.2928\n",
      "Epoch 371/500, Loss: 3.2887\n",
      "Epoch 381/500, Loss: 3.2855\n",
      "Epoch 391/500, Loss: 3.2816\n",
      "Epoch 401/500, Loss: 3.2764\n",
      "Epoch 411/500, Loss: 3.2760\n",
      "Epoch 421/500, Loss: 3.2687\n",
      "Epoch 431/500, Loss: 3.2674\n",
      "Epoch 441/500, Loss: 3.2633\n",
      "Epoch 451/500, Loss: 3.2609\n",
      "Epoch 461/500, Loss: 3.2587\n",
      "Epoch 471/500, Loss: 3.2556\n",
      "Epoch 481/500, Loss: 3.2527\n",
      "Epoch 491/500, Loss: 3.2493\n",
      "Epoch 1/500, Loss: 40.8280\n",
      "Epoch 11/500, Loss: 5.1587\n",
      "Epoch 21/500, Loss: 4.2241\n",
      "Epoch 31/500, Loss: 3.8951\n",
      "Epoch 41/500, Loss: 3.7568\n",
      "Epoch 51/500, Loss: 3.6714\n",
      "Epoch 61/500, Loss: 3.6058\n",
      "Epoch 71/500, Loss: 3.5687\n",
      "Epoch 81/500, Loss: 3.5363\n",
      "Epoch 91/500, Loss: 3.5127\n",
      "Epoch 101/500, Loss: 3.4950\n",
      "Epoch 111/500, Loss: 3.4795\n",
      "Epoch 121/500, Loss: 3.4639\n",
      "Epoch 131/500, Loss: 3.4561\n",
      "Epoch 141/500, Loss: 3.4394\n",
      "Epoch 151/500, Loss: 3.4286\n",
      "Epoch 161/500, Loss: 3.4192\n",
      "Epoch 171/500, Loss: 3.4106\n",
      "Epoch 181/500, Loss: 3.3970\n",
      "Epoch 191/500, Loss: 3.3870\n",
      "Epoch 201/500, Loss: 3.3795\n",
      "Epoch 211/500, Loss: 3.3714\n",
      "Epoch 221/500, Loss: 3.3645\n",
      "Epoch 231/500, Loss: 3.3573\n",
      "Epoch 241/500, Loss: 3.3518\n",
      "Epoch 251/500, Loss: 3.3426\n",
      "Epoch 261/500, Loss: 3.3392\n",
      "Epoch 271/500, Loss: 3.3323\n",
      "Epoch 281/500, Loss: 3.3280\n",
      "Epoch 291/500, Loss: 3.3227\n",
      "Epoch 301/500, Loss: 3.3147\n",
      "Epoch 311/500, Loss: 3.3133\n",
      "Epoch 321/500, Loss: 3.3070\n",
      "Epoch 331/500, Loss: 3.3024\n",
      "Epoch 341/500, Loss: 3.2977\n",
      "Epoch 351/500, Loss: 3.2951\n",
      "Epoch 361/500, Loss: 3.2913\n",
      "Epoch 371/500, Loss: 3.2867\n",
      "Epoch 381/500, Loss: 3.2843\n",
      "Epoch 391/500, Loss: 3.2787\n",
      "Epoch 401/500, Loss: 3.2755\n",
      "Epoch 411/500, Loss: 3.2720\n",
      "Epoch 421/500, Loss: 3.2702\n",
      "Epoch 431/500, Loss: 3.2654\n",
      "Epoch 441/500, Loss: 3.2661\n",
      "Epoch 451/500, Loss: 3.2614\n",
      "Epoch 461/500, Loss: 3.2600\n",
      "Epoch 471/500, Loss: 3.2558\n",
      "Epoch 481/500, Loss: 3.2537\n",
      "Epoch 491/500, Loss: 3.2507\n",
      "Epoch 1/500, Loss: 35.5346\n",
      "Epoch 11/500, Loss: 4.9965\n",
      "Epoch 21/500, Loss: 4.2104\n",
      "Epoch 31/500, Loss: 3.9625\n",
      "Epoch 41/500, Loss: 3.8410\n",
      "Epoch 51/500, Loss: 3.7645\n",
      "Epoch 61/500, Loss: 3.7025\n",
      "Epoch 71/500, Loss: 3.6580\n",
      "Epoch 81/500, Loss: 3.6116\n",
      "Epoch 91/500, Loss: 3.5755\n",
      "Epoch 101/500, Loss: 3.5433\n",
      "Epoch 111/500, Loss: 3.5191\n",
      "Epoch 121/500, Loss: 3.4967\n",
      "Epoch 131/500, Loss: 3.4772\n",
      "Epoch 141/500, Loss: 3.4629\n",
      "Epoch 151/500, Loss: 3.4493\n",
      "Epoch 161/500, Loss: 3.4319\n",
      "Epoch 171/500, Loss: 3.4246\n",
      "Epoch 181/500, Loss: 3.4148\n",
      "Epoch 191/500, Loss: 3.4064\n",
      "Epoch 201/500, Loss: 3.3968\n",
      "Epoch 211/500, Loss: 3.3899\n",
      "Epoch 221/500, Loss: 3.3787\n",
      "Epoch 231/500, Loss: 3.3708\n",
      "Epoch 241/500, Loss: 3.3625\n",
      "Epoch 251/500, Loss: 3.3573\n",
      "Epoch 261/500, Loss: 3.3498\n",
      "Epoch 271/500, Loss: 3.3401\n",
      "Epoch 281/500, Loss: 3.3345\n",
      "Epoch 291/500, Loss: 3.3290\n",
      "Epoch 301/500, Loss: 3.3234\n",
      "Epoch 311/500, Loss: 3.3185\n",
      "Epoch 321/500, Loss: 3.3130\n",
      "Epoch 331/500, Loss: 3.3091\n",
      "Epoch 341/500, Loss: 3.3021\n",
      "Epoch 351/500, Loss: 3.2992\n",
      "Epoch 361/500, Loss: 3.2930\n",
      "Epoch 371/500, Loss: 3.2888\n",
      "Epoch 381/500, Loss: 3.2849\n",
      "Epoch 391/500, Loss: 3.2822\n",
      "Epoch 401/500, Loss: 3.2780\n",
      "Epoch 411/500, Loss: 3.2743\n",
      "Epoch 421/500, Loss: 3.2710\n",
      "Epoch 431/500, Loss: 3.2691\n",
      "Epoch 441/500, Loss: 3.2667\n",
      "Epoch 451/500, Loss: 3.2633\n",
      "Epoch 461/500, Loss: 3.2626\n",
      "Epoch 471/500, Loss: 3.2576\n",
      "Epoch 481/500, Loss: 3.2551\n",
      "Epoch 491/500, Loss: 3.2517\n",
      "Epoch 1/500, Loss: 35.7645\n",
      "Epoch 11/500, Loss: 5.0017\n",
      "Epoch 21/500, Loss: 4.1484\n",
      "Epoch 31/500, Loss: 3.8716\n",
      "Epoch 41/500, Loss: 3.7692\n",
      "Epoch 51/500, Loss: 3.6972\n",
      "Epoch 61/500, Loss: 3.6451\n",
      "Epoch 71/500, Loss: 3.5993\n",
      "Epoch 81/500, Loss: 3.5552\n",
      "Epoch 91/500, Loss: 3.5264\n",
      "Epoch 101/500, Loss: 3.5029\n",
      "Epoch 111/500, Loss: 3.4837\n",
      "Epoch 121/500, Loss: 3.4667\n",
      "Epoch 131/500, Loss: 3.4510\n",
      "Epoch 141/500, Loss: 3.4342\n",
      "Epoch 151/500, Loss: 3.4261\n",
      "Epoch 161/500, Loss: 3.4094\n",
      "Epoch 171/500, Loss: 3.3990\n",
      "Epoch 181/500, Loss: 3.3896\n",
      "Epoch 191/500, Loss: 3.3832\n",
      "Epoch 201/500, Loss: 3.3752\n",
      "Epoch 211/500, Loss: 3.3663\n",
      "Epoch 221/500, Loss: 3.3572\n",
      "Epoch 231/500, Loss: 3.3521\n",
      "Epoch 241/500, Loss: 3.3423\n",
      "Epoch 251/500, Loss: 3.3367\n",
      "Epoch 261/500, Loss: 3.3314\n",
      "Epoch 271/500, Loss: 3.3257\n",
      "Epoch 281/500, Loss: 3.3226\n",
      "Epoch 291/500, Loss: 3.3169\n",
      "Epoch 301/500, Loss: 3.3117\n",
      "Epoch 311/500, Loss: 3.3062\n",
      "Epoch 321/500, Loss: 3.3018\n",
      "Epoch 331/500, Loss: 3.2989\n",
      "Epoch 341/500, Loss: 3.2947\n",
      "Epoch 351/500, Loss: 3.2898\n",
      "Epoch 361/500, Loss: 3.2871\n",
      "Epoch 371/500, Loss: 3.2819\n",
      "Epoch 381/500, Loss: 3.2806\n",
      "Epoch 391/500, Loss: 3.2765\n",
      "Epoch 401/500, Loss: 3.2733\n",
      "Epoch 411/500, Loss: 3.2685\n",
      "Epoch 421/500, Loss: 3.2665\n",
      "Epoch 431/500, Loss: 3.2659\n",
      "Epoch 441/500, Loss: 3.2617\n",
      "Epoch 451/500, Loss: 3.2591\n",
      "Epoch 461/500, Loss: 3.2552\n",
      "Epoch 471/500, Loss: 3.2532\n",
      "Epoch 481/500, Loss: 3.2509\n",
      "Epoch 491/500, Loss: 3.2477\n",
      "Epoch 1/500, Loss: 38.5068\n",
      "Epoch 11/500, Loss: 5.5588\n",
      "Epoch 21/500, Loss: 4.5945\n",
      "Epoch 31/500, Loss: 4.1005\n",
      "Epoch 41/500, Loss: 3.8761\n",
      "Epoch 51/500, Loss: 3.7672\n",
      "Epoch 61/500, Loss: 3.6872\n",
      "Epoch 71/500, Loss: 3.6297\n",
      "Epoch 81/500, Loss: 3.5949\n",
      "Epoch 91/500, Loss: 3.5582\n",
      "Epoch 101/500, Loss: 3.5321\n",
      "Epoch 111/500, Loss: 3.5141\n",
      "Epoch 121/500, Loss: 3.4955\n",
      "Epoch 131/500, Loss: 3.4745\n",
      "Epoch 141/500, Loss: 3.4606\n",
      "Epoch 151/500, Loss: 3.4451\n",
      "Epoch 161/500, Loss: 3.4329\n",
      "Epoch 171/500, Loss: 3.4232\n",
      "Epoch 181/500, Loss: 3.4131\n",
      "Epoch 191/500, Loss: 3.4042\n",
      "Epoch 201/500, Loss: 3.3968\n",
      "Epoch 211/500, Loss: 3.3849\n",
      "Epoch 221/500, Loss: 3.3773\n",
      "Epoch 231/500, Loss: 3.3697\n",
      "Epoch 241/500, Loss: 3.3628\n",
      "Epoch 251/500, Loss: 3.3566\n",
      "Epoch 261/500, Loss: 3.3496\n",
      "Epoch 271/500, Loss: 3.3458\n",
      "Epoch 281/500, Loss: 3.3399\n",
      "Epoch 291/500, Loss: 3.3338\n",
      "Epoch 301/500, Loss: 3.3262\n",
      "Epoch 311/500, Loss: 3.3236\n",
      "Epoch 321/500, Loss: 3.3187\n",
      "Epoch 331/500, Loss: 3.3127\n",
      "Epoch 341/500, Loss: 3.3087\n",
      "Epoch 351/500, Loss: 3.3068\n",
      "Epoch 361/500, Loss: 3.3007\n",
      "Epoch 371/500, Loss: 3.2962\n",
      "Epoch 381/500, Loss: 3.2953\n",
      "Epoch 391/500, Loss: 3.2887\n",
      "Epoch 401/500, Loss: 3.2858\n",
      "Epoch 411/500, Loss: 3.2822\n",
      "Epoch 421/500, Loss: 3.2788\n",
      "Epoch 431/500, Loss: 3.2761\n",
      "Epoch 441/500, Loss: 3.2721\n",
      "Epoch 451/500, Loss: 3.2685\n",
      "Epoch 461/500, Loss: 3.2666\n",
      "Epoch 471/500, Loss: 3.2650\n",
      "Epoch 481/500, Loss: 3.2611\n",
      "Epoch 491/500, Loss: 3.2590\n",
      "Epoch 1/500, Loss: 41.9250\n",
      "Epoch 11/500, Loss: 5.3068\n",
      "Epoch 21/500, Loss: 4.2803\n",
      "Epoch 31/500, Loss: 3.9277\n",
      "Epoch 41/500, Loss: 3.8057\n",
      "Epoch 51/500, Loss: 3.7367\n",
      "Epoch 61/500, Loss: 3.6897\n",
      "Epoch 71/500, Loss: 3.6519\n",
      "Epoch 81/500, Loss: 3.6151\n",
      "Epoch 91/500, Loss: 3.5814\n",
      "Epoch 101/500, Loss: 3.5518\n",
      "Epoch 111/500, Loss: 3.5256\n",
      "Epoch 121/500, Loss: 3.5064\n",
      "Epoch 131/500, Loss: 3.4862\n",
      "Epoch 141/500, Loss: 3.4696\n",
      "Epoch 151/500, Loss: 3.4552\n",
      "Epoch 161/500, Loss: 3.4401\n",
      "Epoch 171/500, Loss: 3.4261\n",
      "Epoch 181/500, Loss: 3.4155\n",
      "Epoch 191/500, Loss: 3.4027\n",
      "Epoch 201/500, Loss: 3.3929\n",
      "Epoch 211/500, Loss: 3.3836\n",
      "Epoch 221/500, Loss: 3.3755\n",
      "Epoch 231/500, Loss: 3.3664\n",
      "Epoch 241/500, Loss: 3.3585\n",
      "Epoch 251/500, Loss: 3.3552\n",
      "Epoch 261/500, Loss: 3.3477\n",
      "Epoch 271/500, Loss: 3.3433\n",
      "Epoch 281/500, Loss: 3.3360\n",
      "Epoch 291/500, Loss: 3.3293\n",
      "Epoch 301/500, Loss: 3.3247\n",
      "Epoch 311/500, Loss: 3.3225\n",
      "Epoch 321/500, Loss: 3.3168\n",
      "Epoch 331/500, Loss: 3.3117\n",
      "Epoch 341/500, Loss: 3.3084\n",
      "Epoch 351/500, Loss: 3.3040\n",
      "Epoch 361/500, Loss: 3.2990\n",
      "Epoch 371/500, Loss: 3.2961\n",
      "Epoch 381/500, Loss: 3.2902\n",
      "Epoch 391/500, Loss: 3.2883\n",
      "Epoch 401/500, Loss: 3.2857\n",
      "Epoch 411/500, Loss: 3.2811\n",
      "Epoch 421/500, Loss: 3.2769\n",
      "Epoch 431/500, Loss: 3.2730\n",
      "Epoch 441/500, Loss: 3.2702\n",
      "Epoch 451/500, Loss: 3.2679\n",
      "Epoch 461/500, Loss: 3.2633\n",
      "Epoch 471/500, Loss: 3.2613\n",
      "Epoch 481/500, Loss: 3.2577\n",
      "Epoch 491/500, Loss: 3.2556\n",
      "Epoch 1/500, Loss: 38.2676\n",
      "Epoch 11/500, Loss: 5.1020\n",
      "Epoch 21/500, Loss: 4.2166\n",
      "Epoch 31/500, Loss: 3.8817\n",
      "Epoch 41/500, Loss: 3.7543\n",
      "Epoch 51/500, Loss: 3.6801\n",
      "Epoch 61/500, Loss: 3.6281\n",
      "Epoch 71/500, Loss: 3.5864\n",
      "Epoch 81/500, Loss: 3.5542\n",
      "Epoch 91/500, Loss: 3.5316\n",
      "Epoch 101/500, Loss: 3.5071\n",
      "Epoch 111/500, Loss: 3.4881\n",
      "Epoch 121/500, Loss: 3.4694\n",
      "Epoch 131/500, Loss: 3.4561\n",
      "Epoch 141/500, Loss: 3.4423\n",
      "Epoch 151/500, Loss: 3.4316\n",
      "Epoch 161/500, Loss: 3.4185\n",
      "Epoch 171/500, Loss: 3.4078\n",
      "Epoch 181/500, Loss: 3.3958\n",
      "Epoch 191/500, Loss: 3.3904\n",
      "Epoch 201/500, Loss: 3.3784\n",
      "Epoch 211/500, Loss: 3.3711\n",
      "Epoch 221/500, Loss: 3.3628\n",
      "Epoch 231/500, Loss: 3.3571\n",
      "Epoch 241/500, Loss: 3.3505\n",
      "Epoch 251/500, Loss: 3.3445\n",
      "Epoch 261/500, Loss: 3.3371\n",
      "Epoch 271/500, Loss: 3.3327\n",
      "Epoch 281/500, Loss: 3.3284\n",
      "Epoch 291/500, Loss: 3.3240\n",
      "Epoch 301/500, Loss: 3.3176\n",
      "Epoch 311/500, Loss: 3.3117\n",
      "Epoch 321/500, Loss: 3.3079\n",
      "Epoch 331/500, Loss: 3.3044\n",
      "Epoch 341/500, Loss: 3.2991\n",
      "Epoch 351/500, Loss: 3.2947\n",
      "Epoch 361/500, Loss: 3.2921\n",
      "Epoch 371/500, Loss: 3.2862\n",
      "Epoch 381/500, Loss: 3.2832\n",
      "Epoch 391/500, Loss: 3.2774\n",
      "Epoch 401/500, Loss: 3.2754\n",
      "Epoch 411/500, Loss: 3.2720\n",
      "Epoch 421/500, Loss: 3.2683\n",
      "Epoch 431/500, Loss: 3.2654\n",
      "Epoch 441/500, Loss: 3.2621\n",
      "Epoch 451/500, Loss: 3.2592\n",
      "Epoch 461/500, Loss: 3.2561\n",
      "Epoch 471/500, Loss: 3.2519\n",
      "Epoch 481/500, Loss: 3.2493\n",
      "Epoch 491/500, Loss: 3.2485\n",
      "Epoch 1/500, Loss: 45.1532\n",
      "Epoch 11/500, Loss: 5.2279\n",
      "Epoch 21/500, Loss: 4.3186\n",
      "Epoch 31/500, Loss: 3.9524\n",
      "Epoch 41/500, Loss: 3.7953\n",
      "Epoch 51/500, Loss: 3.7121\n",
      "Epoch 61/500, Loss: 3.6507\n",
      "Epoch 71/500, Loss: 3.6088\n",
      "Epoch 81/500, Loss: 3.5724\n",
      "Epoch 91/500, Loss: 3.5397\n",
      "Epoch 101/500, Loss: 3.5183\n",
      "Epoch 111/500, Loss: 3.4988\n",
      "Epoch 121/500, Loss: 3.4786\n",
      "Epoch 131/500, Loss: 3.4682\n",
      "Epoch 141/500, Loss: 3.4532\n",
      "Epoch 151/500, Loss: 3.4432\n",
      "Epoch 161/500, Loss: 3.4329\n",
      "Epoch 171/500, Loss: 3.4224\n",
      "Epoch 181/500, Loss: 3.4089\n",
      "Epoch 191/500, Loss: 3.3996\n",
      "Epoch 201/500, Loss: 3.3939\n",
      "Epoch 211/500, Loss: 3.3828\n",
      "Epoch 221/500, Loss: 3.3743\n",
      "Epoch 231/500, Loss: 3.3670\n",
      "Epoch 241/500, Loss: 3.3600\n",
      "Epoch 251/500, Loss: 3.3546\n",
      "Epoch 261/500, Loss: 3.3483\n",
      "Epoch 271/500, Loss: 3.3406\n",
      "Epoch 281/500, Loss: 3.3364\n",
      "Epoch 291/500, Loss: 3.3326\n",
      "Epoch 301/500, Loss: 3.3251\n",
      "Epoch 311/500, Loss: 3.3189\n",
      "Epoch 321/500, Loss: 3.3135\n",
      "Epoch 331/500, Loss: 3.3091\n",
      "Epoch 341/500, Loss: 3.3053\n",
      "Epoch 351/500, Loss: 3.3018\n",
      "Epoch 361/500, Loss: 3.2985\n",
      "Epoch 371/500, Loss: 3.2937\n",
      "Epoch 381/500, Loss: 3.2891\n",
      "Epoch 391/500, Loss: 3.2854\n",
      "Epoch 401/500, Loss: 3.2813\n",
      "Epoch 411/500, Loss: 3.2779\n",
      "Epoch 421/500, Loss: 3.2755\n",
      "Epoch 431/500, Loss: 3.2729\n",
      "Epoch 441/500, Loss: 3.2687\n",
      "Epoch 451/500, Loss: 3.2671\n",
      "Epoch 461/500, Loss: 3.2633\n",
      "Epoch 471/500, Loss: 3.2622\n",
      "Epoch 481/500, Loss: 3.2585\n",
      "Epoch 491/500, Loss: 3.2562\n",
      "Epoch 1/500, Loss: 38.4711\n",
      "Epoch 11/500, Loss: 5.1370\n",
      "Epoch 21/500, Loss: 4.2718\n",
      "Epoch 31/500, Loss: 3.9582\n",
      "Epoch 41/500, Loss: 3.8179\n",
      "Epoch 51/500, Loss: 3.7402\n",
      "Epoch 61/500, Loss: 3.6854\n",
      "Epoch 71/500, Loss: 3.6416\n",
      "Epoch 81/500, Loss: 3.6079\n",
      "Epoch 91/500, Loss: 3.5757\n",
      "Epoch 101/500, Loss: 3.5425\n",
      "Epoch 111/500, Loss: 3.5214\n",
      "Epoch 121/500, Loss: 3.5031\n",
      "Epoch 131/500, Loss: 3.4829\n",
      "Epoch 141/500, Loss: 3.4664\n",
      "Epoch 151/500, Loss: 3.4489\n",
      "Epoch 161/500, Loss: 3.4339\n",
      "Epoch 171/500, Loss: 3.4238\n",
      "Epoch 181/500, Loss: 3.4138\n",
      "Epoch 191/500, Loss: 3.4041\n",
      "Epoch 201/500, Loss: 3.3967\n",
      "Epoch 211/500, Loss: 3.3853\n",
      "Epoch 221/500, Loss: 3.3763\n",
      "Epoch 231/500, Loss: 3.3694\n",
      "Epoch 241/500, Loss: 3.3610\n",
      "Epoch 251/500, Loss: 3.3532\n",
      "Epoch 261/500, Loss: 3.3470\n",
      "Epoch 271/500, Loss: 3.3416\n",
      "Epoch 281/500, Loss: 3.3353\n",
      "Epoch 291/500, Loss: 3.3276\n",
      "Epoch 301/500, Loss: 3.3239\n",
      "Epoch 311/500, Loss: 3.3176\n",
      "Epoch 321/500, Loss: 3.3133\n",
      "Epoch 331/500, Loss: 3.3087\n",
      "Epoch 341/500, Loss: 3.3026\n",
      "Epoch 351/500, Loss: 3.2989\n",
      "Epoch 361/500, Loss: 3.2932\n",
      "Epoch 371/500, Loss: 3.2884\n",
      "Epoch 381/500, Loss: 3.2843\n",
      "Epoch 391/500, Loss: 3.2793\n",
      "Epoch 401/500, Loss: 3.2762\n",
      "Epoch 411/500, Loss: 3.2755\n",
      "Epoch 421/500, Loss: 3.2699\n",
      "Epoch 431/500, Loss: 3.2665\n",
      "Epoch 441/500, Loss: 3.2628\n",
      "Epoch 451/500, Loss: 3.2597\n",
      "Epoch 461/500, Loss: 3.2572\n",
      "Epoch 471/500, Loss: 3.2538\n",
      "Epoch 481/500, Loss: 3.2510\n",
      "Epoch 491/500, Loss: 3.2478\n",
      "Epoch 1/500, Loss: 45.9242\n",
      "Epoch 11/500, Loss: 5.0887\n",
      "Epoch 21/500, Loss: 4.3421\n",
      "Epoch 31/500, Loss: 4.0284\n",
      "Epoch 41/500, Loss: 3.8856\n",
      "Epoch 51/500, Loss: 3.8061\n",
      "Epoch 61/500, Loss: 3.7394\n",
      "Epoch 71/500, Loss: 3.6908\n",
      "Epoch 81/500, Loss: 3.6517\n",
      "Epoch 91/500, Loss: 3.6174\n",
      "Epoch 101/500, Loss: 3.5812\n",
      "Epoch 111/500, Loss: 3.5512\n",
      "Epoch 121/500, Loss: 3.5325\n",
      "Epoch 131/500, Loss: 3.5104\n",
      "Epoch 141/500, Loss: 3.4916\n",
      "Epoch 151/500, Loss: 3.4743\n",
      "Epoch 161/500, Loss: 3.4547\n",
      "Epoch 171/500, Loss: 3.4420\n",
      "Epoch 181/500, Loss: 3.4321\n",
      "Epoch 191/500, Loss: 3.4173\n",
      "Epoch 201/500, Loss: 3.4053\n",
      "Epoch 211/500, Loss: 3.3989\n",
      "Epoch 221/500, Loss: 3.3895\n",
      "Epoch 231/500, Loss: 3.3786\n",
      "Epoch 241/500, Loss: 3.3710\n",
      "Epoch 251/500, Loss: 3.3650\n",
      "Epoch 261/500, Loss: 3.3583\n",
      "Epoch 271/500, Loss: 3.3489\n",
      "Epoch 281/500, Loss: 3.3442\n",
      "Epoch 291/500, Loss: 3.3389\n",
      "Epoch 301/500, Loss: 3.3331\n",
      "Epoch 311/500, Loss: 3.3288\n",
      "Epoch 321/500, Loss: 3.3221\n",
      "Epoch 331/500, Loss: 3.3148\n",
      "Epoch 341/500, Loss: 3.3108\n",
      "Epoch 351/500, Loss: 3.3063\n",
      "Epoch 361/500, Loss: 3.3023\n",
      "Epoch 371/500, Loss: 3.2983\n",
      "Epoch 381/500, Loss: 3.2923\n",
      "Epoch 391/500, Loss: 3.2918\n",
      "Epoch 401/500, Loss: 3.2880\n",
      "Epoch 411/500, Loss: 3.2835\n",
      "Epoch 421/500, Loss: 3.2799\n",
      "Epoch 431/500, Loss: 3.2776\n",
      "Epoch 441/500, Loss: 3.2730\n",
      "Epoch 451/500, Loss: 3.2713\n",
      "Epoch 461/500, Loss: 3.2676\n",
      "Epoch 471/500, Loss: 3.2657\n",
      "Epoch 481/500, Loss: 3.2615\n",
      "Epoch 491/500, Loss: 3.2588\n",
      "Epoch 1/500, Loss: 48.0078\n",
      "Epoch 11/500, Loss: 5.1195\n",
      "Epoch 21/500, Loss: 4.4297\n",
      "Epoch 31/500, Loss: 4.0657\n",
      "Epoch 41/500, Loss: 3.8829\n",
      "Epoch 51/500, Loss: 3.7782\n",
      "Epoch 61/500, Loss: 3.7098\n",
      "Epoch 71/500, Loss: 3.6600\n",
      "Epoch 81/500, Loss: 3.6161\n",
      "Epoch 91/500, Loss: 3.5830\n",
      "Epoch 101/500, Loss: 3.5460\n",
      "Epoch 111/500, Loss: 3.5235\n",
      "Epoch 121/500, Loss: 3.4958\n",
      "Epoch 131/500, Loss: 3.4783\n",
      "Epoch 141/500, Loss: 3.4611\n",
      "Epoch 151/500, Loss: 3.4516\n",
      "Epoch 161/500, Loss: 3.4387\n",
      "Epoch 171/500, Loss: 3.4276\n",
      "Epoch 181/500, Loss: 3.4190\n",
      "Epoch 191/500, Loss: 3.4079\n",
      "Epoch 201/500, Loss: 3.3993\n",
      "Epoch 211/500, Loss: 3.3867\n",
      "Epoch 221/500, Loss: 3.3812\n",
      "Epoch 231/500, Loss: 3.3737\n",
      "Epoch 241/500, Loss: 3.3678\n",
      "Epoch 251/500, Loss: 3.3610\n",
      "Epoch 261/500, Loss: 3.3522\n",
      "Epoch 271/500, Loss: 3.3462\n",
      "Epoch 281/500, Loss: 3.3408\n",
      "Epoch 291/500, Loss: 3.3365\n",
      "Epoch 301/500, Loss: 3.3299\n",
      "Epoch 311/500, Loss: 3.3251\n",
      "Epoch 321/500, Loss: 3.3192\n",
      "Epoch 331/500, Loss: 3.3149\n",
      "Epoch 341/500, Loss: 3.3115\n",
      "Epoch 351/500, Loss: 3.3075\n",
      "Epoch 361/500, Loss: 3.3018\n",
      "Epoch 371/500, Loss: 3.2983\n",
      "Epoch 381/500, Loss: 3.2944\n",
      "Epoch 391/500, Loss: 3.2931\n",
      "Epoch 401/500, Loss: 3.2893\n",
      "Epoch 411/500, Loss: 3.2849\n",
      "Epoch 421/500, Loss: 3.2821\n",
      "Epoch 431/500, Loss: 3.2783\n",
      "Epoch 441/500, Loss: 3.2762\n",
      "Epoch 451/500, Loss: 3.2743\n",
      "Epoch 461/500, Loss: 3.2690\n",
      "Epoch 471/500, Loss: 3.2682\n",
      "Epoch 481/500, Loss: 3.2643\n",
      "Epoch 491/500, Loss: 3.2618\n",
      "Epoch 1/500, Loss: 48.5569\n",
      "Epoch 11/500, Loss: 4.9986\n",
      "Epoch 21/500, Loss: 4.1096\n",
      "Epoch 31/500, Loss: 3.8274\n",
      "Epoch 41/500, Loss: 3.7213\n",
      "Epoch 51/500, Loss: 3.6410\n",
      "Epoch 61/500, Loss: 3.5813\n",
      "Epoch 71/500, Loss: 3.5435\n",
      "Epoch 81/500, Loss: 3.5152\n",
      "Epoch 91/500, Loss: 3.4914\n",
      "Epoch 101/500, Loss: 3.4678\n",
      "Epoch 111/500, Loss: 3.4478\n",
      "Epoch 121/500, Loss: 3.4363\n",
      "Epoch 131/500, Loss: 3.4234\n",
      "Epoch 141/500, Loss: 3.4070\n",
      "Epoch 151/500, Loss: 3.3940\n",
      "Epoch 161/500, Loss: 3.3865\n",
      "Epoch 171/500, Loss: 3.3762\n",
      "Epoch 181/500, Loss: 3.3707\n",
      "Epoch 191/500, Loss: 3.3606\n",
      "Epoch 201/500, Loss: 3.3544\n",
      "Epoch 211/500, Loss: 3.3483\n",
      "Epoch 221/500, Loss: 3.3392\n",
      "Epoch 231/500, Loss: 3.3337\n",
      "Epoch 241/500, Loss: 3.3272\n",
      "Epoch 251/500, Loss: 3.3193\n",
      "Epoch 261/500, Loss: 3.3160\n",
      "Epoch 271/500, Loss: 3.3112\n",
      "Epoch 281/500, Loss: 3.3064\n",
      "Epoch 291/500, Loss: 3.3001\n",
      "Epoch 301/500, Loss: 3.2960\n",
      "Epoch 311/500, Loss: 3.2935\n",
      "Epoch 321/500, Loss: 3.2882\n",
      "Epoch 331/500, Loss: 3.2849\n",
      "Epoch 341/500, Loss: 3.2813\n",
      "Epoch 351/500, Loss: 3.2797\n",
      "Epoch 361/500, Loss: 3.2756\n",
      "Epoch 371/500, Loss: 3.2720\n",
      "Epoch 381/500, Loss: 3.2684\n",
      "Epoch 391/500, Loss: 3.2627\n",
      "Epoch 401/500, Loss: 3.2623\n",
      "Epoch 411/500, Loss: 3.2604\n",
      "Epoch 421/500, Loss: 3.2572\n",
      "Epoch 431/500, Loss: 3.2556\n",
      "Epoch 441/500, Loss: 3.2502\n",
      "Epoch 451/500, Loss: 3.2485\n",
      "Epoch 461/500, Loss: 3.2474\n",
      "Epoch 471/500, Loss: 3.2443\n",
      "Epoch 481/500, Loss: 3.2421\n",
      "Epoch 491/500, Loss: 3.2406\n",
      "Epoch 1/500, Loss: 37.5377\n",
      "Epoch 11/500, Loss: 5.1121\n",
      "Epoch 21/500, Loss: 4.3555\n",
      "Epoch 31/500, Loss: 4.0110\n",
      "Epoch 41/500, Loss: 3.8538\n",
      "Epoch 51/500, Loss: 3.7595\n",
      "Epoch 61/500, Loss: 3.6961\n",
      "Epoch 71/500, Loss: 3.6510\n",
      "Epoch 81/500, Loss: 3.6161\n",
      "Epoch 91/500, Loss: 3.5889\n",
      "Epoch 101/500, Loss: 3.5622\n",
      "Epoch 111/500, Loss: 3.5436\n",
      "Epoch 121/500, Loss: 3.5169\n",
      "Epoch 131/500, Loss: 3.5010\n",
      "Epoch 141/500, Loss: 3.4875\n",
      "Epoch 151/500, Loss: 3.4728\n",
      "Epoch 161/500, Loss: 3.4580\n",
      "Epoch 171/500, Loss: 3.4455\n",
      "Epoch 181/500, Loss: 3.4309\n",
      "Epoch 191/500, Loss: 3.4231\n",
      "Epoch 201/500, Loss: 3.4113\n",
      "Epoch 211/500, Loss: 3.3969\n",
      "Epoch 221/500, Loss: 3.3877\n",
      "Epoch 231/500, Loss: 3.3809\n",
      "Epoch 241/500, Loss: 3.3711\n",
      "Epoch 251/500, Loss: 3.3652\n",
      "Epoch 261/500, Loss: 3.3589\n",
      "Epoch 271/500, Loss: 3.3521\n",
      "Epoch 281/500, Loss: 3.3449\n",
      "Epoch 291/500, Loss: 3.3393\n",
      "Epoch 301/500, Loss: 3.3329\n",
      "Epoch 311/500, Loss: 3.3259\n",
      "Epoch 321/500, Loss: 3.3224\n",
      "Epoch 331/500, Loss: 3.3174\n",
      "Epoch 341/500, Loss: 3.3126\n",
      "Epoch 351/500, Loss: 3.3076\n",
      "Epoch 361/500, Loss: 3.3024\n",
      "Epoch 371/500, Loss: 3.3001\n",
      "Epoch 381/500, Loss: 3.2959\n",
      "Epoch 391/500, Loss: 3.2922\n",
      "Epoch 401/500, Loss: 3.2888\n",
      "Epoch 411/500, Loss: 3.2852\n",
      "Epoch 421/500, Loss: 3.2822\n",
      "Epoch 431/500, Loss: 3.2796\n",
      "Epoch 441/500, Loss: 3.2762\n",
      "Epoch 451/500, Loss: 3.2745\n",
      "Epoch 461/500, Loss: 3.2706\n",
      "Epoch 471/500, Loss: 3.2691\n",
      "Epoch 481/500, Loss: 3.2667\n",
      "Epoch 491/500, Loss: 3.2624\n",
      "Epoch 1/500, Loss: 33.2140\n",
      "Epoch 11/500, Loss: 4.8727\n",
      "Epoch 21/500, Loss: 4.0721\n",
      "Epoch 31/500, Loss: 3.8308\n",
      "Epoch 41/500, Loss: 3.7230\n",
      "Epoch 51/500, Loss: 3.6545\n",
      "Epoch 61/500, Loss: 3.6094\n",
      "Epoch 71/500, Loss: 3.5766\n",
      "Epoch 81/500, Loss: 3.5446\n",
      "Epoch 91/500, Loss: 3.5182\n",
      "Epoch 101/500, Loss: 3.4998\n",
      "Epoch 111/500, Loss: 3.4756\n",
      "Epoch 121/500, Loss: 3.4599\n",
      "Epoch 131/500, Loss: 3.4463\n",
      "Epoch 141/500, Loss: 3.4312\n",
      "Epoch 151/500, Loss: 3.4159\n",
      "Epoch 161/500, Loss: 3.4091\n",
      "Epoch 171/500, Loss: 3.4011\n",
      "Epoch 181/500, Loss: 3.3879\n",
      "Epoch 191/500, Loss: 3.3826\n",
      "Epoch 201/500, Loss: 3.3726\n",
      "Epoch 211/500, Loss: 3.3622\n",
      "Epoch 221/500, Loss: 3.3551\n",
      "Epoch 231/500, Loss: 3.3505\n",
      "Epoch 241/500, Loss: 3.3444\n",
      "Epoch 251/500, Loss: 3.3364\n",
      "Epoch 261/500, Loss: 3.3307\n",
      "Epoch 271/500, Loss: 3.3246\n",
      "Epoch 281/500, Loss: 3.3218\n",
      "Epoch 291/500, Loss: 3.3171\n",
      "Epoch 301/500, Loss: 3.3129\n",
      "Epoch 311/500, Loss: 3.3094\n",
      "Epoch 321/500, Loss: 3.3043\n",
      "Epoch 331/500, Loss: 3.2990\n",
      "Epoch 341/500, Loss: 3.2948\n",
      "Epoch 351/500, Loss: 3.2893\n",
      "Epoch 361/500, Loss: 3.2858\n",
      "Epoch 371/500, Loss: 3.2823\n",
      "Epoch 381/500, Loss: 3.2778\n",
      "Epoch 391/500, Loss: 3.2767\n",
      "Epoch 401/500, Loss: 3.2711\n",
      "Epoch 411/500, Loss: 3.2676\n",
      "Epoch 421/500, Loss: 3.2649\n",
      "Epoch 431/500, Loss: 3.2642\n",
      "Epoch 441/500, Loss: 3.2600\n",
      "Epoch 451/500, Loss: 3.2580\n",
      "Epoch 461/500, Loss: 3.2553\n",
      "Epoch 471/500, Loss: 3.2513\n",
      "Epoch 481/500, Loss: 3.2498\n",
      "Epoch 491/500, Loss: 3.2471\n",
      "Epoch 1/500, Loss: 37.9989\n",
      "Epoch 11/500, Loss: 5.1764\n",
      "Epoch 21/500, Loss: 4.3146\n",
      "Epoch 31/500, Loss: 4.0119\n",
      "Epoch 41/500, Loss: 3.8761\n",
      "Epoch 51/500, Loss: 3.7909\n",
      "Epoch 61/500, Loss: 3.7337\n",
      "Epoch 71/500, Loss: 3.6748\n",
      "Epoch 81/500, Loss: 3.6347\n",
      "Epoch 91/500, Loss: 3.6025\n",
      "Epoch 101/500, Loss: 3.5768\n",
      "Epoch 111/500, Loss: 3.5424\n",
      "Epoch 121/500, Loss: 3.5264\n",
      "Epoch 131/500, Loss: 3.5029\n",
      "Epoch 141/500, Loss: 3.4867\n",
      "Epoch 151/500, Loss: 3.4695\n",
      "Epoch 161/500, Loss: 3.4542\n",
      "Epoch 171/500, Loss: 3.4390\n",
      "Epoch 181/500, Loss: 3.4276\n",
      "Epoch 191/500, Loss: 3.4143\n",
      "Epoch 201/500, Loss: 3.4057\n",
      "Epoch 211/500, Loss: 3.3922\n",
      "Epoch 221/500, Loss: 3.3850\n",
      "Epoch 231/500, Loss: 3.3739\n",
      "Epoch 241/500, Loss: 3.3647\n",
      "Epoch 251/500, Loss: 3.3554\n",
      "Epoch 261/500, Loss: 3.3491\n",
      "Epoch 271/500, Loss: 3.3424\n",
      "Epoch 281/500, Loss: 3.3359\n",
      "Epoch 291/500, Loss: 3.3298\n",
      "Epoch 301/500, Loss: 3.3227\n",
      "Epoch 311/500, Loss: 3.3179\n",
      "Epoch 321/500, Loss: 3.3121\n",
      "Epoch 331/500, Loss: 3.3079\n",
      "Epoch 341/500, Loss: 3.3036\n",
      "Epoch 351/500, Loss: 3.2960\n",
      "Epoch 361/500, Loss: 3.2918\n",
      "Epoch 371/500, Loss: 3.2881\n",
      "Epoch 381/500, Loss: 3.2835\n",
      "Epoch 391/500, Loss: 3.2787\n",
      "Epoch 401/500, Loss: 3.2747\n",
      "Epoch 411/500, Loss: 3.2719\n",
      "Epoch 421/500, Loss: 3.2690\n",
      "Epoch 431/500, Loss: 3.2662\n",
      "Epoch 441/500, Loss: 3.2630\n",
      "Epoch 451/500, Loss: 3.2605\n",
      "Epoch 461/500, Loss: 3.2572\n",
      "Epoch 471/500, Loss: 3.2551\n",
      "Epoch 481/500, Loss: 3.2520\n",
      "Epoch 491/500, Loss: 3.2492\n",
      "Epoch 1/500, Loss: 45.2289\n",
      "Epoch 11/500, Loss: 5.0474\n",
      "Epoch 21/500, Loss: 4.2997\n",
      "Epoch 31/500, Loss: 4.0154\n",
      "Epoch 41/500, Loss: 3.8641\n",
      "Epoch 51/500, Loss: 3.7690\n",
      "Epoch 61/500, Loss: 3.6981\n",
      "Epoch 71/500, Loss: 3.6478\n",
      "Epoch 81/500, Loss: 3.6082\n",
      "Epoch 91/500, Loss: 3.5791\n",
      "Epoch 101/500, Loss: 3.5522\n",
      "Epoch 111/500, Loss: 3.5319\n",
      "Epoch 121/500, Loss: 3.5142\n",
      "Epoch 131/500, Loss: 3.4939\n",
      "Epoch 141/500, Loss: 3.4741\n",
      "Epoch 151/500, Loss: 3.4603\n",
      "Epoch 161/500, Loss: 3.4485\n",
      "Epoch 171/500, Loss: 3.4349\n",
      "Epoch 181/500, Loss: 3.4260\n",
      "Epoch 191/500, Loss: 3.4147\n",
      "Epoch 201/500, Loss: 3.4053\n",
      "Epoch 211/500, Loss: 3.3930\n",
      "Epoch 221/500, Loss: 3.3827\n",
      "Epoch 231/500, Loss: 3.3759\n",
      "Epoch 241/500, Loss: 3.3692\n",
      "Epoch 251/500, Loss: 3.3583\n",
      "Epoch 261/500, Loss: 3.3516\n",
      "Epoch 271/500, Loss: 3.3454\n",
      "Epoch 281/500, Loss: 3.3377\n",
      "Epoch 291/500, Loss: 3.3334\n",
      "Epoch 301/500, Loss: 3.3290\n",
      "Epoch 311/500, Loss: 3.3209\n",
      "Epoch 321/500, Loss: 3.3179\n",
      "Epoch 331/500, Loss: 3.3129\n",
      "Epoch 341/500, Loss: 3.3082\n",
      "Epoch 351/500, Loss: 3.3054\n",
      "Epoch 361/500, Loss: 3.3000\n",
      "Epoch 371/500, Loss: 3.2963\n",
      "Epoch 381/500, Loss: 3.2905\n",
      "Epoch 391/500, Loss: 3.2869\n",
      "Epoch 401/500, Loss: 3.2829\n",
      "Epoch 411/500, Loss: 3.2781\n",
      "Epoch 421/500, Loss: 3.2760\n",
      "Epoch 431/500, Loss: 3.2727\n",
      "Epoch 441/500, Loss: 3.2706\n",
      "Epoch 451/500, Loss: 3.2656\n",
      "Epoch 461/500, Loss: 3.2623\n",
      "Epoch 471/500, Loss: 3.2601\n",
      "Epoch 481/500, Loss: 3.2572\n",
      "Epoch 491/500, Loss: 3.2546\n",
      "Epoch 1/500, Loss: 36.9446\n",
      "Epoch 11/500, Loss: 5.1256\n",
      "Epoch 21/500, Loss: 4.1972\n",
      "Epoch 31/500, Loss: 3.9116\n",
      "Epoch 41/500, Loss: 3.7805\n",
      "Epoch 51/500, Loss: 3.7097\n",
      "Epoch 61/500, Loss: 3.6610\n",
      "Epoch 71/500, Loss: 3.6256\n",
      "Epoch 81/500, Loss: 3.5963\n",
      "Epoch 91/500, Loss: 3.5672\n",
      "Epoch 101/500, Loss: 3.5403\n",
      "Epoch 111/500, Loss: 3.5231\n",
      "Epoch 121/500, Loss: 3.5045\n",
      "Epoch 131/500, Loss: 3.4885\n",
      "Epoch 141/500, Loss: 3.4703\n",
      "Epoch 151/500, Loss: 3.4596\n",
      "Epoch 161/500, Loss: 3.4468\n",
      "Epoch 171/500, Loss: 3.4353\n",
      "Epoch 181/500, Loss: 3.4243\n",
      "Epoch 191/500, Loss: 3.4100\n",
      "Epoch 201/500, Loss: 3.4042\n",
      "Epoch 211/500, Loss: 3.3963\n",
      "Epoch 221/500, Loss: 3.3875\n",
      "Epoch 231/500, Loss: 3.3798\n",
      "Epoch 241/500, Loss: 3.3714\n",
      "Epoch 251/500, Loss: 3.3646\n",
      "Epoch 261/500, Loss: 3.3549\n",
      "Epoch 271/500, Loss: 3.3503\n",
      "Epoch 281/500, Loss: 3.3486\n",
      "Epoch 291/500, Loss: 3.3385\n",
      "Epoch 301/500, Loss: 3.3334\n",
      "Epoch 311/500, Loss: 3.3286\n",
      "Epoch 321/500, Loss: 3.3245\n",
      "Epoch 331/500, Loss: 3.3183\n",
      "Epoch 341/500, Loss: 3.3136\n",
      "Epoch 351/500, Loss: 3.3096\n",
      "Epoch 361/500, Loss: 3.3066\n",
      "Epoch 371/500, Loss: 3.3014\n",
      "Epoch 381/500, Loss: 3.2975\n",
      "Epoch 391/500, Loss: 3.2941\n",
      "Epoch 401/500, Loss: 3.2897\n",
      "Epoch 411/500, Loss: 3.2873\n",
      "Epoch 421/500, Loss: 3.2851\n",
      "Epoch 431/500, Loss: 3.2824\n",
      "Epoch 441/500, Loss: 3.2789\n",
      "Epoch 451/500, Loss: 3.2759\n",
      "Epoch 461/500, Loss: 3.2741\n",
      "Epoch 471/500, Loss: 3.2718\n",
      "Epoch 481/500, Loss: 3.2682\n",
      "Epoch 491/500, Loss: 3.2660\n",
      "Epoch 1/500, Loss: 39.2338\n",
      "Epoch 11/500, Loss: 5.0509\n",
      "Epoch 21/500, Loss: 4.1519\n",
      "Epoch 31/500, Loss: 3.8952\n",
      "Epoch 41/500, Loss: 3.7872\n",
      "Epoch 51/500, Loss: 3.7083\n",
      "Epoch 61/500, Loss: 3.6621\n",
      "Epoch 71/500, Loss: 3.6233\n",
      "Epoch 81/500, Loss: 3.5885\n",
      "Epoch 91/500, Loss: 3.5582\n",
      "Epoch 101/500, Loss: 3.5301\n",
      "Epoch 111/500, Loss: 3.5063\n",
      "Epoch 121/500, Loss: 3.4885\n",
      "Epoch 131/500, Loss: 3.4746\n",
      "Epoch 141/500, Loss: 3.4536\n",
      "Epoch 151/500, Loss: 3.4389\n",
      "Epoch 161/500, Loss: 3.4261\n",
      "Epoch 171/500, Loss: 3.4138\n",
      "Epoch 181/500, Loss: 3.4014\n",
      "Epoch 191/500, Loss: 3.3898\n",
      "Epoch 201/500, Loss: 3.3800\n",
      "Epoch 211/500, Loss: 3.3707\n",
      "Epoch 221/500, Loss: 3.3616\n",
      "Epoch 231/500, Loss: 3.3566\n",
      "Epoch 241/500, Loss: 3.3479\n",
      "Epoch 251/500, Loss: 3.3426\n",
      "Epoch 261/500, Loss: 3.3340\n",
      "Epoch 271/500, Loss: 3.3288\n",
      "Epoch 281/500, Loss: 3.3230\n",
      "Epoch 291/500, Loss: 3.3188\n",
      "Epoch 301/500, Loss: 3.3131\n",
      "Epoch 311/500, Loss: 3.3109\n",
      "Epoch 321/500, Loss: 3.3053\n",
      "Epoch 331/500, Loss: 3.2987\n",
      "Epoch 341/500, Loss: 3.2944\n",
      "Epoch 351/500, Loss: 3.2914\n",
      "Epoch 361/500, Loss: 3.2893\n",
      "Epoch 371/500, Loss: 3.2830\n",
      "Epoch 381/500, Loss: 3.2797\n",
      "Epoch 391/500, Loss: 3.2770\n",
      "Epoch 401/500, Loss: 3.2723\n",
      "Epoch 411/500, Loss: 3.2685\n",
      "Epoch 421/500, Loss: 3.2654\n",
      "Epoch 431/500, Loss: 3.2633\n",
      "Epoch 441/500, Loss: 3.2587\n",
      "Epoch 451/500, Loss: 3.2553\n",
      "Epoch 461/500, Loss: 3.2525\n",
      "Epoch 471/500, Loss: 3.2514\n",
      "Epoch 481/500, Loss: 3.2473\n",
      "Epoch 491/500, Loss: 3.2460\n",
      "Epoch 1/500, Loss: 37.2531\n",
      "Epoch 11/500, Loss: 5.1805\n",
      "Epoch 21/500, Loss: 4.3959\n",
      "Epoch 31/500, Loss: 4.0199\n",
      "Epoch 41/500, Loss: 3.8538\n",
      "Epoch 51/500, Loss: 3.7663\n",
      "Epoch 61/500, Loss: 3.7110\n",
      "Epoch 71/500, Loss: 3.6631\n",
      "Epoch 81/500, Loss: 3.6274\n",
      "Epoch 91/500, Loss: 3.6002\n",
      "Epoch 101/500, Loss: 3.5754\n",
      "Epoch 111/500, Loss: 3.5501\n",
      "Epoch 121/500, Loss: 3.5305\n",
      "Epoch 131/500, Loss: 3.5084\n",
      "Epoch 141/500, Loss: 3.4941\n",
      "Epoch 151/500, Loss: 3.4778\n",
      "Epoch 161/500, Loss: 3.4668\n",
      "Epoch 171/500, Loss: 3.4506\n",
      "Epoch 181/500, Loss: 3.4390\n",
      "Epoch 191/500, Loss: 3.4272\n",
      "Epoch 201/500, Loss: 3.4145\n",
      "Epoch 211/500, Loss: 3.4035\n",
      "Epoch 221/500, Loss: 3.3937\n",
      "Epoch 231/500, Loss: 3.3844\n",
      "Epoch 241/500, Loss: 3.3743\n",
      "Epoch 251/500, Loss: 3.3679\n",
      "Epoch 261/500, Loss: 3.3594\n",
      "Epoch 271/500, Loss: 3.3523\n",
      "Epoch 281/500, Loss: 3.3474\n",
      "Epoch 291/500, Loss: 3.3397\n",
      "Epoch 301/500, Loss: 3.3356\n",
      "Epoch 311/500, Loss: 3.3297\n",
      "Epoch 321/500, Loss: 3.3248\n",
      "Epoch 331/500, Loss: 3.3199\n",
      "Epoch 341/500, Loss: 3.3141\n",
      "Epoch 351/500, Loss: 3.3105\n",
      "Epoch 361/500, Loss: 3.3056\n",
      "Epoch 371/500, Loss: 3.3019\n",
      "Epoch 381/500, Loss: 3.2962\n",
      "Epoch 391/500, Loss: 3.2928\n",
      "Epoch 401/500, Loss: 3.2908\n",
      "Epoch 411/500, Loss: 3.2870\n",
      "Epoch 421/500, Loss: 3.2842\n",
      "Epoch 431/500, Loss: 3.2793\n",
      "Epoch 441/500, Loss: 3.2774\n",
      "Epoch 451/500, Loss: 3.2755\n",
      "Epoch 461/500, Loss: 3.2718\n",
      "Epoch 471/500, Loss: 3.2688\n",
      "Epoch 481/500, Loss: 3.2645\n",
      "Epoch 491/500, Loss: 3.2622\n",
      "Epoch 1/500, Loss: 37.6842\n",
      "Epoch 11/500, Loss: 5.0250\n",
      "Epoch 21/500, Loss: 4.1340\n",
      "Epoch 31/500, Loss: 3.8693\n",
      "Epoch 41/500, Loss: 3.7641\n",
      "Epoch 51/500, Loss: 3.6990\n",
      "Epoch 61/500, Loss: 3.6445\n",
      "Epoch 71/500, Loss: 3.6020\n",
      "Epoch 81/500, Loss: 3.5669\n",
      "Epoch 91/500, Loss: 3.5403\n",
      "Epoch 101/500, Loss: 3.5185\n",
      "Epoch 111/500, Loss: 3.4991\n",
      "Epoch 121/500, Loss: 3.4805\n",
      "Epoch 131/500, Loss: 3.4660\n",
      "Epoch 141/500, Loss: 3.4559\n",
      "Epoch 151/500, Loss: 3.4411\n",
      "Epoch 161/500, Loss: 3.4266\n",
      "Epoch 171/500, Loss: 3.4164\n",
      "Epoch 181/500, Loss: 3.4032\n",
      "Epoch 191/500, Loss: 3.3924\n",
      "Epoch 201/500, Loss: 3.3795\n",
      "Epoch 211/500, Loss: 3.3739\n",
      "Epoch 221/500, Loss: 3.3682\n",
      "Epoch 231/500, Loss: 3.3608\n",
      "Epoch 241/500, Loss: 3.3521\n",
      "Epoch 251/500, Loss: 3.3432\n",
      "Epoch 261/500, Loss: 3.3391\n",
      "Epoch 271/500, Loss: 3.3313\n",
      "Epoch 281/500, Loss: 3.3252\n",
      "Epoch 291/500, Loss: 3.3209\n",
      "Epoch 301/500, Loss: 3.3167\n",
      "Epoch 311/500, Loss: 3.3137\n",
      "Epoch 321/500, Loss: 3.3054\n",
      "Epoch 331/500, Loss: 3.3010\n",
      "Epoch 341/500, Loss: 3.2988\n",
      "Epoch 351/500, Loss: 3.2934\n",
      "Epoch 361/500, Loss: 3.2889\n",
      "Epoch 371/500, Loss: 3.2850\n",
      "Epoch 381/500, Loss: 3.2824\n",
      "Epoch 391/500, Loss: 3.2786\n",
      "Epoch 401/500, Loss: 3.2745\n",
      "Epoch 411/500, Loss: 3.2729\n",
      "Epoch 421/500, Loss: 3.2689\n",
      "Epoch 431/500, Loss: 3.2651\n",
      "Epoch 441/500, Loss: 3.2626\n",
      "Epoch 451/500, Loss: 3.2598\n",
      "Epoch 461/500, Loss: 3.2576\n",
      "Epoch 471/500, Loss: 3.2553\n",
      "Epoch 481/500, Loss: 3.2521\n",
      "Epoch 491/500, Loss: 3.2482\n"
     ]
    }
   ],
   "source": [
    "model_zoo, variables_selected, representatives = create_model_zoo(data_tensor, id_class_dictionary, keys, other_genes, n_models=50, n_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "89e539b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_to_position_mapper = {}\n",
    "max_un = (len(variables_selected[0])-len(representatives[0]))\n",
    "for i in range(len(gene_names)):\n",
    "    if i < (len(variables_selected[0])-len(representatives[0])):\n",
    "        gene_to_position_mapper[i] = i\n",
    "\n",
    "start_idx = max_un\n",
    "current_idx = max_un\n",
    "for k in cluster_mapping:\n",
    "    for elem in cluster_mapping[k]:\n",
    "        gene_to_position_mapper[current_idx] =start_idx \n",
    "        current_idx = current_idx+1\n",
    "    start_idx = start_idx+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "45a881dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 461/461 [10:25<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "AnnData object with n_obs  n_vars = 1000  461\n",
      "Creating anndata\n",
      "Setting vars\n"
     ]
    }
   ],
   "source": [
    "def set_mu_true(model_zoo):\n",
    "    # forward only mu\n",
    "    for mo in model_zoo:\n",
    "        mo.forward_mu_only = True\n",
    "        mo.forward_theta_only = False\n",
    "        mo.latent_only = False\n",
    "        mo.forward_pi_only = False\n",
    "    return model_zoo\n",
    "\n",
    "def all_false(model_zoo):\n",
    "    # forward only mu\n",
    "    for mo in model_zoo:\n",
    "        mo.forward_mu_only = False\n",
    "        mo.forward_theta_only = False\n",
    "        mo.latent_only = False\n",
    "        mo.forward_pi_only = False\n",
    "    return model_zoo\n",
    "\n",
    "model_zoo  = all_false(model_zoo)\n",
    "\n",
    "\n",
    "config.raw_attribution = True\n",
    "config.percentile = 55\n",
    "grn_adata = wrapper(model_zoo, data_tensor.cuda(), gene_names, variables_selected,  config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "70b7b009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>edge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AR</td>\n",
       "      <td>AATF</td>\n",
       "      <td>AR_AATF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AR</td>\n",
       "      <td>ABCC4</td>\n",
       "      <td>AR_ABCC4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AR</td>\n",
       "      <td>ABCE1</td>\n",
       "      <td>AR_ABCE1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR</td>\n",
       "      <td>ADAM9</td>\n",
       "      <td>AR_ADAM9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AR</td>\n",
       "      <td>AGR3</td>\n",
       "      <td>AR_AGR3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>AR</td>\n",
       "      <td>UGT2B17</td>\n",
       "      <td>AR_UGT2B17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>AR</td>\n",
       "      <td>URI1</td>\n",
       "      <td>AR_URI1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>AR</td>\n",
       "      <td>USP26</td>\n",
       "      <td>AR_USP26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>AR</td>\n",
       "      <td>VAPA</td>\n",
       "      <td>AR_VAPA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>AR</td>\n",
       "      <td>XPA</td>\n",
       "      <td>AR_XPA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    source   target        edge\n",
       "0       AR     AATF     AR_AATF\n",
       "1       AR    ABCC4    AR_ABCC4\n",
       "2       AR    ABCE1    AR_ABCE1\n",
       "3       AR    ADAM9    AR_ADAM9\n",
       "4       AR     AGR3     AR_AGR3\n",
       "..     ...      ...         ...\n",
       "118     AR  UGT2B17  AR_UGT2B17\n",
       "119     AR     URI1     AR_URI1\n",
       "120     AR    USP26    AR_USP26\n",
       "121     AR     VAPA     AR_VAPA\n",
       "122     AR      XPA      AR_XPA\n",
       "\n",
       "[123 rows x 3 columns]"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "grn_adata.obs['grn'] = pd.Categorical(adata.obs['grn'])\n",
    "grn_adata[:, pd.DataFrame(grn_adata[grn_adata.obs['grn']==2.0].layers['counter']).sum()>200].var.merge(net1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "5d0f993e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        487.840\n",
       "1         27.540\n",
       "2          0.846\n",
       "3         11.200\n",
       "4         79.020\n",
       "           ...  \n",
       "212516    50.840\n",
       "212517    42.460\n",
       "212518   485.880\n",
       "212519   394.380\n",
       "212520   133.540\n",
       "Length: 212521, dtype: float64"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grn_adata[grn_adata.obs['grn']==2.0].layers['counter']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "b63ff8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>edge</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ESRRA</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESRRG</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOXD4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KDM3B</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KLF16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCOR2</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NR0B1</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NR2F1</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NRIP1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ONECUT2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PPARA</th>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PPARD</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PPARG</th>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PPARGC1B</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RXRA</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRIB3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF692</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF746</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          target  edge\n",
       "source                \n",
       "ESRRA         15    15\n",
       "ESRRG          9     9\n",
       "FOXD4          1     1\n",
       "KDM3B          1     1\n",
       "KLF16          1     1\n",
       "MED23          1     1\n",
       "NCOR2         13    13\n",
       "NR0B1          9     9\n",
       "NR2F1         21    21\n",
       "NRIP1          1     1\n",
       "ONECUT2        1     1\n",
       "PPARA         87    87\n",
       "PPARD         36    36\n",
       "PPARG         91    91\n",
       "PPARGC1B       3     3\n",
       "RXRA          27    27\n",
       "TRIB3          1     1\n",
       "ZNF692         1     1\n",
       "ZNF746         1     1"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net0.groupby('source').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "08a89437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AATF_AATF</th>\n",
       "      <td>AATF</td>\n",
       "      <td>AATF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABCC4_AATF</th>\n",
       "      <td>ABCC4</td>\n",
       "      <td>AATF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABCE1_AATF</th>\n",
       "      <td>ABCE1</td>\n",
       "      <td>AATF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABCG1_AATF</th>\n",
       "      <td>ABCG1</td>\n",
       "      <td>AATF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABCG8_AATF</th>\n",
       "      <td>ABCG8</td>\n",
       "      <td>AATF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZBTB20_ZNF746</th>\n",
       "      <td>ZBTB20</td>\n",
       "      <td>ZNF746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZBTB22_ZNF746</th>\n",
       "      <td>ZBTB22</td>\n",
       "      <td>ZNF746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZBTB7C_ZNF746</th>\n",
       "      <td>ZBTB7C</td>\n",
       "      <td>ZNF746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF513_ZNF746</th>\n",
       "      <td>ZNF513</td>\n",
       "      <td>ZNF746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF692_ZNF746</th>\n",
       "      <td>ZNF692</td>\n",
       "      <td>ZNF746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93887 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               source  target\n",
       "index                        \n",
       "AATF_AATF        AATF    AATF\n",
       "ABCC4_AATF      ABCC4    AATF\n",
       "ABCE1_AATF      ABCE1    AATF\n",
       "ABCG1_AATF      ABCG1    AATF\n",
       "ABCG8_AATF      ABCG8    AATF\n",
       "...               ...     ...\n",
       "ZBTB20_ZNF746  ZBTB20  ZNF746\n",
       "ZBTB22_ZNF746  ZBTB22  ZNF746\n",
       "ZBTB7C_ZNF746  ZBTB7C  ZNF746\n",
       "ZNF513_ZNF746  ZNF513  ZNF746\n",
       "ZNF692_ZNF746  ZNF692  ZNF746\n",
       "\n",
       "[93887 rows x 2 columns]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grn_adata[:, pd.DataFrame(grn_adata.layers['counter']).sum()>250].var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "33d4352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribution_one_target( \n",
    "        target_gene,\n",
    "        lrp_model,\n",
    "        input_data,\n",
    "        background,\n",
    "        selected_variables,\n",
    "        xai_type='lrp-like',\n",
    "        randomize_background = False):\n",
    "    \n",
    "    attributions_list = []\n",
    "    for m in range(len(lrp_model)):\n",
    "\n",
    "        # Randomize backgorund for each round\n",
    "        if randomize_background:\n",
    "            current_background = background[:, selected_variables[m]]\n",
    "            current_background = shuffle_each_column_independently(current_background)\n",
    "\n",
    "        current_data = input_data[:, selected_variables[m]]\n",
    "        model = lrp_model[m]\n",
    "        #for _ in range(num_iterations):\n",
    "        if xai_type == 'lrp-like':\n",
    "            attribution = model.attribute(current_data, target=target_gene)\n",
    "                \n",
    "        elif xai_type == 'shap-like':\n",
    "            attribution = model.attribute(current_data, baselines = current_background, target = target_gene)\n",
    "\n",
    "        attributions_list.append(attribution.detach().cpu().numpy())\n",
    "    return attributions_list\n",
    "\n",
    "\n",
    "def attribution_one_target_one_model( \n",
    "        target_gene,\n",
    "        lrp_model,\n",
    "        input_data,\n",
    "        background,\n",
    "        selected_variables,\n",
    "        xai_type='lrp-like',\n",
    "        randomize_background = False):\n",
    "    \n",
    "    # Randomize backgorund for each round\n",
    "    if randomize_background:\n",
    "        background = shuffle_each_column_independently(background)\n",
    "\n",
    "    model = lrp_model\n",
    "    #for _ in range(num_iterations):\n",
    "    if xai_type == 'lrp-like':\n",
    "        #print(input_data)\n",
    "        #print(target_gene)\n",
    "        attribution = model.attribute(input_data, target=target_gene)\n",
    "            \n",
    "    elif xai_type == 'shap-like':\n",
    "        attribution = model.attribute(input_data, baselines = background, target = target_gene)\n",
    "\n",
    "    return attribution.detach().cpu().numpy()\n",
    "\n",
    "def get_top_edges_per_cell(grn_adata, top_edges):\n",
    "    \n",
    "    counters = np.zeros((grn_adata.shape))\n",
    "\n",
    "    idex = grn_adata.shape[1]-top_edges\n",
    "    b = np.argpartition(grn_adata, idex, axis=1)[:, idex:]\n",
    "\n",
    "    np.put_along_axis(counters, b, 1, axis=1)\n",
    "    return counters\n",
    "\n",
    "    \n",
    "def assemble_attributions(current_attr, variables_selected, a_shape):\n",
    "    assembled_attributions = np.zeros(a_shape)\n",
    "    column_counter = np.zeros(a_shape[1])\n",
    "    for i in range(len(variables_selected)):\n",
    "        for j in range(len(variables_selected[i])):\n",
    "            assembled_attributions[:, variables_selected[i][j]] += current_attr[i][:, j]\n",
    "            column_counter[variables_selected[i][j]] +=1\n",
    "    for i in range(len(column_counter)):\n",
    "        if column_counter[i] == 0:\n",
    "            column_counter[i] =1\n",
    "    assembled_attributions = assembled_attributions/column_counter\n",
    "    return assembled_attributions, column_counter\n",
    "\n",
    "def wrapper(models, data_train_full_tensor, gene_names, variables_selected, config):\n",
    "\n",
    "    data = data_train_full_tensor.detach().cpu().numpy()\n",
    "    tms = []\n",
    "    name_list = []\n",
    "    target_names = []\n",
    "\n",
    "\n",
    "    for trained_model in models:        \n",
    "        trained_model.forward_mu_only = True\n",
    "        explainer, xai_type = get_explainer(trained_model, config.xai_method, config.raw_attribution)\n",
    "        tms.append(explainer)\n",
    "\n",
    "    attributions = []\n",
    "    all_counter = []\n",
    "    ## ATTRIBUTIONS\n",
    "    for g in tqdm(range(data_train_full_tensor.shape[1])):\n",
    "    #for g in tqdm(range(len(variables_selected[0]))):\n",
    "        current_gene = gene_to_position_mapper[g]\n",
    "        current_attributions = []\n",
    "        counter_list = []\n",
    "        for m in range(len(tms)):\n",
    "\n",
    "            attributions_list = attribution_one_target_one_model(\n",
    "                current_gene,\n",
    "                tms[m], # Select the correct model\n",
    "                data_train_full_tensor[:, variables_selected[m]],\n",
    "                data_train_full_tensor[:, variables_selected[m]],\n",
    "                variables_selected, \n",
    "                xai_type=xai_type,\n",
    "                randomize_background = True)\n",
    "            counters = get_top_edges_per_cell(attributions_list, 100)\n",
    "            counter_list.append(counters)\n",
    "            current_attributions.append(attributions_list)\n",
    "\n",
    "        current_attributions, column_counter = assemble_attributions(current_attributions, variables_selected, data_train_full_tensor.shape)\n",
    "        counter_list, col = assemble_attributions(counter_list, variables_selected, data_train_full_tensor.shape)\n",
    "        attributions.append(current_attributions)\n",
    "        all_counter.append(counter_list)\n",
    "\n",
    "\n",
    "\n",
    "    ## AGGREGATION: REPLACE LIST BY AGGREGATED DATA\n",
    "    # for i in range(len(attributions)):\n",
    "    #     # CURRENTLY MEAN\n",
    "    #     attributions[i] = aggregate_attributions(attributions[i], strategy=config.aggregation_strategy )\n",
    "    \n",
    "\n",
    "    ## PENALIZE:\n",
    "    if config.penalty != 'None':\n",
    "        penalty_matrix = compute_correlation_metric(data, cor_type=config.penalty)\n",
    "        for i in range(len(attributions)):\n",
    "            # CURRENTLY MEAN\n",
    "            attributions[i] = np.dot(attributions[i], (1-penalty_matrix))\n",
    "\n",
    "    \n",
    "    ## CLUSTERING: CLUSTER EACH TARGET INDVIDUALLY\n",
    "    for i in range(len(attributions)):\n",
    " \n",
    "        attributions[i] = ad.AnnData(attributions[i])\n",
    "        print(attributions[i])\n",
    "        sc.pp.scale(attributions[i])\n",
    "        try:\n",
    "            sc.pp.pca(attributions[i],n_comps=50)\n",
    "        except:\n",
    "            try:\n",
    "                sc.pp.pca(attributions[i],n_comps=50 )\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "        sc.pp.neighbors(attributions[i], n_neighbors=15)\n",
    "        sc.tl.leiden(attributions[i], resolution=0.1)\n",
    "\n",
    "        #clusterings[f'T_{gene_names[i]}'] = np.array(attributions[i].obs['leiden'])\n",
    "\n",
    "    #EDGE SELECTION:\n",
    "    for i in range(len(attributions)):\n",
    "        edge_indices = get_edges(attributions[i], use_differential=config.use_differential, percentile=config.percentile)\n",
    "        name_list = name_list + list(gene_names[edge_indices])\n",
    "        target_names = target_names+[gene_names[i]]* len(edge_indices)\n",
    "        attributions[i] = attributions[i][:,edge_indices].X\n",
    "        all_counter[i] = all_counter[i][:, edge_indices]\n",
    "\n",
    "    attributions = np.hstack(attributions)\n",
    "    all_counter = np.hstack(all_counter)\n",
    "\n",
    "    \n",
    "    index_list = [f\"{s}_{t}\" for (s, t) in zip(name_list, target_names)]\n",
    "    cou = pd.DataFrame({'index': index_list, 'source':name_list, 'target':target_names})\n",
    "    cou = cou.set_index('index')\n",
    "\n",
    "    #clusterings = pd.DataFrame(clusterings)\n",
    "\n",
    "    #grn_adata = attribution_to_anndata(attributions, var=cou, obs = clusterings)\n",
    "    grn_adata = attribution_to_anndata(attributions, var=cou)\n",
    "    grn_adata.layers['counter'] = all_counter\n",
    "    return grn_adata\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "468c3c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 5]\n",
      " [5 2]\n",
      " [5 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 1.],\n",
       "       [0., 0., 1., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = np.zeros((3,6))\n",
    "ar = np.array([[1,2,3,4,5,6], [2,2,5,1,2,3], [1,212,2,2,1,6]])\n",
    "p = np.argpartition(np.array([[1,2,3,4,5,6], [2,2,5,1,2,3], [1,212,2,2,1,6]]), 4, axis=1)[:, 4:]\n",
    "print(p)\n",
    "np.put_along_axis(counter, p, 1, axis=1)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461390ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import itertools\n",
    "\n",
    "def find_consistent_pairs(grn_adata, gene_names):\n",
    "    \"\"\"\n",
    "    Creates a dictionary of binary masks for each cell and pair of genes,\n",
    "    including both forward, reverse, and self-pairs (which are all zeros).\n",
    "\n",
    "    Args:\n",
    "        matrix_cells_x_genes (np.ndarray): A 2D numpy array where rows are cells\n",
    "                                          and columns are genes.\n",
    "        gene_list (list): A list of strings containing the names of the genes,\n",
    "                          in the same order as the columns in the matrix.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are gene pair strings (e.g., 'GeneA_GeneB')\n",
    "              and values are 1D numpy arrays representing the binary mask for that pair\n",
    "              across all cells.\n",
    "    \"\"\"\n",
    "\n",
    "    num_cells, num_edges = grn_adata.X.shape\n",
    "\n",
    "    pairwise_mask_dict = {}\n",
    "\n",
    "    gene_pairs_indices = list(itertools.combinations(gene_names, 2))\n",
    "    for g1_idx, g2_idx in gene_pairs_indices:\n",
    "        pairwise_mask_dict[f\"{g1_idx}_{g2_idx}\"] = st.spearmanr(grn_adata[:, [f\"{g1_idx}_{g2_idx}\", f\"{g2_idx}_{g1_idx}\"]].X).statistic\n",
    "        # add reverse bc I am lazy\n",
    "        pairwise_mask_dict[f\"{g2_idx}_{g1_idx}\"] = pairwise_mask_dict[f\"{g1_idx}_{g2_idx}\"]\n",
    "    return pairwise_mask_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d98329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pariwise_consistency_1 = find_consistent_pairs(grn_adata[grn_adata.obs['grn'] == 1.0, :].copy(), gene_names1)\n",
    "pariwise_consistency_2 = find_consistent_pairs(grn_adata[grn_adata.obs['grn'] == 2.0, :].copy(), gene_names1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f2638",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_consistency_1 = pd.DataFrame(pariwise_consistency_1.items())\n",
    "pairwise_consistency_2 = pd.DataFrame(pariwise_consistency_2.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed78b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_consistency_1.columns = ['edge', 'spearman']\n",
    "pairwise_consistency_2.columns = ['edge', 'spearman']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd4953",
   "metadata": {},
   "outputs": [],
   "source": [
    "consisten_edges2 = grn_adata[:, pairwise_consistency_2[pairwise_consistency_2.spearman>0.2].edge].var\n",
    "grn_adata[:, pairwise_consistency_1[pairwise_consistency_1.spearman>0.3].edge].var\n",
    "grn_adata[:, pairwise_consistency_2[pairwise_consistency_2.spearman>0.2].edge].var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83225d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grn_adata.obs['grn'] = pd.Categorical(adata.obs['grn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c661693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_edges_per_cell(grn_adata, top_edges):\n",
    "    \n",
    "    idex = grn_adata.shape[1]-top_edges\n",
    "    b = np.argpartition(grn_adata.X, idex, axis=1)    # top 3 values from each row\n",
    "    top_idx = b[:,idex:]\n",
    "\n",
    "\n",
    "    scgenerai_var_index_np = np.array(grn_adata.var)\n",
    "    top_edges_per_cell = pd.DataFrame(np.concat(scgenerai_var_index_np[top_idx]))\n",
    "    top_edges_per_cell.columns = ['source', 'target', 'n_cells']\n",
    "    top_edges_per_cell['cell_barcode'] = np.repeat(grn_adata.obs.index, repeats=top_edges)\n",
    "    return top_edges_per_cell\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9850d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grn_adata = downstream_recipe(grn_adata)\n",
    "#grn_adata.var.columns = ['source', 'target', 'n_cells']\n",
    "# Example UsageCreate the binary mask with both forward and reverse pairs and named strings\n",
    "mask_full = create_pairwise_binary_mask(counts.values, counts.columns)\n",
    "mask_full = dict_to_dataframe(mask_full, grn_adata.var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29042389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grn_adata.layers['mask'] = mask_full\n",
    "grn_adata.layers['masked'] = np.multiply(grn_adata.X, grn_adata.layers['mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb5d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "grn_adata.obs['grn'] = pd.Categorical(adata.obs['grn'])\n",
    "sc.tl.rank_genes_groups(grn_adata, \"grn\", method=\"t-test\", use_raw=False, rankby_abs=True, layer='masked')\n",
    "sc.tl.filter_rank_genes_groups(grn_adata, min_fold_change=3)\n",
    "de_grn = sc.get.rank_genes_groups_df(grn_adata, group='2.0')\n",
    "de_grn[['source', 'target']] = de_grn['names'].str.split('_', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716bd091",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_grn[de_grn.names == 'AR_EPB41L4B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c725639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_grn[de_grn.names == 'EPB41L4B_AR']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd697e1",
   "metadata": {},
   "source": [
    "de_grn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_grn.merge(net1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(adata, keys=[ 'PCK1', 'TRNT1', 'AR', 'EPB41L4B'], groupby='grn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393edd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(grn_adata, keys=['PCK1_TRNT1', 'AR_SORD', 'AR_EPB41L4B'], use_raw=False, groupby='grn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net0 =  nets[0]\n",
    "net0['edge'] = net0['source']+'_'+net0['target']\n",
    "\n",
    "net1 =  nets[1]\n",
    "net1['edge'] = net1['source']+'_'+net1['target']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c532f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs['grn'] = pd.Categorical(adata.obs['grn'])\n",
    "sc.pp.log1p(adata)\n",
    "sc.tl.rank_genes_groups(adata, \"grn\", method=\"wilcoxon\", use_raw=False, rankby_abs=True)\n",
    "sc.tl.filter_rank_genes_groups(adata, min_fold_change=3)\n",
    "de = sc.get.rank_genes_groups_df(adata, group='2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dcf30f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96401c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "grn_adata[0:50000].merge(net1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbbd1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_grn[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b02ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = downstream_recipe(adata)\n",
    "sc.pl.umap(adata, color = ['grn', 'RASGRP3','BDP1', 'CREBBP','VSX2', 'TBP','TAF5', 'FLI1', 'KAT2B', 'IGFBP7', 'AEBP1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb8fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grn_adata.obs['AEBP1'] = adata[:, 'AEBP1'].X.copy().flatten()\n",
    "grn_adata.obs['IGFBP7'] = adata[:, 'IGFBP7'].X.copy().flatten()\n",
    "\n",
    "grn_adata.obs['BBX'] = adata[:, 'BBX'].X.copy().flatten()\n",
    "grn_adata.obs['PHF2'] = adata[:, 'PHF2'].X.copy().flatten()\n",
    "grn_adata.obs['TAF10'] = adata[:, 'TAF10'].X.copy().flatten()\n",
    "sc.pl.umap(grn_adata, color= ['IGFBP7_AEBP1', 'leiden', 'AEBP1', 'IGFBP7', 'IGFBP7_BBX', 'PHF2_TAF10', 'TAF10', 'PHF2'], use_raw=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f351a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd0876",
   "metadata": {},
   "outputs": [],
   "source": [
    "de[(de.pvals_adj<0.01)].merge(net0, left_on='names', right_on='edge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d141ed5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b738a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da8cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "de[de.names == 'KAT2B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7280f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(grn_adata, keys= ['TAT_CNOT4', 'ZBTB8B_ZW10', 'GATA3_BLCAP', 'TAT_ST8SIA2', 'NF1_ADD1', 'GATA3_FNDC3B'], use_raw=False, groupby='grn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e237dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(adata, keys=['TAT','ST8SIA2', 'ZBTB8B', 'ZW10', 'GATA3', 'BLCAP', 'CNOT4', 'ADD1', 'NF1', 'FNDC3B'], groupby='grn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e604201",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene = 'CNOT4'\n",
    "print(net0[net0.source == gene])\n",
    "print(net0[net0.target == gene])\n",
    "print(net1[net1.source == gene])\n",
    "print(net1[net1.target == gene])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e48033",
   "metadata": {},
   "outputs": [],
   "source": [
    "net0[net0.target == 'NF1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d777152",
   "metadata": {},
   "outputs": [],
   "source": [
    "grn_adata.obs['grn'] = pd.Categorical(adata.obs['grn'])\n",
    "top = get_top_edges_per_cell(grn_adata[grn_adata.obs['grn'] == 2.0], 1000)\n",
    "top['edgenames'] = top['source']+'_'+top['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top = top.loc[:, ['source', 'target']].groupby(['source', 'target']).value_counts().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21422a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top.columns = ['source', 'target', 'counter']\n",
    "top.merge(nets[1]).hist('counter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c05af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top.hist('counter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc93cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "top.sort_values('counter', ascending=False)[0:10000].loc[:, ['source']].groupby('source').value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771080ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "grn_adata.obs['grn'] = pd.Categorical(adata.obs['grn'])\n",
    "adata.obs['grn'] = pd.Categorical(adata.obs['grn'])\n",
    "sc.pl.violin(adata, keys=['ZBTB8A', 'ZBTB8B', 'CDKN1A'], groupby='grn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596f2101",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "for key in cluster_mapping:\n",
    "    for i in cluster_mapping[key]:\n",
    "        gene = gene_names[i]\n",
    "        indx = [g for g in cluster_mapping[key] if gene_names[g]!=gene]\n",
    "        mapping[gene] = gene_names[indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d821fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf9de11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext.loc[:, ['source', 'target']].duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69778d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_edges(mapping, top):\n",
    "    expanded = []\n",
    "    for i, edges in top.iterrows():\n",
    "        # Add combinations all all mapped edges in the lists\n",
    "        if edges['source'] in mapping and edges['target'] in mapping:\n",
    "            for se in mapping[edges['source']]:\n",
    "                for st in mapping[edges['target']]:\n",
    "                    expanded.append([se, st])\n",
    "        if edges['source'] in mapping:\n",
    "            for se in mapping[edges['source']]:\n",
    "                expanded.append([se, edges['target']])\n",
    "        if edges['target'] in mapping:\n",
    "            for se in mapping[edges['target']]:\n",
    "                expanded.append([edges['source'], se])\n",
    "\n",
    "    expanded = pd.DataFrame(expanded)\n",
    "    expanded.columns = ['source', 'target']\n",
    "    all_edges = pd.concat([top, expanded])\n",
    "    return all_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a481834",
   "metadata": {},
   "outputs": [],
   "source": [
    "grn_adata = downstream_recipe(grn_adata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c300b5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c061705",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = grn_adata[adata.obs.grn == 2.0, grn_adata.var.source == 'MTA1']\n",
    "gatatop = get_top_edges_per_cell(sub, 75)\n",
    "gatatop = gatatop.loc[:, ['source', 'target']].groupby(['source', 'target']).value_counts().reset_index().sort_values('count', ascending=False)\n",
    "gatatop = gatatop[gatatop['count']>250]\n",
    "gatatop = extend_edges(mapping, gatatop)\n",
    "gatatop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b2557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gatatop.merge(nets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69344da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nets[0][nets[0].source == 'MTA1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b7db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nets[0][nets[0].source == 'GATA3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8cc93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b9d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(all_edges.sort_values('count', ascending=False)[0:200].source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36b3776",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edges.sort_values('count', ascending=False)[0:10000].hist('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e757d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nets0 = nets[0]\n",
    "nets0[nets0.target == 'WTAP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ffc5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edges.sort_values('count', ascending=False).hist('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f458da",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.grn = pd.Categorical(adata.obs.grn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9af861",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edges[all_edges['count']>20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5507f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edges[all_edges['count']>20].sort_values('count', ascending=False).source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c38b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(adata, keys = ['TMPRSS5', 'ITGB3BP'], groupby= 'grn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(adata, keys = np.unique(all_edges.sort_values('count', ascending=False)[0:200].source)[0:5], groupby= 'grn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a27ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(adata, keys = np.unique(all_edges.sort_values('count', ascending=False)[0:200].source)[5:10], groupby= 'grn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6506935",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(adata, keys = np.unique(all_edges.sort_values('count', ascending=False)[0:200].source)[10:15], groupby= 'grn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9892125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(adata, keys = np.unique(all_edges.sort_values('count', ascending=False)[0:200].source)[15:20], groupby= 'grn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e84f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def binarize_and_compute_contingency(df_counts):\n",
    "    \"\"\"\n",
    "    Binarizes raw gene count data and computes pairwise contingency tables efficiently.\n",
    "\n",
    "    Args:\n",
    "        df_counts (pd.DataFrame): A DataFrame with genes as rows and samples as columns.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing four DataFrames for the contingency table values:\n",
    "              'a' (both genes expressed), 'b' (gene A expressed, gene B not),\n",
    "              'c' (gene A not, gene B expressed), and 'd' (neither expressed).\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Binarize the gene expression data\n",
    "    # Convert counts to 1 (expressed) or 0 (not expressed)\n",
    "    df_binary = (df_counts > 0).astype(int)\n",
    "\n",
    "    # Step 2: Compute pairwise contingency using matrix multiplication\n",
    "    # 'a': Both genes are expressed (count > 0 in the same sample)\n",
    "    a_matrix = df_binary.dot(df_binary.T)\n",
    "\n",
    "    # 'd': Neither gene is expressed (count == 0 in the same sample)\n",
    "    # We invert the binary matrix to find where both are 0\n",
    "    df_binary_inverted = 1 - df_binary\n",
    "    d_matrix = df_binary_inverted.dot(df_binary_inverted.T)\n",
    "\n",
    "    # Calculate marginal sums for 'b' and 'c'\n",
    "    # Row-wise sum gives total number of samples where each gene is expressed\n",
    "    expressed_counts = df_binary.sum(axis=1)\n",
    "\n",
    "    # 'b': Gene A is expressed, Gene B is not\n",
    "    # This is calculated as (total samples where Gene A is expressed) - (samples where both are expressed)\n",
    "    b_matrix = expressed_counts.to_frame().values - a_matrix\n",
    "\n",
    "    # 'c': Gene B is expressed, Gene A is not\n",
    "    # This is calculated as (total samples where Gene B is expressed) - (samples where both are expressed)\n",
    "    # We use a transpose here to align the dimensions correctly\n",
    "    c_matrix = expressed_counts.to_frame().T.values - a_matrix\n",
    "\n",
    "    # Correcting for floating point inaccuracies by rounding to nearest integer\n",
    "    b_matrix = b_matrix.round().astype(int)\n",
    "    c_matrix = c_matrix.round().astype(int)\n",
    "\n",
    "    return {\n",
    "        'a': pd.DataFrame(a_matrix, index=df_counts.index, columns=df_counts.index),\n",
    "        'b': pd.DataFrame(b_matrix, index=df_counts.index, columns=df_counts.index),\n",
    "        'c': pd.DataFrame(c_matrix, index=df_counts.index, columns=df_counts.index),\n",
    "        'd': pd.DataFrame(d_matrix, index=df_counts.index, columns=df_counts.index)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a6225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def compute_single_chi2(args):\n",
    "    \"\"\"\n",
    "    Helper function to compute chi-squared for a single pair of genes.\n",
    "    This function is designed to be used with multiprocessing.Pool.\n",
    "    \"\"\"\n",
    "    contingency_tables, genea, geneb = args\n",
    "    genenames = contingency_tables['a'].columns\n",
    "    contingency_dict = {\n",
    "        'expressed': {'expressed': contingency_tables['a'].loc[genenames[genea], genenames[geneb]],\n",
    "                      'not_expressed': contingency_tables['b'].loc[genenames[genea], genenames[geneb]]},\n",
    "        'not_expressed': {'expressed': contingency_tables['c'].loc[genenames[genea], genenames[geneb]],\n",
    "                          'not_expressed': contingency_tables['d'].loc[genenames[genea], genenames[geneb]]}\n",
    "    }\n",
    "    contingency_df2 = pd.DataFrame(contingency_dict)\n",
    "    try:\n",
    "        cont = chi2_contingency(contingency_df2)\n",
    "        return genea, geneb, cont.pvalue, cont.statistic\n",
    "    except ValueError:\n",
    "        return genea, geneb, np.nan, np.nan\n",
    "\n",
    "def compute_stats_mat_parallel(contingency_tables):\n",
    "    \"\"\"\n",
    "    Computes the chi-squared p-values and test statistics in parallel\n",
    "    using the multiprocessing library.\n",
    "    \"\"\"\n",
    "    loop_end = contingency_tables['a'].shape[1]\n",
    "    pvalues = np.zeros((loop_end, loop_end))\n",
    "    teststat = np.zeros((loop_end, loop_end))\n",
    "    \n",
    "    tasks = [(contingency_tables, genea, geneb) for genea in range(loop_end) for geneb in range(genea + 1, loop_end)]\n",
    "    \n",
    "    with Pool() as pool:\n",
    "        results = pool.map(compute_single_chi2, tasks)\n",
    "    \n",
    "    for genea, geneb, pvalue, statistic in results:\n",
    "        pvalues[genea, geneb] = pvalue\n",
    "        teststat[genea, geneb] = statistic\n",
    "        \n",
    "    pvalues = pvalues + pvalues.T\n",
    "    teststat = teststat + teststat.T\n",
    "\n",
    "\n",
    "    np.fill_diagonal(pvalues, 1)\n",
    "    np.fill_diagonal(teststat, 0)\n",
    "\n",
    "\n",
    "    return pvalues, teststat, gene_names\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b0b1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def binarize_and_compute_contingency(df_counts):\n",
    "    \"\"\"\n",
    "    Binarizes raw gene count data with a conditional logic and computes\n",
    "    pairwise contingency tables efficiently.\n",
    "\n",
    "    Args:\n",
    "        df_counts (pd.DataFrame): A DataFrame with genes as rows and samples as columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with binarized gene expression data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine the number of samples (columns)\n",
    "    num_samples = df_counts.shape[1]\n",
    "\n",
    "    # Step 1: Binarize the gene expression data based on a condition\n",
    "    df_binary = pd.DataFrame(index=df_counts.index, columns=df_counts.columns, dtype=int)\n",
    "    for gene in df_counts.index:\n",
    "        gene_data = df_counts.loc[gene]\n",
    "        # Count the number of zeros\n",
    "        zero_count = (gene_data == 0).sum()\n",
    "\n",
    "        if zero_count / num_samples > 0.10:\n",
    "            # Case 1: More than 10% zeros, binarize as count > 0\n",
    "            df_binary.loc[gene] = (gene_data > 0).astype(int)\n",
    "        else:\n",
    "            # Case 2: 10% or fewer zeros, binarize using the median\n",
    "            # The labels=False argument ensures pd.qcut returns integer bin codes (0, 1, 2, 3)\n",
    "            # The duplicates='drop' handles cases where bin edges might be the same.\n",
    "            df_binary.loc[gene] = pd.qcut(gene_data, q=4, labels=False, duplicates='drop')\n",
    "\n",
    "    return df_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915858da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "\n",
    "def compute_single_chi2(args):\n",
    "    \"\"\"\n",
    "    Helper function to compute chi-squared for a single pair of genes\n",
    "    by directly computing a crosstab on the binarized data.\n",
    "    \"\"\"\n",
    "    contingency_table, genea_index, geneb_index = args\n",
    "\n",
    "    # Chi-squared test and return p-value and statistic\n",
    "    try:\n",
    "        cont = chi2_contingency(contingency_table)\n",
    "        return genea_index, geneb_index, cont.pvalue, cont.statistic\n",
    "    except ValueError:\n",
    "        return genea_index, geneb_index, np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def compute_single_chi2(args):\n",
    "    \"\"\"\n",
    "    Helper function to compute chi-squared for a single pair of genes\n",
    "    from a pre-computed contingency table.\n",
    "    \"\"\"\n",
    "    contingency_table, genea_index, geneb_index = args\n",
    "    \n",
    "    # Chi-squared test and return p-value and statistic\n",
    "    try:\n",
    "        cont = chi2_contingency(contingency_table)\n",
    "        return genea_index, geneb_index, cont.pvalue, cont.statistic\n",
    "    except ValueError:\n",
    "        return genea_index, geneb_index, np.nan, np.nan\n",
    "\n",
    "def compute_stats_mat_parallel(df_binary):\n",
    "    \"\"\"\n",
    "    Computes the chi-squared p-values and test statistics in parallel\n",
    "    by first computing all crosstabs before parallelization.\n",
    "    \"\"\"\n",
    "    loop_end = df_binary.shape[0]\n",
    "    gene_names = df_binary.index\n",
    "    \n",
    "    pvalues = np.zeros((loop_end, loop_end))\n",
    "    teststat = np.zeros((loop_end, loop_end))\n",
    "\n",
    "    print(\"compute oontinencies\")\n",
    "    tasks = []\n",
    "    for genea in range(loop_end):\n",
    "        for geneb in range(genea + 1, loop_end):\n",
    "            gene_a_data = df_binary.iloc[genea]\n",
    "            gene_b_data = df_binary.iloc[geneb]\n",
    "            \n",
    "            # Create the contingency table using pandas.crosstab\n",
    "            contingency_table = pd.crosstab(gene_a_data, gene_b_data)\n",
    "            \n",
    "            # Add the pre-computed table and indices to the task list\n",
    "            tasks.append((contingency_table, genea, geneb))\n",
    "\n",
    "    print('Compute tests')\n",
    "    with Pool() as pool:\n",
    "        results = pool.map(compute_single_chi2, tasks)\n",
    "    \n",
    "    # Step 3: Populate the result matrices\n",
    "    for genea, geneb, pvalue, statistic in results:\n",
    "        pvalues[genea, geneb] = pvalue\n",
    "        teststat[genea, geneb] = statistic\n",
    "        \n",
    "    # Symmetrize the matrices\n",
    "    pvalues = pvalues + pvalues.T\n",
    "    teststat = teststat + teststat.T\n",
    "\n",
    "    np.fill_diagonal(pvalues, 1)\n",
    "    np.fill_diagonal(teststat, 0)\n",
    "\n",
    "    return pvalues, teststat, gene_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a0b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat(pvalues, teststat):\n",
    "\n",
    "    pvalues = pd.DataFrame(pvalues)\n",
    "    teststat = pd.DataFrame(teststat)\n",
    "    pvalues.columns = gene_names\n",
    "    teststat.columns = gene_names\n",
    "    pvalues.index = gene_names\n",
    "    teststat.index = gene_names\n",
    "    pvalues = pvalues.melt(ignore_index=False).reset_index()\n",
    "    pvalues.columns = ['source', 'target', 'pvalue']\n",
    "    teststat = teststat.melt(ignore_index=False).reset_index()\n",
    "    teststat.columns = ['source', 'target', 'teststat']\n",
    "    return pvalues, teststat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9bfe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def create_pairwise_binary_mask(matrix_cells_x_genes, gene_list):\n",
    "    \"\"\"\n",
    "    Creates a dictionary of binary masks for each cell and pair of genes,\n",
    "    including both forward, reverse, and self-pairs (which are all zeros).\n",
    "\n",
    "    Args:\n",
    "        matrix_cells_x_genes (np.ndarray): A 2D numpy array where rows are cells\n",
    "                                          and columns are genes.\n",
    "        gene_list (list): A list of strings containing the names of the genes,\n",
    "                          in the same order as the columns in the matrix.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are gene pair strings (e.g., 'GeneA_GeneB')\n",
    "              and values are 1D numpy arrays representing the binary mask for that pair\n",
    "              across all cells.\n",
    "    \"\"\"\n",
    "    binary_matrix = (matrix_cells_x_genes > 0).astype(int)\n",
    "    num_cells, num_genes = binary_matrix.shape\n",
    "\n",
    "    if len(gene_list) != num_genes:\n",
    "        raise ValueError(\"The length of the gene_list must match the number of genes (columns) in the matrix.\")\n",
    "\n",
    "    pairwise_mask_dict = {}\n",
    "    zero_vector = np.zeros(num_cells, dtype=int)\n",
    "    for g_idx, gene_name in enumerate(gene_list):\n",
    "        key = f\"{gene_name}_{gene_name}\"\n",
    "        pairwise_mask_dict[key] = zero_vector\n",
    "\n",
    "    gene_pairs_indices = list(itertools.combinations(range(num_genes), 2))\n",
    "    for g1_idx, g2_idx in gene_pairs_indices:\n",
    "        mask = binary_matrix[:, g1_idx] * binary_matrix[:, g2_idx]\n",
    "        key_fwd = f\"{gene_list[g1_idx]}_{gene_list[g2_idx]}\"\n",
    "        pairwise_mask_dict[key_fwd] = mask\n",
    "        key_rev = f\"{gene_list[g2_idx]}_{gene_list[g1_idx]}\"\n",
    "        pairwise_mask_dict[key_rev] = mask\n",
    "\n",
    "    return pairwise_mask_dict\n",
    "\n",
    "def dict_to_dataframe(mask_dict, column_order_list):\n",
    "    \"\"\"\n",
    "    Converts a dictionary of binary masks into a pandas DataFrame,\n",
    "    respecting a specified column order.\n",
    "\n",
    "    Args:\n",
    "        mask_dict (dict): A dictionary where keys are gene pair strings and\n",
    "                          values are 1D numpy arrays (the masks).\n",
    "        column_order_list (list): A list of gene pair strings specifying the\n",
    "                                  desired order of the DataFrame columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with masks as columns, in the specified order.\n",
    "    \"\"\"\n",
    "    # 1. Create a dictionary with only the ordered columns\n",
    "    ordered_data = {col: mask_dict[col] for col in column_order_list if col in mask_dict}\n",
    "    \n",
    "    # 2. Check if all specified columns were found\n",
    "    if len(ordered_data) != len(column_order_list):\n",
    "        missing_columns = set(column_order_list) - set(ordered_data.keys())\n",
    "        print(f\"Warning: The following columns were not found in the mask dictionary: {missing_columns}\")\n",
    "\n",
    "    # 3. Create the DataFrame from the ordered dictionary\n",
    "    df = pd.DataFrame(ordered_data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Example UsageCreate the binary mask with both forward and reverse pairs and named strings\n",
    "mask_full = create_pairwise_binary_mask(counts.values, counts.columns)\n",
    "mask_full = dict_to_dataframe(mask_full, grn_adata.var_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f7e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_full.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00dbfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net1[net1.edge.isin(mask_full.iloc[:, np.where(mask_full.sum()<100)[0]].columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac29b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_full.iloc[:, np.where(mask_full.sum()<100)[0]].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c400fbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grn_adata.obs['grn'] = pd.Categorical(adata.obs['grn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c812e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.raw_attribution = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a813cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "nets[0].merge(grn_adata.var, left_on=['source', 'target'], right_on=['source', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab511b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pariwise_consistency_1 = find_consistent_pairs(grn_adata[grn_adata.obs['grn'] == 1.0, :].copy(), adata.var_names)\n",
    "pariwise_consistency_2 = find_consistent_pairs(grn_adata[grn_adata.obs['grn'] == 2.0, :].copy(), adata.var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4de348",
   "metadata": {},
   "outputs": [],
   "source": [
    "consistencies2 = pd.DataFrame(pariwise_consistency_2.items())\n",
    "consistencies2.columns = ['key', 'value']\n",
    "\n",
    "consistencies1 = pd.DataFrame(pariwise_consistency_1.items())\n",
    "consistencies1.columns = ['key', 'value']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e9801",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "#consistent_edges= np.unique(list(consistencies1[consistencies1.value>threshold].key)+ list((consistencies1[consistencies2.value>threshold].key)))\n",
    "consistent_edges= np.unique(list(consistencies1[np.abs(consistencies1.value)>threshold].key)+ list((consistencies1[np.abs(consistencies2.value)>threshold].key)))\n",
    "\n",
    "print(len(consistent_edges))\n",
    "\n",
    "grn_sub = grn_adata[:, mask_full.sum(axis = 0)>0]\n",
    "\n",
    "grn_sub.var_names.isin(consistent_edges)\n",
    "grn_sub = grn_sub[:, grn_sub.var_names.isin(consistent_edges)]\n",
    "print(grn_sub)\n",
    "\n",
    "nets[0].merge(grn_sub.var, left_on=['source', 'target'], right_on=['source', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5296833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.compute_metrics import  get_top_edges_per_cell, get_top_edges_per_cell_per_cluster, downstream_recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ecbc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_full\n",
    "\n",
    "grn_adata.layers['mask'] = mask_full.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95713d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero1 = grn_adata.layers['mask'][adata.obs['grn']==1.0].sum(axis=0)\n",
    "non_zero2 = grn_adata.layers['mask'][adata.obs['grn']==2.0].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda8272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "consistencies1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73eb884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c057f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nets[0].shape)\n",
    "print(nets[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ee137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3039c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grn_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed19ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "grn_adata.obs['grn'] = pd.Categorical(adata.obs['grn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0079b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grn_adata[grn_adata.obs['T_ACADVL'] == '1', ['CCNA2_BBX', 'BBX_CCNA2', 'KAT7_CCNA2', 'CCNA2_KAT7', 'TBP_CCNA2', 'CCNA2_TBP', 'BCAS3_CCNA2', 'CCNA2_BCAS3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54536e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(st.spearmanr(grn_adata[:, ['CCNA2_BBX', 'BBX_CCNA2', 'KAT7_CCNA2', 'CCNA2_KAT7', 'TBP_CCNA2', 'CCNA2_TBP', 'BCAS3_CCNA2', 'CCNA2_BCAS3',  'BATF2_CCNA2', 'CCNA2_BATF2','TAF1_CCNA2', 'CCNA2_TAF1','POLI_CCNA2', 'CCNA2_POLI' ]].X).statistic)\n",
    "df.columns = ['CCNA2_BBX', 'BBX_CCNA2', 'KAT7_CCNA2', 'CCNA2_KAT7', 'TBP_CCNA2', 'CCNA2_TBP', 'BCAS3_CCNA2', 'CCNA2_BCAS3', 'BATF2_CCNA2', 'CCNA2_BATF2','TAF1_CCNA2', 'CCNA2_TAF1', 'POLI_CCNA2', 'CCNA2_POLI' ]\n",
    "df.index = ['CCNA2_BBX', 'BBX_CCNA2', 'KAT7_CCNA2', 'CCNA2_KAT7', 'TBP_CCNA2', 'CCNA2_TBP', 'BCAS3_CCNA2', 'CCNA2_BCAS3', 'BATF2_CCNA2', 'CCNA2_BATF2','TAF1_CCNA2', 'CCNA2_TAF1', 'POLI_CCNA2', 'CCNA2_POLI' ]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b27d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(st.spearmanr(grn_adata[grn_adata.obs['grn'] == 1.0, ['CCNA2_BBX', 'BBX_CCNA2', 'KAT7_CCNA2', 'CCNA2_KAT7', 'TBP_CCNA2', 'CCNA2_TBP', 'BCAS3_CCNA2', 'CCNA2_BCAS3',  'BATF2_CCNA2', 'CCNA2_BATF2','TAF1_CCNA2', 'CCNA2_TAF1','POLI_CCNA2', 'CCNA2_POLI' ]].X).statistic)\n",
    "df.columns = ['CCNA2_BBX', 'BBX_CCNA2', 'KAT7_CCNA2', 'CCNA2_KAT7', 'TBP_CCNA2', 'CCNA2_TBP', 'BCAS3_CCNA2', 'CCNA2_BCAS3', 'BATF2_CCNA2', 'CCNA2_BATF2','TAF1_CCNA2', 'CCNA2_TAF1', 'POLI_CCNA2', 'CCNA2_POLI' ]\n",
    "df.index = ['CCNA2_BBX', 'BBX_CCNA2', 'KAT7_CCNA2', 'CCNA2_KAT7', 'TBP_CCNA2', 'CCNA2_TBP', 'BCAS3_CCNA2', 'CCNA2_BCAS3', 'BATF2_CCNA2', 'CCNA2_BATF2','TAF1_CCNA2', 'CCNA2_TAF1', 'POLI_CCNA2', 'CCNA2_POLI' ]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b00e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(st.spearmanr(grn_adata[grn_adata.obs['grn'] == 2.0, ['CCNA2_BBX', 'BBX_CCNA2', 'KAT7_CCNA2', 'CCNA2_KAT7', 'TBP_CCNA2', 'CCNA2_TBP', 'BCAS3_CCNA2', 'CCNA2_BCAS3', 'BATF2_CCNA2', 'CCNA2_BATF2','TAF1_CCNA2', 'CCNA2_TAF1' , 'POLI_CCNA2', 'CCNA2_POLI' ]].X).statistic)\n",
    "df.columns = ['CCNA2_BBX', 'BBX_CCNA2', 'KAT7_CCNA2', 'CCNA2_KAT7', 'TBP_CCNA2', 'CCNA2_TBP', 'BCAS3_CCNA2', 'CCNA2_BCAS3', 'BATF2_CCNA2', 'CCNA2_BATF2','TAF1_CCNA2', 'CCNA2_TAF1', 'POLI_CCNA2', 'CCNA2_POLI' ]\n",
    "df.index = ['CCNA2_BBX', 'BBX_CCNA2', 'KAT7_CCNA2', 'CCNA2_KAT7', 'TBP_CCNA2', 'CCNA2_TBP', 'BCAS3_CCNA2', 'CCNA2_BCAS3', 'BATF2_CCNA2', 'CCNA2_BATF2','TAF1_CCNA2', 'CCNA2_TAF1', 'POLI_CCNA2', 'CCNA2_POLI']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e82d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68988209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.compute_metrics import  get_top_edges_per_cell, get_top_edges_per_cell_per_cluster, downstream_recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81697a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grn_adata.obs['grn'] = pd.Categorical(adata.obs['grn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b266567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_genes = grn_adata.var.index.intersection(pairs_full)\n",
    "intersection_indices = [pairs_full.index(gene) for gene in intersection_genes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06af2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(pairs_full)[intersection_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d11c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.multiply(grn_adata.X[:, grn_adata.var.index.isin(grn_adata.var.index.intersection(pairs_full))], mask_full[:, intersection_indices])\n",
    "aa = ad.AnnData(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12406974",
   "metadata": {},
   "outputs": [],
   "source": [
    "grn_adata = downstream_recipe(grn_adata)\n",
    "grn_adata.var.columns = ['source', 'target', 'n_cells']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a3ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "top  = top.loc[:,['source', 'target']].groupby(['source', 'target']).value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7579e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "top.sort_values('count', ascending=False)[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fccb98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773d1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "top.sort_values('count', ascending=False)\n",
    "top.merge(nets[0], left_on = ['source', 'target'], right_on=['source', 'target']).sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cdfec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "top.sort_values('count', ascending=False)\n",
    "top.merge(nets[0], left_on = ['source', 'target'], right_on=['source', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e06e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "['APP', 'ETS2', 'KAT2A', 'KRT36', 'POLI', 'SLC3A1', 'TRPV4']\n",
    "top.sort_values('count', ascending=False)[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f78792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top.merge(common[0], left_on = ['source', 'target'], right_on=['source', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab97fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top.merge(common[0], left_on = ['source', 'target'], right_on=['source', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f67fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.grn = pd.Categorical(adata.obs.grn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffef7369",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(adata, keys = ['POT1', 'LRMDA'], groupby='grn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cf077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top.merge(nets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d99e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.obs['grn'] = pd.Categorical(adata.obs['grn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9562dbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "grn_adata2 = aa[:, aa.var_names.isin(top['edgenames'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeea8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grn_adata2  = downstream_recipe(grn_adata2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ba80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(grn_adata2, color = [ 'grn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d9fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mynets = [(1.0, nets[0]), (2.0, nets[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20678427",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_edges_per_cell_per_cluster(aa,  mynets, cluster_var = 'grn', top_edges= 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80cb071",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%8f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b77832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "suba = grn_adata[:, grn_adata.var.source=='MTA1']\n",
    "suba = grn_adata[:, grn_adata.var.source=='GATA3']\n",
    "\n",
    "corr_matrix = np.corrcoef(suba.X.T)\n",
    "\n",
    "corr_dist = 1 - corr_matrix\n",
    "dist_linkage = hierarchy.average(corr_dist)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(25, 15))\n",
    "\n",
    "# We convert the correlation matrix to a distance matrix before performing\n",
    "# hierarchical clustering using Ward's linkage.\n",
    "\n",
    "dendro = hierarchy.dendrogram(\n",
    "    dist_linkage, labels=suba.var.target.to_list(), ax=ax1, leaf_rotation=90\n",
    ")\n",
    "dendro_idx = np.arange(0, len(dendro[\"ivl\"]))\n",
    "\n",
    "ax2.imshow(corr_matrix[dendro[\"leaves\"], :][:, dendro[\"leaves\"]])\n",
    "ax2.set_xticks(dendro_idx)\n",
    "ax2.set_yticks(dendro_idx)\n",
    "ax2.set_xticklabels(dendro[\"ivl\"], rotation=\"vertical\")\n",
    "ax2.set_yticklabels(dendro[\"ivl\"])\n",
    "_ = fig.tight_layout()\n",
    "    \n",
    "clusters = hierarchy.fcluster(dl, t=2, criterion='distance')\n",
    "cluster_to_genes = {}\n",
    "for gene_name, cluster_id in zip(gene_names1, clusters):\n",
    "    if cluster_id not in cluster_to_genes:\n",
    "        cluster_to_genes[cluster_id] = []\n",
    "    cluster_to_genes[cluster_id].append(gene_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3fcc7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c4cf98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixi_netmap",
   "language": "python",
   "name": "pixi_netmap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
