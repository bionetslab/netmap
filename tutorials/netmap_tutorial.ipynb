{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45190821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data_nfs/og86asub/netmap/netmap-evaluation/')\n",
    "\n",
    "import scanpy as sc\n",
    "import time \n",
    "\n",
    "from netmap.src.utils.misc import write_config\n",
    "\n",
    "from netmap.src.model.negbinautoencoder import *\n",
    "import scanpy as sc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from captum.attr import GradientShap, LRP\n",
    "from netmap.src.model.inferrence_simple import *\n",
    "from netmap.src.utils.data_utils import attribution_to_anndata\n",
    "from netmap.src.model.pipeline import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "import os.path as op\n",
    "import os\n",
    "\n",
    "import anndata as ad\n",
    "from statsmodels.stats.nonparametric import rank_compare_2indep\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as scs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from captum.attr import *\n",
    "import pingouin as pingu\n",
    "\n",
    "def create_model_zoo(data_tensor, n_models = 4, n_epochs = 500):\n",
    "    model_zoo = []\n",
    "    for _ in range(n_models):\n",
    "\n",
    "        data_train2, data_test2 = train_test_split(data_tensor,test_size=0.01, shuffle=True)\n",
    "\n",
    "        trained_model2 = NegativeBinomialAutoencoder(input_dim=data_tensor.shape[1], latent_dim=10, dropout_rate = 0.02)\n",
    "        trained_model2 = trained_model2.cuda()\n",
    "\n",
    "        optimizer2 = torch.optim.Adam(trained_model2.parameters(), lr=1e-4)\n",
    "\n",
    "        trained_model2 = train_autoencoder(\n",
    "                trained_model2,\n",
    "                data_train2.cuda(),\n",
    "                optimizer2,\n",
    "                num_epochs=n_epochs\n",
    "\n",
    "            )\n",
    "        model_zoo.append(trained_model2)\n",
    "    return model_zoo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_latent_true(model_zoo):\n",
    "    for mo in model_zoo:\n",
    "        mo.forward_mu_only = False\n",
    "        mo.forward_theta_only = False\n",
    "        mo.latent_only = True\n",
    "    return model_zoo\n",
    "\n",
    "\n",
    "def set_all_false(model_zoo):\n",
    "    for mo in model_zoo:\n",
    "        mo.forward_mu_only = False\n",
    "        mo.forward_theta_only = False\n",
    "        mo.latent_only = False\n",
    "    return model_zoo\n",
    "\n",
    "def shuffle_each_column_independently(tensor):\n",
    "    \"\"\"\n",
    "    Shuffles each column of a 2D PyTorch tensor independently.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): The input tensor.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A new tensor with each of its columns independently shuffled.\n",
    "    \"\"\"\n",
    "    if tensor.dim() != 2:\n",
    "        raise ValueError(\"Input tensor must be 2-dimensional to shuffle columns.\")\n",
    "\n",
    "    # Create an empty tensor of the same size to store the shuffled columns\n",
    "    shuffled_tensor = torch.empty_like(tensor)\n",
    "\n",
    "    # Iterate through each column, shuffle it, and place it in the new tensor\n",
    "    for i in range(tensor.size(1)):\n",
    "        column = tensor[:, i]\n",
    "        idx = torch.randperm(column.nelement())\n",
    "        shuffled_tensor[:, i] = column[idx]\n",
    "\n",
    "    return shuffled_tensor\n",
    "\n",
    "\n",
    "def attribution_one_target( \n",
    "        target_gene,\n",
    "        lrp_model,\n",
    "        input_data,\n",
    "        background,\n",
    "        xai_type='lrp-like',\n",
    "        randomize_background = False):\n",
    "    \n",
    "    attributions_list = []\n",
    "    for m in range(len(lrp_model)):\n",
    "        # Randomize backgorund for each round\n",
    "        if randomize_background:\n",
    "            background = shuffle_each_column_independently(background)\n",
    "\n",
    "        model = lrp_model[m]\n",
    "        #for _ in range(num_iterations):\n",
    "        if xai_type == 'lrp-like':\n",
    "            attribution = model.attribute(input_data, target=target_gene)\n",
    "                \n",
    "        elif xai_type == 'shap-like':\n",
    "            attribution = model.attribute(input_data, baselines = background, target = target_gene)\n",
    "\n",
    "        attributions_list.append(attribution.detach().cpu().numpy())\n",
    "    return attributions_list\n",
    "\n",
    "def get_differential_edges(attribution_anndata, percentile = 10):\n",
    "    genelist = []\n",
    "    if len(np.unique(attribution_anndata.obs['leiden']))>1 :\n",
    "        for cat in np.unique(attribution_anndata.obs['leiden']):\n",
    "            statisi =rank_compare_2indep(x1=attribution_anndata.X[attribution_anndata.obs['leiden']==cat], x2= attribution_anndata.X[attribution_anndata.obs['leiden']!=cat])\n",
    "            sig_and_high = np.where((statisi.pvalue<(0.01/(attribution_anndata.X.shape[1]*attribution_anndata.X.shape[1])))  & (statisi.prob1>= 0.9))\n",
    "            genelist = genelist+ list(sig_and_high[0])\n",
    "\n",
    "    else:\n",
    "        # FALLBACk\n",
    "        m = np.abs(attribution_anndata.X).mean(axis=0)\n",
    "        # Get the indices of genes in the top 10%\n",
    "        top_10_percent_indices = np.where(m > np.percentile(m, 100-percentile))[0]\n",
    "\n",
    "        # Get the indices of genes in the bottom 10%\n",
    "        bottom_10_percent_indices = np.where(m < np.percentile(m, percentile))[0]\n",
    "\n",
    "        # Combine the two arrays of indices and sort them\n",
    "        genelist = np.unique(np.sort(\n",
    "            np.concatenate((top_10_percent_indices, bottom_10_percent_indices))\n",
    "        ))\n",
    "    return genelist\n",
    "\n",
    "def get_percentile_edges(attribution_anndata, percentile = 10):\n",
    "    # FALLBACk\n",
    "    m = attribution_anndata.X.mean(axis=0)\n",
    "    # Get the indices of genes in the top 10%\n",
    "    top_10_percent_indices = np.where(m > np.percentile(m, 100-percentile))[0]\n",
    "\n",
    "    # Get the indices of genes in the bottom 10%\n",
    "    bottom_10_percent_indices = np.where(m < np.percentile(m, percentile))[0]\n",
    "\n",
    "    # Combine the two arrays of indices and sort them\n",
    "    genelist = np.sort(\n",
    "        np.concatenate((top_10_percent_indices, bottom_10_percent_indices))\n",
    "    )\n",
    "    return genelist\n",
    "\n",
    "def get_edges(attribution_anndata, use_differential=False, percentile = 10):\n",
    "    if use_differential:\n",
    "        return get_differential_edges(attribution_anndata, percentile=percentile)\n",
    "    else:\n",
    "        return get_percentile_edges(attribution_anndata, percentile=percentile)\n",
    "    \n",
    "def get_explainer(model, explainer_type, raw=False):\n",
    "    if explainer_type in ['GuidedBackprop', 'Deconvolution']:\n",
    "        explainer_mode = 'lrp-like'\n",
    "    else:\n",
    "        explainer_mode = 'shap-like'\n",
    "    \n",
    "        \n",
    "    if explainer_type == 'GuidedBackprop': #fast\n",
    "        explainer = GuidedBackprop(model)\n",
    "    elif explainer_type == 'GradientShap': #fast\n",
    "        if raw:\n",
    "            explainer = GradientShap(model, multiply_by_inputs=False)\n",
    "        else:\n",
    "            explainer = GradientShap(model, multiply_by_inputs=True)\n",
    "\n",
    "    elif explainer_type == 'Deconvolution': #fast\n",
    "        explainer = Deconvolution(model)\n",
    "    else:\n",
    "        raise ValueError('no such method')\n",
    "        \n",
    "    return explainer, explainer_mode\n",
    "\n",
    "def compute_correlation_metric(data, cor_type):\n",
    "    # Compute gene correlation measure\n",
    "    #  'pingouin.pcorr', 'np.cov', 'np.corcoeff'\n",
    "    if cor_type ==  'pingouin.pcorr':\n",
    "        cov = pingu.pcorr(pd.DataFrame(data))\n",
    "    elif cor_type == 'np.cov':\n",
    "        cov = np.cov(data.T)\n",
    "    elif cor_type == 'np.corrcoeff':\n",
    "        cov = np.corrcoef(data.T)\n",
    "    elif cor_type == 'None':\n",
    "        cov = 1\n",
    "    else: \n",
    "        cov = 1\n",
    "    return cov\n",
    "\n",
    "def aggregate_attributions(attributions, strategy = 'mean'):\n",
    "    if strategy == 'mean':\n",
    "        return np.mean(attributions, axis = 0)\n",
    "    elif strategy == 'sum':\n",
    "        return np.sum(attributions, axis = 0)\n",
    "    elif strategy == 'median':\n",
    "        return np.median(attributions, axis = 0)\n",
    "    else:\n",
    "        # Default to mean aggregation\n",
    "        return np.mean(attributions, axis = 0)\n",
    "    \n",
    "def wrapper(models, data_train_full_tensor, gene_names, config):\n",
    "\n",
    "    data = data_train_full_tensor.detach().cpu().numpy()\n",
    "    tms = []\n",
    "    name_list = []\n",
    "    target_names = []\n",
    "    clusterings = {}\n",
    "    for trained_model in models:        \n",
    "        trained_model.forward_mu_only = True\n",
    "        explainer, xai_type = get_explainer(trained_model, config.xai_method, config.raw_attribution)\n",
    "        tms.append(explainer)\n",
    "\n",
    "    attributions = []\n",
    "    ## ATTRIBUTIONS\n",
    "    for g in tqdm(range(data_train_full_tensor.shape[1])):\n",
    "    #for g in range(2):\n",
    "\n",
    "        attributions_list = attribution_one_target(\n",
    "            g,\n",
    "            tms,\n",
    "            data_train_full_tensor,\n",
    "            data_train_full_tensor,\n",
    "            xai_type=xai_type,\n",
    "            randomize_background = True)\n",
    "        print(attributions_list)\n",
    "        attributions.append(attributions_list)\n",
    "\n",
    "    \n",
    "\n",
    "    ## AGGREGATION: REPLACE LIST BY AGGREGATED DATA\n",
    "    for i in range(len(attributions)):\n",
    "        # CURRENTLY MEAN\n",
    "        attributions[i] = aggregate_attributions(attributions[i], strategy=config.aggregation_strategy )\n",
    "    \n",
    "    ## PENALIZE:\n",
    "    if config.penalty != 'None':\n",
    "        penalty_matrix = compute_correlation_metric(data, cor_type=config.penalty)\n",
    "        for i in range(len(attributions)):\n",
    "            # CURRENTLY MEAN\n",
    "            attributions[i] = np.dot(attributions[i], (1-penalty_matrix))\n",
    "\n",
    "\n",
    "    ## CLUSTERING: CLUSTER EACH TARGET INDVIDUALLY\n",
    "    for i in range(len(attributions)):\n",
    " \n",
    "        attributions[i] = ad.AnnData(attributions[i])\n",
    "        sc.pp.scale(attributions[i])\n",
    "        try:\n",
    "            sc.pp.pca(attributions[i],n_comps=50)\n",
    "        except:\n",
    "            try:\n",
    "                sc.pp.pca(attributions[i],n_comps=50 )\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "        sc.pp.neighbors(attributions[i], n_neighbors=15)\n",
    "        sc.tl.leiden(attributions[i], resolution=0.1)\n",
    "\n",
    "        clusterings[f'T_{gene_names[i]}'] = np.array(attributions[i].obs['leiden'])\n",
    "\n",
    "    \n",
    "    #EDGE SELECTION:\n",
    "    for i in range(len(attributions)):\n",
    "        edge_indices = get_edges(attributions[i], use_differential=config.use_differential, percentile=config.percentile)\n",
    "        name_list = name_list + list(gene_names[edge_indices])\n",
    "        target_names = target_names+[gene_names[i]]* len(edge_indices)\n",
    "        attributions[i] = attributions[i][:,edge_indices].X\n",
    "\n",
    "    attributions = np.hstack(attributions)\n",
    "    \n",
    "    index_list = [f\"{s}_{t}\" for (s, t) in zip(name_list, target_names)]\n",
    "    cou = pd.DataFrame({'index': index_list, 'source':name_list, 'target':target_names})\n",
    "    cou = cou.set_index('index')\n",
    "\n",
    "    clusterings = pd.DataFrame(clusterings)\n",
    "\n",
    "    grn_adata = attribution_to_anndata(attributions, var=cou, obs = clusterings)\n",
    "\n",
    "    return grn_adata\n",
    "\n",
    "def run_netmap(config, dataset_config):\n",
    "\n",
    "    print('Version 2')\n",
    "    start_total = time.monotonic()\n",
    "    \n",
    "    ## Load config and setup outputs\n",
    "    os.makedirs(config.output_directory, exist_ok=True)\n",
    "    sc.settings.figdir = config.output_directory\n",
    "    config.write_yaml(yaml_file=op.join(config.output_directory, 'config.yaml'))\n",
    "\n",
    "    ## load data\n",
    "    adata = sc.read_h5ad(config.input_data)\n",
    "    \n",
    "\n",
    "    ## Get the data matrix from the CustumAnndata obeject\n",
    "\n",
    "    gene_names = np.array(adata.var.index)\n",
    "    model_start = time.monotonic()\n",
    "\n",
    "    if config.layer == 'counts':\n",
    "        data_tensor = adata.layers['counts']\n",
    "    else:\n",
    "        data_tensor = adata.X\n",
    "\n",
    "    if scs.issparse(data_tensor):\n",
    "        data_tensor = torch.tensor(data_tensor.todense(), dtype=torch.float32)\n",
    "    else:\n",
    "        data_tensor = torch.tensor(data_tensor, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    print(data_tensor.shape)\n",
    "\n",
    "    model_zoo = create_model_zoo(data_tensor, n_models=config.n_models, n_epochs=500)\n",
    "    grn_adata = wrapper(model_zoo, data_tensor.cuda(), gene_names, config)\n",
    "\n",
    "    adob = adata.obs.reset_index()\n",
    "    grn_adata.obs['cell_id'] = np.array(adob['cell_id'])\n",
    "    grn_adata.obs['grn'] = np.array(adob['grn'])\n",
    "\n",
    "    \n",
    "    model_elapsed = time.monotonic()-model_start\n",
    "    grn_adata.write_h5ad(op.join(config.output_directory,config.adata_filename))\n",
    "\n",
    "    time_elapsed_total = time.monotonic()-start_total\n",
    "\n",
    "\n",
    "    res = {'time_elapsed_total': time_elapsed_total, 'time_elapsed_netmap': model_elapsed} \n",
    "    write_config(res, file=op.join(config.output_directory, 'results.yaml'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9de3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_data': '/data_nfs/og86asub/netmap/netmap-evaluation/data/simulated_data/config_easy/net_105_14145_net_76_12762_net_51_13833/data.h5ad', 'layer': 'X', 'output_directory': '/data_nfs/og86asub/netmap/netmap-evaluation/results/netmap/config/config_easy/net_105_14145_net_76_12762_net_51_13833', 'transcription_factors': '/data_nfs/datasets/SCENIC_DB/tf_lists/allTFs_hg38.txt', 'tf_only': False, 'penalize_error': True, 'adata_filename': 'grn_lrp.h5ad', 'grn': 'grn_lrp.tsv', 'masking_percentage': 0.1, 'masking_value': 0, 'print_every': 100, 'optimizer': 'Adam', 'learning_rate': 0.005, 'epochs': 150, 'overwrite': True, 'n_models': 10, 'n_top_edges': 100, 'test_size': 0.3, 'edge_count': 10000, 'model': 'standard', 'loss_fn': 'MSELoss', 'xai_method': 'GradientShap', 'top_perc': True}\n",
      "torch.Size([1000, 232])\n",
      "Epoch 1/500, Loss: 48.2598\n",
      "Epoch 11/500, Loss: 5.4054\n",
      "Epoch 21/500, Loss: 4.3740\n",
      "Epoch 31/500, Loss: 4.1581\n",
      "Epoch 41/500, Loss: 4.0460\n",
      "Epoch 51/500, Loss: 3.9722\n",
      "Epoch 61/500, Loss: 3.9075\n",
      "Epoch 71/500, Loss: 3.8562\n",
      "Epoch 81/500, Loss: 3.7936\n",
      "Epoch 91/500, Loss: 3.7480\n",
      "Epoch 101/500, Loss: 3.7184\n",
      "Epoch 111/500, Loss: 3.6901\n",
      "Epoch 121/500, Loss: 3.6725\n",
      "Epoch 131/500, Loss: 3.6590\n",
      "Epoch 141/500, Loss: 3.6437\n",
      "Epoch 151/500, Loss: 3.6292\n",
      "Epoch 161/500, Loss: 3.6204\n",
      "Epoch 171/500, Loss: 3.6127\n",
      "Epoch 181/500, Loss: 3.6080\n",
      "Epoch 191/500, Loss: 3.5956\n",
      "Epoch 201/500, Loss: 3.5878\n",
      "Epoch 211/500, Loss: 3.5799\n",
      "Epoch 221/500, Loss: 3.5719\n",
      "Epoch 231/500, Loss: 3.5669\n",
      "Epoch 241/500, Loss: 3.5600\n",
      "Epoch 251/500, Loss: 3.5534\n",
      "Epoch 261/500, Loss: 3.5477\n",
      "Epoch 271/500, Loss: 3.5436\n",
      "Epoch 281/500, Loss: 3.5368\n",
      "Epoch 291/500, Loss: 3.5337\n",
      "Epoch 301/500, Loss: 3.5289\n",
      "Epoch 311/500, Loss: 3.5238\n",
      "Epoch 321/500, Loss: 3.5212\n",
      "Epoch 331/500, Loss: 3.5178\n",
      "Epoch 341/500, Loss: 3.5123\n",
      "Epoch 351/500, Loss: 3.5107\n",
      "Epoch 361/500, Loss: 3.5059\n",
      "Epoch 371/500, Loss: 3.5017\n",
      "Epoch 381/500, Loss: 3.4975\n",
      "Epoch 391/500, Loss: 3.4978\n",
      "Epoch 401/500, Loss: 3.4938\n",
      "Epoch 411/500, Loss: 3.4901\n",
      "Epoch 421/500, Loss: 3.4886\n",
      "Epoch 431/500, Loss: 3.4843\n",
      "Epoch 441/500, Loss: 3.4811\n",
      "Epoch 451/500, Loss: 3.4812\n",
      "Epoch 461/500, Loss: 3.4767\n",
      "Epoch 471/500, Loss: 3.4749\n",
      "Epoch 481/500, Loss: 3.4725\n",
      "Epoch 491/500, Loss: 3.4730\n",
      "Epoch 1/500, Loss: 41.6543\n",
      "Epoch 11/500, Loss: 5.0822\n",
      "Epoch 21/500, Loss: 4.3100\n",
      "Epoch 31/500, Loss: 4.1270\n",
      "Epoch 41/500, Loss: 4.0202\n",
      "Epoch 51/500, Loss: 3.9395\n",
      "Epoch 61/500, Loss: 3.8653\n",
      "Epoch 71/500, Loss: 3.8087\n",
      "Epoch 81/500, Loss: 3.7623\n",
      "Epoch 91/500, Loss: 3.7310\n",
      "Epoch 101/500, Loss: 3.6952\n",
      "Epoch 111/500, Loss: 3.6725\n",
      "Epoch 121/500, Loss: 3.6607\n",
      "Epoch 131/500, Loss: 3.6437\n",
      "Epoch 141/500, Loss: 3.6299\n",
      "Epoch 151/500, Loss: 3.6172\n",
      "Epoch 161/500, Loss: 3.6129\n",
      "Epoch 171/500, Loss: 3.6005\n",
      "Epoch 181/500, Loss: 3.5903\n",
      "Epoch 191/500, Loss: 3.5813\n",
      "Epoch 201/500, Loss: 3.5739\n",
      "Epoch 211/500, Loss: 3.5679\n",
      "Epoch 221/500, Loss: 3.5578\n",
      "Epoch 231/500, Loss: 3.5505\n",
      "Epoch 241/500, Loss: 3.5478\n",
      "Epoch 251/500, Loss: 3.5409\n",
      "Epoch 261/500, Loss: 3.5350\n",
      "Epoch 271/500, Loss: 3.5301\n",
      "Epoch 281/500, Loss: 3.5275\n",
      "Epoch 291/500, Loss: 3.5247\n",
      "Epoch 301/500, Loss: 3.5210\n",
      "Epoch 311/500, Loss: 3.5164\n",
      "Epoch 321/500, Loss: 3.5153\n",
      "Epoch 331/500, Loss: 3.5132\n",
      "Epoch 341/500, Loss: 3.5085\n",
      "Epoch 351/500, Loss: 3.5052\n",
      "Epoch 361/500, Loss: 3.5024\n",
      "Epoch 371/500, Loss: 3.5010\n",
      "Epoch 381/500, Loss: 3.4978\n",
      "Epoch 391/500, Loss: 3.4945\n",
      "Epoch 401/500, Loss: 3.4940\n",
      "Epoch 411/500, Loss: 3.4906\n",
      "Epoch 421/500, Loss: 3.4889\n",
      "Epoch 431/500, Loss: 3.4855\n",
      "Epoch 441/500, Loss: 3.4851\n",
      "Epoch 451/500, Loss: 3.4815\n",
      "Epoch 461/500, Loss: 3.4770\n",
      "Epoch 471/500, Loss: 3.4754\n",
      "Epoch 481/500, Loss: 3.4748\n",
      "Epoch 491/500, Loss: 3.4713\n",
      "Epoch 1/500, Loss: 37.3080\n",
      "Epoch 11/500, Loss: 5.1560\n",
      "Epoch 21/500, Loss: 4.3223\n",
      "Epoch 31/500, Loss: 4.1004\n",
      "Epoch 41/500, Loss: 4.0018\n",
      "Epoch 51/500, Loss: 3.9233\n",
      "Epoch 61/500, Loss: 3.8703\n",
      "Epoch 71/500, Loss: 3.8149\n",
      "Epoch 81/500, Loss: 3.7690\n",
      "Epoch 91/500, Loss: 3.7294\n",
      "Epoch 101/500, Loss: 3.7025\n",
      "Epoch 111/500, Loss: 3.6803\n",
      "Epoch 121/500, Loss: 3.6541\n",
      "Epoch 131/500, Loss: 3.6341\n",
      "Epoch 141/500, Loss: 3.6234\n",
      "Epoch 151/500, Loss: 3.6130\n",
      "Epoch 161/500, Loss: 3.6035\n",
      "Epoch 171/500, Loss: 3.5970\n",
      "Epoch 181/500, Loss: 3.5871\n",
      "Epoch 191/500, Loss: 3.5799\n",
      "Epoch 201/500, Loss: 3.5774\n",
      "Epoch 211/500, Loss: 3.5716\n",
      "Epoch 221/500, Loss: 3.5689\n",
      "Epoch 231/500, Loss: 3.5587\n",
      "Epoch 241/500, Loss: 3.5522\n",
      "Epoch 251/500, Loss: 3.5484\n",
      "Epoch 261/500, Loss: 3.5432\n",
      "Epoch 271/500, Loss: 3.5376\n",
      "Epoch 281/500, Loss: 3.5343\n",
      "Epoch 291/500, Loss: 3.5308\n",
      "Epoch 301/500, Loss: 3.5270\n",
      "Epoch 311/500, Loss: 3.5255\n",
      "Epoch 321/500, Loss: 3.5188\n",
      "Epoch 331/500, Loss: 3.5149\n",
      "Epoch 341/500, Loss: 3.5121\n",
      "Epoch 351/500, Loss: 3.5062\n",
      "Epoch 361/500, Loss: 3.5027\n",
      "Epoch 371/500, Loss: 3.5020\n",
      "Epoch 381/500, Loss: 3.4964\n",
      "Epoch 391/500, Loss: 3.4948\n",
      "Epoch 401/500, Loss: 3.4924\n",
      "Epoch 411/500, Loss: 3.4899\n",
      "Epoch 421/500, Loss: 3.4868\n",
      "Epoch 431/500, Loss: 3.4851\n",
      "Epoch 441/500, Loss: 3.4798\n",
      "Epoch 451/500, Loss: 3.4785\n",
      "Epoch 461/500, Loss: 3.4765\n",
      "Epoch 471/500, Loss: 3.4750\n",
      "Epoch 481/500, Loss: 3.4722\n",
      "Epoch 491/500, Loss: 3.4690\n",
      "Epoch 1/500, Loss: 40.2287\n",
      "Epoch 11/500, Loss: 5.7384\n",
      "Epoch 21/500, Loss: 4.8860\n",
      "Epoch 31/500, Loss: 4.3496\n",
      "Epoch 41/500, Loss: 4.1389\n",
      "Epoch 51/500, Loss: 4.0307\n",
      "Epoch 61/500, Loss: 3.9502\n",
      "Epoch 71/500, Loss: 3.8923\n",
      "Epoch 81/500, Loss: 3.8364\n",
      "Epoch 91/500, Loss: 3.7877\n",
      "Epoch 101/500, Loss: 3.7618\n",
      "Epoch 111/500, Loss: 3.7323\n",
      "Epoch 121/500, Loss: 3.7074\n",
      "Epoch 131/500, Loss: 3.6865\n",
      "Epoch 141/500, Loss: 3.6687\n",
      "Epoch 151/500, Loss: 3.6526\n",
      "Epoch 161/500, Loss: 3.6401\n",
      "Epoch 171/500, Loss: 3.6287\n",
      "Epoch 181/500, Loss: 3.6166\n",
      "Epoch 191/500, Loss: 3.6056\n",
      "Epoch 201/500, Loss: 3.6022\n",
      "Epoch 211/500, Loss: 3.5901\n",
      "Epoch 221/500, Loss: 3.5815\n",
      "Epoch 231/500, Loss: 3.5720\n",
      "Epoch 241/500, Loss: 3.5652\n",
      "Epoch 251/500, Loss: 3.5594\n",
      "Epoch 261/500, Loss: 3.5516\n",
      "Epoch 271/500, Loss: 3.5474\n",
      "Epoch 281/500, Loss: 3.5397\n",
      "Epoch 291/500, Loss: 3.5338\n",
      "Epoch 301/500, Loss: 3.5317\n",
      "Epoch 311/500, Loss: 3.5275\n",
      "Epoch 321/500, Loss: 3.5212\n",
      "Epoch 331/500, Loss: 3.5179\n",
      "Epoch 341/500, Loss: 3.5130\n",
      "Epoch 351/500, Loss: 3.5111\n",
      "Epoch 361/500, Loss: 3.5085\n",
      "Epoch 371/500, Loss: 3.5053\n",
      "Epoch 381/500, Loss: 3.5020\n",
      "Epoch 391/500, Loss: 3.4990\n",
      "Epoch 401/500, Loss: 3.4955\n",
      "Epoch 411/500, Loss: 3.4927\n",
      "Epoch 421/500, Loss: 3.4910\n",
      "Epoch 431/500, Loss: 3.4877\n",
      "Epoch 441/500, Loss: 3.4848\n",
      "Epoch 451/500, Loss: 3.4852\n",
      "Epoch 461/500, Loss: 3.4813\n",
      "Epoch 471/500, Loss: 3.4780\n",
      "Epoch 481/500, Loss: 3.4768\n",
      "Epoch 491/500, Loss: 3.4753\n",
      "Epoch 1/500, Loss: 36.8918\n",
      "Epoch 11/500, Loss: 5.3684\n",
      "Epoch 21/500, Loss: 4.4602\n",
      "Epoch 31/500, Loss: 4.2128\n",
      "Epoch 41/500, Loss: 4.0911\n",
      "Epoch 51/500, Loss: 4.0123\n",
      "Epoch 61/500, Loss: 3.9413\n",
      "Epoch 71/500, Loss: 3.8859\n",
      "Epoch 81/500, Loss: 3.8394\n",
      "Epoch 91/500, Loss: 3.7980\n",
      "Epoch 101/500, Loss: 3.7665\n",
      "Epoch 111/500, Loss: 3.7332\n",
      "Epoch 121/500, Loss: 3.7066\n",
      "Epoch 131/500, Loss: 3.6848\n",
      "Epoch 141/500, Loss: 3.6637\n",
      "Epoch 151/500, Loss: 3.6534\n",
      "Epoch 161/500, Loss: 3.6381\n",
      "Epoch 171/500, Loss: 3.6254\n",
      "Epoch 181/500, Loss: 3.6128\n",
      "Epoch 191/500, Loss: 3.6047\n",
      "Epoch 201/500, Loss: 3.5969\n",
      "Epoch 211/500, Loss: 3.5856\n",
      "Epoch 221/500, Loss: 3.5778\n",
      "Epoch 231/500, Loss: 3.5727\n",
      "Epoch 241/500, Loss: 3.5667\n",
      "Epoch 251/500, Loss: 3.5569\n",
      "Epoch 261/500, Loss: 3.5489\n",
      "Epoch 271/500, Loss: 3.5403\n",
      "Epoch 281/500, Loss: 3.5364\n",
      "Epoch 291/500, Loss: 3.5301\n",
      "Epoch 301/500, Loss: 3.5276\n",
      "Epoch 311/500, Loss: 3.5229\n",
      "Epoch 321/500, Loss: 3.5182\n",
      "Epoch 331/500, Loss: 3.5158\n",
      "Epoch 341/500, Loss: 3.5103\n",
      "Epoch 351/500, Loss: 3.5110\n",
      "Epoch 361/500, Loss: 3.5054\n",
      "Epoch 371/500, Loss: 3.5028\n",
      "Epoch 381/500, Loss: 3.4991\n",
      "Epoch 391/500, Loss: 3.4967\n",
      "Epoch 401/500, Loss: 3.4950\n",
      "Epoch 411/500, Loss: 3.4905\n",
      "Epoch 421/500, Loss: 3.4903\n",
      "Epoch 431/500, Loss: 3.4866\n",
      "Epoch 441/500, Loss: 3.4838\n",
      "Epoch 451/500, Loss: 3.4817\n",
      "Epoch 461/500, Loss: 3.4792\n",
      "Epoch 471/500, Loss: 3.4772\n",
      "Epoch 481/500, Loss: 3.4748\n",
      "Epoch 491/500, Loss: 3.4731\n",
      "Epoch 1/500, Loss: 44.3854\n",
      "Epoch 11/500, Loss: 5.0130\n",
      "Epoch 21/500, Loss: 4.2705\n",
      "Epoch 31/500, Loss: 4.0901\n",
      "Epoch 41/500, Loss: 3.9946\n",
      "Epoch 51/500, Loss: 3.9224\n",
      "Epoch 61/500, Loss: 3.8560\n",
      "Epoch 71/500, Loss: 3.8076\n",
      "Epoch 81/500, Loss: 3.7592\n",
      "Epoch 91/500, Loss: 3.7201\n",
      "Epoch 101/500, Loss: 3.6897\n",
      "Epoch 111/500, Loss: 3.6721\n",
      "Epoch 121/500, Loss: 3.6520\n",
      "Epoch 131/500, Loss: 3.6390\n",
      "Epoch 141/500, Loss: 3.6180\n",
      "Epoch 151/500, Loss: 3.6045\n",
      "Epoch 161/500, Loss: 3.5977\n",
      "Epoch 171/500, Loss: 3.5890\n",
      "Epoch 181/500, Loss: 3.5814\n",
      "Epoch 191/500, Loss: 3.5781\n",
      "Epoch 201/500, Loss: 3.5716\n",
      "Epoch 211/500, Loss: 3.5671\n",
      "Epoch 221/500, Loss: 3.5622\n",
      "Epoch 231/500, Loss: 3.5572\n",
      "Epoch 241/500, Loss: 3.5507\n",
      "Epoch 251/500, Loss: 3.5504\n",
      "Epoch 261/500, Loss: 3.5457\n",
      "Epoch 271/500, Loss: 3.5387\n",
      "Epoch 281/500, Loss: 3.5334\n",
      "Epoch 291/500, Loss: 3.5288\n",
      "Epoch 301/500, Loss: 3.5237\n",
      "Epoch 311/500, Loss: 3.5208\n",
      "Epoch 321/500, Loss: 3.5152\n",
      "Epoch 331/500, Loss: 3.5134\n",
      "Epoch 341/500, Loss: 3.5105\n",
      "Epoch 351/500, Loss: 3.5070\n",
      "Epoch 361/500, Loss: 3.5043\n",
      "Epoch 371/500, Loss: 3.4979\n",
      "Epoch 381/500, Loss: 3.4962\n",
      "Epoch 391/500, Loss: 3.4935\n",
      "Epoch 401/500, Loss: 3.4907\n",
      "Epoch 411/500, Loss: 3.4865\n",
      "Epoch 421/500, Loss: 3.4852\n",
      "Epoch 431/500, Loss: 3.4840\n",
      "Epoch 441/500, Loss: 3.4778\n",
      "Epoch 451/500, Loss: 3.4766\n",
      "Epoch 461/500, Loss: 3.4726\n",
      "Epoch 471/500, Loss: 3.4729\n",
      "Epoch 481/500, Loss: 3.4708\n",
      "Epoch 491/500, Loss: 3.4659\n",
      "Epoch 1/500, Loss: 49.2122\n",
      "Epoch 11/500, Loss: 5.2322\n",
      "Epoch 21/500, Loss: 4.3046\n",
      "Epoch 31/500, Loss: 4.0956\n",
      "Epoch 41/500, Loss: 3.9949\n",
      "Epoch 51/500, Loss: 3.9108\n",
      "Epoch 61/500, Loss: 3.8516\n",
      "Epoch 71/500, Loss: 3.7973\n",
      "Epoch 81/500, Loss: 3.7520\n",
      "Epoch 91/500, Loss: 3.7203\n",
      "Epoch 101/500, Loss: 3.7012\n",
      "Epoch 111/500, Loss: 3.6832\n",
      "Epoch 121/500, Loss: 3.6606\n",
      "Epoch 131/500, Loss: 3.6514\n",
      "Epoch 141/500, Loss: 3.6411\n",
      "Epoch 151/500, Loss: 3.6243\n",
      "Epoch 161/500, Loss: 3.6175\n",
      "Epoch 171/500, Loss: 3.6160\n",
      "Epoch 181/500, Loss: 3.6029\n",
      "Epoch 191/500, Loss: 3.5943\n",
      "Epoch 201/500, Loss: 3.5877\n",
      "Epoch 211/500, Loss: 3.5820\n",
      "Epoch 221/500, Loss: 3.5731\n",
      "Epoch 231/500, Loss: 3.5662\n",
      "Epoch 241/500, Loss: 3.5615\n",
      "Epoch 251/500, Loss: 3.5506\n",
      "Epoch 261/500, Loss: 3.5483\n",
      "Epoch 271/500, Loss: 3.5415\n",
      "Epoch 281/500, Loss: 3.5386\n",
      "Epoch 291/500, Loss: 3.5336\n",
      "Epoch 301/500, Loss: 3.5288\n",
      "Epoch 311/500, Loss: 3.5264\n",
      "Epoch 321/500, Loss: 3.5237\n",
      "Epoch 331/500, Loss: 3.5178\n",
      "Epoch 341/500, Loss: 3.5188\n",
      "Epoch 351/500, Loss: 3.5117\n",
      "Epoch 361/500, Loss: 3.5076\n",
      "Epoch 371/500, Loss: 3.5054\n",
      "Epoch 381/500, Loss: 3.5033\n",
      "Epoch 391/500, Loss: 3.5025\n",
      "Epoch 401/500, Loss: 3.4985\n",
      "Epoch 411/500, Loss: 3.4962\n",
      "Epoch 421/500, Loss: 3.4925\n",
      "Epoch 431/500, Loss: 3.4923\n",
      "Epoch 441/500, Loss: 3.4883\n",
      "Epoch 451/500, Loss: 3.4859\n",
      "Epoch 461/500, Loss: 3.4827\n",
      "Epoch 471/500, Loss: 3.4809\n",
      "Epoch 481/500, Loss: 3.4783\n",
      "Epoch 491/500, Loss: 3.4776\n",
      "Epoch 1/500, Loss: 41.2868\n",
      "Epoch 11/500, Loss: 5.6727\n",
      "Epoch 21/500, Loss: 4.4330\n",
      "Epoch 31/500, Loss: 4.1031\n",
      "Epoch 41/500, Loss: 3.9601\n",
      "Epoch 51/500, Loss: 3.8644\n",
      "Epoch 61/500, Loss: 3.7963\n",
      "Epoch 71/500, Loss: 3.7443\n",
      "Epoch 81/500, Loss: 3.7147\n",
      "Epoch 91/500, Loss: 3.6876\n",
      "Epoch 101/500, Loss: 3.6681\n",
      "Epoch 111/500, Loss: 3.6516\n",
      "Epoch 121/500, Loss: 3.6414\n",
      "Epoch 131/500, Loss: 3.6266\n",
      "Epoch 141/500, Loss: 3.6187\n",
      "Epoch 151/500, Loss: 3.6060\n",
      "Epoch 161/500, Loss: 3.6029\n",
      "Epoch 171/500, Loss: 3.5920\n",
      "Epoch 181/500, Loss: 3.5848\n",
      "Epoch 191/500, Loss: 3.5765\n",
      "Epoch 201/500, Loss: 3.5698\n",
      "Epoch 211/500, Loss: 3.5623\n",
      "Epoch 221/500, Loss: 3.5586\n",
      "Epoch 231/500, Loss: 3.5532\n",
      "Epoch 241/500, Loss: 3.5502\n",
      "Epoch 251/500, Loss: 3.5432\n",
      "Epoch 261/500, Loss: 3.5408\n",
      "Epoch 271/500, Loss: 3.5331\n",
      "Epoch 281/500, Loss: 3.5274\n",
      "Epoch 291/500, Loss: 3.5276\n",
      "Epoch 301/500, Loss: 3.5210\n",
      "Epoch 311/500, Loss: 3.5193\n",
      "Epoch 321/500, Loss: 3.5134\n",
      "Epoch 331/500, Loss: 3.5108\n",
      "Epoch 341/500, Loss: 3.5075\n",
      "Epoch 351/500, Loss: 3.5055\n",
      "Epoch 361/500, Loss: 3.5023\n",
      "Epoch 371/500, Loss: 3.4992\n",
      "Epoch 381/500, Loss: 3.4964\n",
      "Epoch 391/500, Loss: 3.4933\n",
      "Epoch 401/500, Loss: 3.4910\n",
      "Epoch 411/500, Loss: 3.4871\n",
      "Epoch 421/500, Loss: 3.4851\n",
      "Epoch 431/500, Loss: 3.4850\n",
      "Epoch 441/500, Loss: 3.4809\n",
      "Epoch 451/500, Loss: 3.4805\n",
      "Epoch 461/500, Loss: 3.4773\n",
      "Epoch 471/500, Loss: 3.4730\n",
      "Epoch 481/500, Loss: 3.4702\n",
      "Epoch 491/500, Loss: 3.4700\n",
      "Epoch 1/500, Loss: 40.3606\n",
      "Epoch 11/500, Loss: 5.3486\n",
      "Epoch 21/500, Loss: 4.3434\n",
      "Epoch 31/500, Loss: 4.0997\n",
      "Epoch 41/500, Loss: 3.9911\n",
      "Epoch 51/500, Loss: 3.9052\n",
      "Epoch 61/500, Loss: 3.8320\n",
      "Epoch 71/500, Loss: 3.7780\n",
      "Epoch 81/500, Loss: 3.7349\n",
      "Epoch 91/500, Loss: 3.6979\n",
      "Epoch 101/500, Loss: 3.6688\n",
      "Epoch 111/500, Loss: 3.6528\n",
      "Epoch 121/500, Loss: 3.6367\n",
      "Epoch 131/500, Loss: 3.6244\n",
      "Epoch 141/500, Loss: 3.6134\n",
      "Epoch 151/500, Loss: 3.6022\n",
      "Epoch 161/500, Loss: 3.5947\n",
      "Epoch 171/500, Loss: 3.5891\n",
      "Epoch 181/500, Loss: 3.5806\n",
      "Epoch 191/500, Loss: 3.5748\n",
      "Epoch 201/500, Loss: 3.5722\n",
      "Epoch 211/500, Loss: 3.5640\n",
      "Epoch 221/500, Loss: 3.5607\n",
      "Epoch 231/500, Loss: 3.5549\n",
      "Epoch 241/500, Loss: 3.5501\n",
      "Epoch 251/500, Loss: 3.5421\n",
      "Epoch 261/500, Loss: 3.5379\n",
      "Epoch 271/500, Loss: 3.5339\n",
      "Epoch 281/500, Loss: 3.5290\n",
      "Epoch 291/500, Loss: 3.5275\n",
      "Epoch 301/500, Loss: 3.5208\n",
      "Epoch 311/500, Loss: 3.5171\n",
      "Epoch 321/500, Loss: 3.5128\n",
      "Epoch 331/500, Loss: 3.5098\n",
      "Epoch 341/500, Loss: 3.5079\n",
      "Epoch 351/500, Loss: 3.5044\n",
      "Epoch 361/500, Loss: 3.5027\n",
      "Epoch 371/500, Loss: 3.4985\n",
      "Epoch 381/500, Loss: 3.4981\n",
      "Epoch 391/500, Loss: 3.4964\n",
      "Epoch 401/500, Loss: 3.4909\n",
      "Epoch 411/500, Loss: 3.4892\n",
      "Epoch 421/500, Loss: 3.4865\n",
      "Epoch 431/500, Loss: 3.4850\n",
      "Epoch 441/500, Loss: 3.4829\n",
      "Epoch 451/500, Loss: 3.4791\n",
      "Epoch 461/500, Loss: 3.4774\n",
      "Epoch 471/500, Loss: 3.4769\n",
      "Epoch 481/500, Loss: 3.4741\n",
      "Epoch 491/500, Loss: 3.4723\n",
      "Epoch 1/500, Loss: 44.6950\n",
      "Epoch 11/500, Loss: 5.3794\n",
      "Epoch 21/500, Loss: 4.4860\n",
      "Epoch 31/500, Loss: 4.2041\n",
      "Epoch 41/500, Loss: 4.0872\n",
      "Epoch 51/500, Loss: 3.9920\n",
      "Epoch 61/500, Loss: 3.9078\n",
      "Epoch 71/500, Loss: 3.8287\n",
      "Epoch 81/500, Loss: 3.7787\n",
      "Epoch 91/500, Loss: 3.7318\n",
      "Epoch 101/500, Loss: 3.7078\n",
      "Epoch 111/500, Loss: 3.6845\n",
      "Epoch 121/500, Loss: 3.6686\n",
      "Epoch 131/500, Loss: 3.6539\n",
      "Epoch 141/500, Loss: 3.6377\n",
      "Epoch 151/500, Loss: 3.6245\n",
      "Epoch 161/500, Loss: 3.6157\n",
      "Epoch 171/500, Loss: 3.6080\n",
      "Epoch 181/500, Loss: 3.6006\n",
      "Epoch 191/500, Loss: 3.5924\n",
      "Epoch 201/500, Loss: 3.5859\n",
      "Epoch 211/500, Loss: 3.5810\n",
      "Epoch 221/500, Loss: 3.5703\n",
      "Epoch 231/500, Loss: 3.5683\n",
      "Epoch 241/500, Loss: 3.5617\n",
      "Epoch 251/500, Loss: 3.5586\n",
      "Epoch 261/500, Loss: 3.5527\n",
      "Epoch 271/500, Loss: 3.5488\n",
      "Epoch 281/500, Loss: 3.5398\n",
      "Epoch 291/500, Loss: 3.5392\n",
      "Epoch 301/500, Loss: 3.5308\n",
      "Epoch 311/500, Loss: 3.5278\n",
      "Epoch 321/500, Loss: 3.5246\n",
      "Epoch 331/500, Loss: 3.5195\n",
      "Epoch 341/500, Loss: 3.5157\n",
      "Epoch 351/500, Loss: 3.5086\n",
      "Epoch 361/500, Loss: 3.5065\n",
      "Epoch 371/500, Loss: 3.5030\n",
      "Epoch 381/500, Loss: 3.5009\n",
      "Epoch 391/500, Loss: 3.4974\n",
      "Epoch 401/500, Loss: 3.4942\n",
      "Epoch 411/500, Loss: 3.4901\n",
      "Epoch 421/500, Loss: 3.4882\n",
      "Epoch 431/500, Loss: 3.4847\n",
      "Epoch 441/500, Loss: 3.4797\n",
      "Epoch 451/500, Loss: 3.4797\n",
      "Epoch 461/500, Loss: 3.4779\n",
      "Epoch 471/500, Loss: 3.4750\n",
      "Epoch 481/500, Loss: 3.4720\n",
      "Epoch 491/500, Loss: 3.4707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232/232 [01:40<00:00,  2.31it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'obs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     63\u001b[39m grn_adata = wrapper(model_zoo, data_tensor.cuda(), gene_names,  config)\n\u001b[32m     64\u001b[39m adob = adata.obs.reset_index()\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[43mgrn_adata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobs\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mcell_id\u001b[39m\u001b[33m'\u001b[39m] = np.array(adob[\u001b[33m'\u001b[39m\u001b[33mcell_id\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     66\u001b[39m grn_adata.obs[\u001b[33m'\u001b[39m\u001b[33mgrn\u001b[39m\u001b[33m'\u001b[39m] = np.array(adob[\u001b[33m'\u001b[39m\u001b[33mgrn\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     69\u001b[39m model_elapsed = time.monotonic()-model_start\n",
      "\u001b[31mAttributeError\u001b[39m: 'tuple' object has no attribute 'obs'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "sys.path.append('/data_nfs/og86asub/netmap/netmap-evaluation/')\n",
    "from netmap.src.utils.data_utils import *\n",
    "from netmap.src.utils.tf_utils import *\n",
    "from netmap.src.utils.netmap_config import NetmapConfig\n",
    "from netmap.src.model.negbinautoencoder import *\n",
    "from netmap.src.model.negbinautoencoder import train_autoencoder\n",
    "from netmap.src.model.inferrence_simple import *\n",
    "from netmap.src.model.pipeline import *\n",
    "\n",
    "from src.data_simulation.data_simulation_config import DataSimulationConfig\n",
    "\n",
    "\n",
    "import yaml\n",
    "def read_config(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "import os.path as op\n",
    "\n",
    "#config = NetmapConfig.read_yaml(\"/data_nfs/og86asub/netmap/netmap-evaluation/results/configurations/netmap/config/perturb_seq/\")\n",
    "dada = \"/data_nfs/og86asub/netmap/netmap-evaluation/results/configurations/data_simulation/config_easy/net_131_13677_net_67_13435_net_54_13860.config.yaml\"\n",
    "dataset_config = read_config(\"/data_nfs/og86asub/netmap/netmap-evaluation/results/configurations/data_simulation/config_easy/net_131_13677_net_67_13435_net_54_13860.config.yaml\")\n",
    "\n",
    "nets = [pd.read_csv(op.join(\"/data_nfs/og86asub/netmap/netmap-evaluation/data/clustered_network/\", filename), sep='\\t') for filename in dataset_config['edgelist']]\n",
    "\n",
    "netnet = pd.read_csv('/data_nfs/og86asub/netmap/netmap-evaluation/data/simulated_data/config_easy/net_131_13677_net_67_13435_net_54_13860/net.tsv', sep='\\t')\n",
    "    \n",
    "config = NetmapConfig.read_yaml('/data_nfs/og86asub/netmap/netmap-evaluation/results/netmap/config/config_easy/net_105_14145_net_76_12762_net_51_13833/config.yaml')\n",
    "dataset_config = DataSimulationConfig.read_yaml(dada)\n",
    "\n",
    "\n",
    "\n",
    "start_total = time.monotonic()\n",
    "\n",
    "## Load config and setup outputs\n",
    "os.makedirs(config.output_directory, exist_ok=True)\n",
    "sc.settings.figdir = config.output_directory\n",
    "config.write_yaml(yaml_file=op.join(config.output_directory, 'config.yaml'))\n",
    "\n",
    "## load data\n",
    "adata = sc.read_h5ad(config.input_data)\n",
    "\n",
    "\n",
    "## Get the data matrix from the CustumAnndata obeject\n",
    "gene_names = np.array(adata.var.index)\n",
    "model_start = time.monotonic()\n",
    "\n",
    "if config.layer == 'counts':\n",
    "    data_tensor = adata.layers['counts']\n",
    "else:\n",
    "    data_tensor = adata.X\n",
    "\n",
    "if scs.issparse(data_tensor):\n",
    "    data_tensor = torch.tensor(data_tensor.todense(), dtype=torch.float32)\n",
    "else:\n",
    "    data_tensor = torch.tensor(data_tensor, dtype=torch.float32)\n",
    "\n",
    "\n",
    "print(data_tensor.shape)\n",
    "\n",
    "model_zoo = create_model_zoo(data_tensor, n_models=config.n_models, n_epochs=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45a881dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/232 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232/232 [01:59<00:00,  1.95it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'leiden'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/data_nfs/og86asub/netmap/netmap-evaluation/netmap/.pixi/envs/default/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'leiden'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m grn_adata = \u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_zoo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgene_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 288\u001b[39m, in \u001b[36mwrapper\u001b[39m\u001b[34m(models, data_train_full_tensor, gene_names, config)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m#EDGE SELECTION:\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(attributions)):\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     edge_indices = \u001b[43mget_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattributions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_differential\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43muse_differential\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpercentile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpercentile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m     name_list = name_list + \u001b[38;5;28mlist\u001b[39m(gene_names[edge_indices])\n\u001b[32m    290\u001b[39m     target_names = target_names+[gene_names[i]]* \u001b[38;5;28mlen\u001b[39m(edge_indices)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 173\u001b[39m, in \u001b[36mget_edges\u001b[39m\u001b[34m(attribution_anndata, use_differential, percentile)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_edges\u001b[39m(attribution_anndata, use_differential=\u001b[38;5;28;01mFalse\u001b[39;00m, percentile = \u001b[32m10\u001b[39m):\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_differential:\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_differential_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattribution_anndata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpercentile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpercentile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    175\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m get_percentile_edges(attribution_anndata, percentile=percentile)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 135\u001b[39m, in \u001b[36mget_differential_edges\u001b[39m\u001b[34m(attribution_anndata, percentile)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_differential_edges\u001b[39m(attribution_anndata, percentile = \u001b[32m10\u001b[39m):\n\u001b[32m    134\u001b[39m     genelist = []\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np.unique(\u001b[43mattribution_anndata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mleiden\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m))>\u001b[32m1\u001b[39m :\n\u001b[32m    136\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m cat \u001b[38;5;129;01min\u001b[39;00m np.unique(attribution_anndata.obs[\u001b[33m'\u001b[39m\u001b[33mleiden\u001b[39m\u001b[33m'\u001b[39m]):\n\u001b[32m    137\u001b[39m             statisi =rank_compare_2indep(x1=attribution_anndata.X[attribution_anndata.obs[\u001b[33m'\u001b[39m\u001b[33mleiden\u001b[39m\u001b[33m'\u001b[39m]==cat], x2= attribution_anndata.X[attribution_anndata.obs[\u001b[33m'\u001b[39m\u001b[33mleiden\u001b[39m\u001b[33m'\u001b[39m]!=cat])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data_nfs/og86asub/netmap/netmap-evaluation/netmap/.pixi/envs/default/lib/python3.12/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data_nfs/og86asub/netmap/netmap-evaluation/netmap/.pixi/envs/default/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'leiden'"
     ]
    }
   ],
   "source": [
    "\n",
    "grn_adata = wrapper(model_zoo, data_tensor.cuda(), gene_names,  config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c72ff23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, None, None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grn_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3288d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      " tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "\n",
      "Tensor with Each Column Shuffled Independently:\n",
      " tensor([[ 7,  5,  3],\n",
      "        [ 1, 11, 12],\n",
      "        [ 4,  2,  6],\n",
      "        [10,  8,  9]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "# If you had a tensor like this:\n",
    "original_data = torch.tensor([[1, 2, 3],\n",
    "                              [4, 5, 6],\n",
    "                              [7, 8, 9],\n",
    "                              [10, 11, 12]])\n",
    "\n",
    "shuffled_data = shuffle_each_column_independently(original_data)\n",
    "print(\"Original Tensor:\\n\", original_data)\n",
    "print(\"\\nTensor with Each Column Shuffled Independently:\\n\", shuffled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eddf7e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0  1  2  3\n",
       "1  4  5  6\n",
       "2  7  8  9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(original_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f16d61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0  1  3  2\n",
       "1  4  6  5\n",
       "2  7  9  8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(shuffled_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixi_netmap",
   "language": "python",
   "name": "pixi_netmap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
