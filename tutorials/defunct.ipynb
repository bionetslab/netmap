{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02bf214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def flatten_upper_triangular_excluding_diagonal(matrix):\n",
    "    \"\"\"\n",
    "    Flattens the upper triangular part of a matrix, excluding the diagonal.\n",
    "\n",
    "    Args:\n",
    "        matrix: A list of lists or a NumPy array representing the matrix.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array containing the flattened upper triangular elements.\n",
    "    \"\"\"\n",
    "    # Convert the input to a NumPy array for efficient operations.\n",
    "    np_matrix = np.array(matrix)\n",
    "\n",
    "    # Get the upper triangular part of the matrix, excluding the diagonal.\n",
    "    # The 'k=1' argument specifies that the diagonal should not be included.\n",
    "    upper_triangle = np.triu(np_matrix, k=1)\n",
    "\n",
    "    # Flatten the resulting matrix.\n",
    "    flattened_matrix = upper_triangle.flatten()\n",
    "\n",
    "    # Filter out the zero values that were not part of the original matrix.\n",
    "    # We use a boolean mask to keep only non-zero elements.\n",
    "    result = flattened_matrix[flattened_matrix != 0]\n",
    "\n",
    "    return result\n",
    "\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "\n",
    "def get_max_identity_class_size(high_average_similarity_idx):\n",
    "    # order not important\n",
    "    identity_class_sizes_l = []\n",
    "    identity_class_sizes = {}\n",
    "    for k in high_average_similarity_idx.keys():\n",
    "        identity_class_sizes[k] = len(high_average_similarity_idx[k])\n",
    "        identity_class_sizes_l.append(len(high_average_similarity_idx[k]))\n",
    "    return np.max(identity_class_sizes_l)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_id_class_representative(data_dict: OrderedDict, keys):\n",
    "\n",
    "    elements = []\n",
    "    for key in keys:\n",
    "        value = data_dict[key]\n",
    "        \n",
    "        # Check if the 'unused' list is empty\n",
    "        if not value['unused']:\n",
    "            value['unused'].extend(value['used'])\n",
    "            value['used'].clear()\n",
    "            random.shuffle(value['unused'])\n",
    "\n",
    "        random_index = random.randint(0, len(value['unused']) - 1)\n",
    "        element = value['unused'].pop(random_index)\n",
    "        value['used'].append(element)\n",
    "        elements.append(element)\n",
    "\n",
    "    return elements, data_dict\n",
    "def create_dataset_assembly_dict(high_average_similarity_idx):\n",
    "    id_class_dictionary = {}\n",
    "    all_genes_in_identity_class = []\n",
    "    for k in  high_average_similarity_idx:\n",
    "        id_class_dictionary[k] = {'unused': high_average_similarity_idx[k].copy(), 'used' : []}\n",
    "        all_genes_in_identity_class = all_genes_in_identity_class+high_average_similarity_idx[k]\n",
    "    return id_class_dictionary, all_genes_in_identity_class\n",
    "\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "\n",
    "def get_id_class_representative(data_dict: OrderedDict, keys) -> OrderedDict:\n",
    "\n",
    "    elements = []\n",
    "    for key in keys:\n",
    "        value = data_dict[key]\n",
    "        \n",
    "        # Check if the 'unused' list is empty\n",
    "        if not value['unused']:\n",
    "            value['unused'].extend(value['used'])\n",
    "            value['used'].clear()\n",
    "            random.shuffle(value['unused'])\n",
    "\n",
    "        random_index = random.randint(0, len(value['unused']) - 1)\n",
    "        element = value['unused'].pop(random_index)\n",
    "        value['used'].append(element)\n",
    "        elements.append(element)\n",
    "\n",
    "    return elements, data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae6726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data_nfs/og86asub/netmap/netmap-evaluation/')\n",
    "\n",
    "import scanpy as sc\n",
    "import time \n",
    "\n",
    "from netmap.src.utils.misc import write_config\n",
    "\n",
    "from netmap.src.model.negbinautoencoder import *\n",
    "import scanpy as sc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from captum.attr import GradientShap, LRP\n",
    "from netmap.src.model.inferrence_simple import *\n",
    "from netmap.src.utils.data_utils import attribution_to_anndata\n",
    "from netmap.src.model.pipeline import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "import os.path as op\n",
    "import os\n",
    "\n",
    "import anndata as ad\n",
    "from statsmodels.stats.nonparametric import rank_compare_2indep\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as scs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from captum.attr import *\n",
    "import pingouin as pingu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class ZINBLoss(nn.Module):\n",
    "    def __init__(self, scale_factor=1.0, eps=1e-10, ridge_lambda=0.0):\n",
    "        \"\"\"\n",
    "        Zero-Inflated Negative Binomial (ZINB) Loss\n",
    "        Args:\n",
    "            scale_factor (float): Scale factor applied to predictions.\n",
    "            eps (float): Small value for numerical stability.\n",
    "            ridge_lambda (float): Regularization weight for the zero-inflation probability (pi).\n",
    "        \"\"\"\n",
    "        super(ZINBLoss, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.eps = eps\n",
    "        self.ridge_lambda = ridge_lambda\n",
    "\n",
    "    def forward(self, y_true, y_pred, theta, pi):\n",
    "        \"\"\"\n",
    "        Compute the ZINB loss.\n",
    "        Args:\n",
    "            y_true (torch.Tensor): Ground truth counts (non-negative integers).\n",
    "            y_pred (torch.Tensor): Predicted mean values (mu).\n",
    "            theta (torch.Tensor): Dispersion parameter (shape parameter).\n",
    "            pi (torch.Tensor): Zero-inflation probability (between 0 and 1).\n",
    "        Returns:\n",
    "            torch.Tensor: ZINB negative log-likelihood.\n",
    "        \"\"\"\n",
    "        eps = self.eps\n",
    "        y_true = y_true.float()\n",
    "        y_pred = y_pred.float() * self.scale_factor\n",
    "        theta = theta.float()\n",
    "        pi = torch.clamp(pi.float(), min=eps, max=1 - eps)  # Ensure pi is in (0, 1)\n",
    "\n",
    "        # Clip theta to avoid numerical issues\n",
    "        theta = torch.clamp(theta, max=1e6)\n",
    "\n",
    "        # Negative binomial log-likelihood\n",
    "        nb_case = (\n",
    "            torch.lgamma(theta + eps)\n",
    "            + torch.lgamma(y_true + 1.0)\n",
    "            - torch.lgamma(y_true + theta + eps)\n",
    "            + (theta + y_true) * torch.log(1.0 + (y_pred / (theta + eps)))\n",
    "            + y_true * (torch.log(theta + eps) - torch.log(y_pred + eps))\n",
    "        )\n",
    "\n",
    "        # Zero-inflation log-likelihood for y_true = 0\n",
    "        zero_nb = torch.pow(theta / (theta + y_pred + eps), theta)\n",
    "        zero_case = -torch.log(pi + ((1.0 - pi) * zero_nb) + eps)\n",
    "\n",
    "        # Combine cases: zero or NB\n",
    "        result = torch.where(y_true < eps, zero_case, nb_case)\n",
    "\n",
    "        # Add ridge penalty for pi\n",
    "        ridge = self.ridge_lambda * torch.square(pi)\n",
    "        result += ridge\n",
    "\n",
    "        return torch.mean(result)  # Return mean loss over the batch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class ZINBAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, dropout_rate=0.0, hidden_dim = 128):\n",
    "        super(ZINBAutoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),  # Dropout after activation\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder for mean (mu)\n",
    "        self.decoder_mu = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),  # Dropout after activation\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Softplus()  # Ensure non-negative predictions\n",
    "        )\n",
    "        \n",
    "        # Decoder for dispersion (theta)\n",
    "        self.decoder_theta = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),  # Dropout after activation\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Softplus()  # Ensure non-negative dispersion\n",
    "        )\n",
    "        \n",
    "        # Decoder for zero-inflation probability (pi)\n",
    "        self.decoder_pi = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),  # Dropout after activation\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()  # Ensure probability values between 0 and 1\n",
    "        )\n",
    "        \n",
    "        self.zinb_loss = ZINBLoss()  # Use ZINBLoss for the computation\n",
    "        self.forward_mu_only = False\n",
    "        self.forward_theta_only = False\n",
    "        self.latent_only = False\n",
    "        self.forward_pi_only = False\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        latent = self.encoder(x)\n",
    "        mu = self.decoder_mu(latent)\n",
    "        theta = self.decoder_theta(latent)\n",
    "        pi = self.decoder_pi(latent)\n",
    "\n",
    "        #data = self.decoder_data(latent)\n",
    "        if self.forward_theta_only:\n",
    "            return theta\n",
    "        elif self.forward_mu_only:\n",
    "            return mu \n",
    "        elif self.latent_only:\n",
    "            return latent\n",
    "        elif self.forward_pi_only:\n",
    "            return pi\n",
    "        else:\n",
    "            return mu, theta, pi\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     # Latent representation\n",
    "    #     latent = self.encoder(x)\n",
    "        \n",
    "    #     # Decode into mu, theta, and pi\n",
    "    #     mu = self.decoder_mu(latent)\n",
    "    #     theta = self.decoder_theta(latent)\n",
    "    #     pi = self.decoder_pi(latent)\n",
    "        \n",
    "    #     return mu, theta, pi\n",
    "\n",
    "    def compute_loss(self, x):\n",
    "        # Forward pass\n",
    "        mu, theta, pi = self.forward(x)\n",
    "        \n",
    "        # Compute ZINB loss\n",
    "        loss = self.zinb_loss(x, mu, theta, pi)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "def create_model_zoo(data_tensor, n_models = 4, n_epochs = 500):\n",
    "    model_zoo = []\n",
    "    for _ in range(n_models):\n",
    "\n",
    "        data_train2, data_test2 = train_test_split(data_tensor,test_size=0.01, shuffle=True)\n",
    "\n",
    "        trained_model2 = NegativeBinomialAutoencoder(input_dim=data_tensor.shape[1], latent_dim=10, dropout_rate = 0.02)\n",
    "        trained_model2 = trained_model2.cuda()\n",
    "\n",
    "        optimizer2 = torch.optim.Adam(trained_model2.parameters(), lr=1e-4)\n",
    "\n",
    "        trained_model2 = train_autoencoder(\n",
    "                trained_model2,\n",
    "                data_train2.cuda(),\n",
    "                optimizer2,\n",
    "                num_epochs=n_epochs\n",
    "\n",
    "            )\n",
    "        model_zoo.append(trained_model2)\n",
    "    return model_zoo\n",
    "\n",
    "\n",
    "def create_model_zoo(data_tensor, n_models = 4, n_epochs = 500):\n",
    "    model_zoo = []\n",
    "    for _ in range(n_models):\n",
    "\n",
    "        data_train2, data_test2 = train_test_split(data_tensor,test_size=0.01, shuffle=True)\n",
    "\n",
    "        trained_model2 = ZINBAutoencoder(input_dim=data_tensor.shape[1], latent_dim=10, dropout_rate = 0.02)\n",
    "        trained_model2 = trained_model2.cuda()\n",
    "\n",
    "        optimizer2 = torch.optim.Adam(trained_model2.parameters(), lr=1e-4)\n",
    "\n",
    "        trained_model2 = train_autoencoder(\n",
    "                trained_model2,\n",
    "                data_train2.cuda(),\n",
    "                optimizer2,\n",
    "                num_epochs=n_epochs\n",
    "\n",
    "            )\n",
    "        model_zoo.append(trained_model2)\n",
    "    return model_zoo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_latent_true(model_zoo):\n",
    "    for mo in model_zoo:\n",
    "        mo.forward_mu_only = False\n",
    "        mo.forward_theta_only = False\n",
    "        mo.latent_only = True\n",
    "    return model_zoo\n",
    "\n",
    "\n",
    "def set_all_false(model_zoo):\n",
    "    for mo in model_zoo:\n",
    "        mo.forward_mu_only = False\n",
    "        mo.forward_theta_only = False\n",
    "        mo.latent_only = False\n",
    "    return model_zoo\n",
    "\n",
    "def shuffle_each_column_independently(tensor):\n",
    "    \"\"\"\n",
    "    Shuffles each column of a 2D PyTorch tensor independently.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): The input tensor.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A new tensor with each of its columns independently shuffled.\n",
    "    \"\"\"\n",
    "    if tensor.dim() != 2:\n",
    "        raise ValueError(\"Input tensor must be 2-dimensional to shuffle columns.\")\n",
    "\n",
    "    # Create an empty tensor of the same size to store the shuffled columns\n",
    "    shuffled_tensor = torch.empty_like(tensor)\n",
    "\n",
    "    # Iterate through each column, shuffle it, and place it in the new tensor\n",
    "    for i in range(tensor.size(1)):\n",
    "        column = tensor[:, i]\n",
    "        idx = torch.randperm(column.nelement())\n",
    "        shuffled_tensor[:, i] = column[idx]\n",
    "\n",
    "    return shuffled_tensor\n",
    "\n",
    "\n",
    "def attribution_one_target( \n",
    "        target_gene,\n",
    "        lrp_model,\n",
    "        input_data,\n",
    "        background,\n",
    "        xai_type='lrp-like',\n",
    "        randomize_background = False):\n",
    "    \n",
    "    attributions_list = []\n",
    "    for m in range(len(lrp_model)):\n",
    "        # Randomize backgorund for each round\n",
    "        if randomize_background:\n",
    "            background = shuffle_each_column_independently(background)\n",
    "\n",
    "        model = lrp_model[m]\n",
    "        #for _ in range(num_iterations):\n",
    "        if xai_type == 'lrp-like':\n",
    "            #print(input_data)\n",
    "            #print(target_gene)\n",
    "            attribution = model.attribute(input_data, target=target_gene)\n",
    "                \n",
    "        elif xai_type == 'shap-like':\n",
    "            attribution = model.attribute(input_data, baselines = background, target = target_gene)\n",
    "\n",
    "        attributions_list.append(attribution.detach().cpu().numpy())\n",
    "    return attributions_list\n",
    "\n",
    "def get_differential_edges(attribution_anndata, percentile = 10):\n",
    "    genelist = []\n",
    "    if len(np.unique(attribution_anndata.obs['leiden']))>1 :\n",
    "        for cat in np.unique(attribution_anndata.obs['leiden']):\n",
    "            statisi =rank_compare_2indep(x1=attribution_anndata.X[attribution_anndata.obs['leiden']==cat], x2= attribution_anndata.X[attribution_anndata.obs['leiden']!=cat])\n",
    "            sig_and_high = np.where((statisi.pvalue<(0.01/(attribution_anndata.X.shape[1]*attribution_anndata.X.shape[1])))  & (statisi.prob1>= 0.9))\n",
    "            genelist = genelist+ list(sig_and_high[0])\n",
    "\n",
    "    else:\n",
    "        # FALLBACk\n",
    "        m = np.abs(attribution_anndata.X).mean(axis=0)\n",
    "        # Get the indices of genes in the top 10%\n",
    "        top_10_percent_indices = np.where(m > np.percentile(m, 100-percentile))[0]\n",
    "\n",
    "        # Get the indices of genes in the bottom 10%\n",
    "        bottom_10_percent_indices = np.where(m < np.percentile(m, percentile))[0]\n",
    "\n",
    "        # Combine the two arrays of indices and sort them\n",
    "        genelist = np.unique(np.sort(\n",
    "            np.concatenate((top_10_percent_indices, bottom_10_percent_indices))\n",
    "        ))\n",
    "    return genelist\n",
    "\n",
    "def get_percentile_edges(attribution_anndata, percentile = 10):\n",
    "    # FALLBACk\n",
    "    m = attribution_anndata.X.mean(axis=0)\n",
    "    # Get the indices of genes in the top 10%\n",
    "    top_10_percent_indices = np.where(m > np.percentile(m, 100-percentile))[0]\n",
    "\n",
    "    # Get the indices of genes in the bottom 10%\n",
    "    bottom_10_percent_indices = np.where(m < np.percentile(m, percentile))[0]\n",
    "\n",
    "    # Combine the two arrays of indices and sort them\n",
    "    genelist = np.unique(np.sort(\n",
    "        np.concatenate((top_10_percent_indices, bottom_10_percent_indices))\n",
    "    ))\n",
    "    return genelist\n",
    "\n",
    "def get_edges(attribution_anndata, use_differential=False, percentile = 10):\n",
    "    if use_differential:\n",
    "        return get_differential_edges(attribution_anndata, percentile=percentile)\n",
    "    else:\n",
    "        return get_percentile_edges(attribution_anndata, percentile=percentile)\n",
    "    \n",
    "def get_explainer(model, explainer_type, raw=False):\n",
    "    if explainer_type in ['GuidedBackprop', 'Deconvolution']:\n",
    "        explainer_mode = 'lrp-like'\n",
    "    else:\n",
    "        explainer_mode = 'shap-like'\n",
    "    \n",
    "        \n",
    "    if explainer_type == 'GuidedBackprop': #fast\n",
    "        explainer = GuidedBackprop(model)\n",
    "    elif explainer_type == 'GradientShap': #fast\n",
    "        if raw:\n",
    "            explainer = GradientShap(model, multiply_by_inputs=False)\n",
    "        else:\n",
    "            explainer = GradientShap(model, multiply_by_inputs=True)\n",
    "\n",
    "    elif explainer_type == 'Deconvolution': #fast\n",
    "        explainer = Deconvolution(model)\n",
    "    else:\n",
    "        raise ValueError('no such method')\n",
    "        \n",
    "    return explainer, explainer_mode\n",
    "\n",
    "def compute_correlation_metric(data, cor_type):\n",
    "    # Compute gene correlation measure\n",
    "    #  'pingouin.pcorr', 'np.cov', 'np.corcoeff'\n",
    "    if cor_type ==  'pingouin.pcorr':\n",
    "        cov = pingu.pcorr(pd.DataFrame(data))\n",
    "    elif cor_type == 'np.cov':\n",
    "        cov = np.cov(data.T)\n",
    "    elif cor_type == 'np.corrcoeff':\n",
    "        cov = np.corrcoef(data.T)\n",
    "    elif cor_type == 'None':\n",
    "        cov = 1\n",
    "    else: \n",
    "        cov = 1\n",
    "    return cov\n",
    "\n",
    "def aggregate_attributions(attributions, strategy = 'mean'):\n",
    "    if strategy == 'mean':\n",
    "        return np.mean(attributions, axis = 0)\n",
    "    elif strategy == 'sum':\n",
    "        return np.sum(attributions, axis = 0)\n",
    "    elif strategy == 'median':\n",
    "        return np.median(attributions, axis = 0)\n",
    "    else:\n",
    "        # Default to mean aggregation\n",
    "        return np.mean(attributions, axis = 0)\n",
    "    \n",
    "\n",
    "    \n",
    "def wrapper(models, data_train_full_tensor, gene_names, config):\n",
    "\n",
    "    data = data_train_full_tensor.detach().cpu().numpy()\n",
    "    tms = []\n",
    "    name_list = []\n",
    "    target_names = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ings = {}\n",
    "    for trained_model in models:        \n",
    "        trained_model.forward_mu_only = True\n",
    "        explainer, xai_type = get_explainer(trained_model, config.xai_method, config.raw_attribution)\n",
    "        tms.append(explainer)\n",
    "\n",
    "    attributions = []\n",
    "    ## ATTRIBUTIONS\n",
    "    for g in tqdm(range(data_train_full_tensor.shape[1])):\n",
    "    #for g in range(2):\n",
    "\n",
    "        attributions_list = attribution_one_target(\n",
    "            g,\n",
    "            tms,\n",
    "            data_train_full_tensor,\n",
    "            data_train_full_tensor,\n",
    "            xai_type=xai_type,\n",
    "            randomize_background = True)\n",
    "        attributions.append(attributions_list)\n",
    "\n",
    "    \n",
    "\n",
    "    ## AGGREGATION: REPLACE LIST BY AGGREGATED DATA\n",
    "    for i in range(len(attributions)):\n",
    "        # CURRENTLY MEAN\n",
    "        attributions[i] = aggregate_attributions(attributions[i], strategy=config.aggregation_strategy )\n",
    "    \n",
    "    print(attributions)\n",
    "    ## PENALIZE:\n",
    "    if config.penalty != 'None':\n",
    "        penalty_matrix = compute_correlation_metric(data, cor_type=config.penalty)\n",
    "        for i in range(len(attributions)):\n",
    "            # CURRENTLY MEAN\n",
    "            attributions[i] = np.dot(attributions[i], (1-penalty_matrix))\n",
    "\n",
    "    print(attributions)\n",
    "    \n",
    "    ## CLUSTERING: CLUSTER EACH TARGET INDVIDUALLY\n",
    "    for i in range(len(attributions)):\n",
    " \n",
    "        attributions[i] = ad.AnnData(attributions[i])\n",
    "        print(attributions[i])\n",
    "        sc.pp.scale(attributions[i])\n",
    "        try:\n",
    "            sc.pp.pca(attributions[i],n_comps=50)\n",
    "        except:\n",
    "            try:\n",
    "                sc.pp.pca(attributions[i],n_comps=50 )\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "        sc.pp.neighbors(attributions[i], n_neighbors=15)\n",
    "        sc.tl.leiden(attributions[i], resolution=0.1)\n",
    "\n",
    "        clusterings[f'T_{gene_names[i]}'] = np.array(attributions[i].obs['leiden'])\n",
    "\n",
    "    \n",
    "    #EDGE SELECTION:\n",
    "    for i in range(len(attributions)):\n",
    "        edge_indices = get_edges(attributions[i], use_differential=config.use_differential, percentile=config.percentile)\n",
    "        name_list = name_list + list(gene_names[edge_indices])\n",
    "        target_names = target_names+[gene_names[i]]* len(edge_indices)\n",
    "        attributions[i] = attributions[i][:,edge_indices].X\n",
    "\n",
    "    attributions = np.hstack(attributions)\n",
    "    \n",
    "    index_list = [f\"{s}_{t}\" for (s, t) in zip(name_list, target_names)]\n",
    "    cou = pd.DataFrame({'index': index_list, 'source':name_list, 'target':target_names})\n",
    "    cou = cou.set_index('index')\n",
    "\n",
    "    clusterings = pd.DataFrame(clusterings)\n",
    "\n",
    "    grn_adata = attribution_to_anndata(attributions, var=cou, obs = clusterings)\n",
    "\n",
    "    return grn_adata\n",
    "\n",
    "def run_netmap(config, dataset_config):\n",
    "\n",
    "    print('Version 2')\n",
    "    start_total = time.monotonic()\n",
    "    \n",
    "    ## Load config and setup outputs\n",
    "    os.makedirs(config.output_directory, exist_ok=True)\n",
    "    sc.settings.figdir = config.output_directory\n",
    "    config.write_yaml(yaml_file=op.join(config.output_directory, 'config.yaml'))\n",
    "\n",
    "    ## load data\n",
    "    adata = sc.read_h5ad(config.input_data)\n",
    "    \n",
    "\n",
    "    ## Get the data matrix from the CustumAnndata obeject\n",
    "\n",
    "    gene_names = np.array(adata.var.index)\n",
    "    model_start = time.monotonic()\n",
    "\n",
    "    if config.layer == 'counts':\n",
    "        data_tensor = adata.layers['counts']\n",
    "    else:\n",
    "        data_tensor = adata.X\n",
    "\n",
    "    if scs.issparse(data_tensor):\n",
    "        data_tensor = torch.tensor(data_tensor.todense(), dtype=torch.float32)\n",
    "    else:\n",
    "        data_tensor = torch.tensor(data_tensor, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    print(data_tensor.shape)\n",
    "\n",
    "    model_zoo = create_model_zoo(data_tensor, n_models=config.n_models, n_epochs=500)\n",
    "    grn_adata = wrapper(model_zoo, data_tensor.cuda(), gene_names, config)\n",
    "\n",
    "    adob = adata.obs.reset_index()\n",
    "    grn_adata.obs['cell_id'] = np.array(adob['cell_id'])\n",
    "    grn_adata.obs['grn'] = np.array(adob['grn'])\n",
    "\n",
    "    \n",
    "    model_elapsed = time.monotonic()-model_start\n",
    "    grn_adata.write_h5ad(op.join(config.output_directory,config.adata_filename))\n",
    "\n",
    "    time_elapsed_total = time.monotonic()-start_total\n",
    "\n",
    "\n",
    "    res = {'time_elapsed_total': time_elapsed_total, 'time_elapsed_netmap': model_elapsed} \n",
    "    write_config(res, file=op.join(config.output_directory, 'results.yaml'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "sys.path.append('/data_nfs/og86asub/netmap/netmap-evaluation/')\n",
    "from netmap.src.utils.data_utils import *\n",
    "from netmap.src.utils.tf_utils import *\n",
    "from netmap.src.utils.netmap_config import NetmapConfig\n",
    "from netmap.src.model.negbinautoencoder import *\n",
    "from netmap.src.model.negbinautoencoder import train_autoencoder\n",
    "from netmap.src.model.inferrence_simple import *\n",
    "from netmap.src.model.pipeline import *\n",
    "\n",
    "from src.data_simulation.data_simulation_config import DataSimulationConfig\n",
    "\n",
    "\n",
    "import yaml\n",
    "def read_config(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "import os.path as op\n",
    "\n",
    "#config = NetmapConfig.read_yaml(\"/data_nfs/og86asub/netmap/netmap-evaluation/results/configurations/netmap/config/perturb_seq/\")\n",
    "dada = \"/data_nfs/og86asub/netmap/netmap-evaluation/results/configurations/data_simulation/config_easy/net_172_54892_net_131_54992_net_158_55084.config.yaml\"\n",
    "dataset_config = read_config(\"/data_nfs/og86asub/netmap/netmap-evaluation/results/configurations/data_simulation/config_easy/net_172_54892_net_131_54992_net_158_55084.config.yaml\")\n",
    "\n",
    "nets = [pd.read_csv(op.join(\"/data_nfs/og86asub/netmap/netmap-evaluation/data/clustered_network/\", filename), sep='\\t') for filename in dataset_config['edgelist']]\n",
    "common = [pd.read_csv(op.join(\"/data_nfs/og86asub/netmap/netmap-evaluation/data/clustered_network/\", filename), sep='\\t') for filename in dataset_config['common_edges']]\n",
    "\n",
    "    \n",
    "config = NetmapConfig.read_yaml('/data_nfs/og86asub/netmap/netmap-evaluation/results/netmap/config_22/config_easy/net_172_54892_net_131_54992_net_158_55084/config.yaml')\n",
    "dataset_config = DataSimulationConfig.read_yaml(dada)\n",
    "\n",
    "\n",
    "\n",
    "start_total = time.monotonic()\n",
    "\n",
    "## Load config and setup outputs\n",
    "os.makedirs(config.output_directory, exist_ok=True)\n",
    "sc.settings.figdir = config.output_directory\n",
    "config.write_yaml(yaml_file=op.join(config.output_directory, 'config.yaml'))\n",
    "\n",
    "## load data\n",
    "adata = sc.read_h5ad(config.input_data)\n",
    "\n",
    "\n",
    "## Get the data matrix from the CustumAnndata obeject\n",
    "gene_names = np.array(adata.var.index)\n",
    "model_start = time.monotonic()\n",
    "\n",
    "if config.layer == 'counts':\n",
    "    data_tensor = adata.layers['counts']\n",
    "else:\n",
    "    data_tensor = adata.X\n",
    "\n",
    "if scs.issparse(data_tensor):\n",
    "    data_tensor = torch.tensor(data_tensor.todense(), dtype=torch.float32)\n",
    "else:\n",
    "    data_tensor = torch.tensor(data_tensor, dtype=torch.float32)\n",
    "\n",
    "\n",
    "print(data_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76c226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model_zoo(data_tensor, id_class_dictionary, keys, other_genes, n_models = 4, n_epochs = 500):\n",
    "    model_zoo = []\n",
    "    variables_selected = []\n",
    "    representatives = []\n",
    "    for _ in range(n_models):\n",
    "\n",
    "        elements, id_class_dictionary = get_id_class_representative(id_class_dictionary, keys)\n",
    "\n",
    "        current_data_selection = list(other_genes)+list(elements)\n",
    "        current_data = data_tensor[:,current_data_selection]\n",
    "        data_train2, data_test2 = train_test_split(current_data,test_size=0.01, shuffle=True)\n",
    "\n",
    "        trained_model2 = ZINBAutoencoder(input_dim=current_data.shape[1], latent_dim=10, dropout_rate = 0.02)\n",
    "        trained_model2 = trained_model2.cuda()\n",
    "\n",
    "        optimizer2 = torch.optim.Adam(trained_model2.parameters(), lr=1e-4)\n",
    "\n",
    "        trained_model2 = train_autoencoder(\n",
    "                trained_model2,\n",
    "                data_train2.cuda(),\n",
    "                optimizer2,\n",
    "                num_epochs=n_epochs\n",
    "\n",
    "            )\n",
    "        model_zoo.append(trained_model2)\n",
    "        variables_selected.append(current_data_selection)\n",
    "        representatives.append(elements)\n",
    "    return model_zoo, variables_selected, representatives\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee2e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "from scipy.stats import spearmanr\n",
    "from statsmodels.formula.api import quantreg\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def get_hierarchical_clustering(adata):\n",
    "\n",
    "    corr_matrix, _ = spearmanr(adata.X, axis=0)\n",
    "\n",
    "    corr_matrix = np.corrcoef(adata.X.T)\n",
    "\n",
    "    corr_dist = 1 - corr_matrix\n",
    "    dist_linkage = hierarchy.average(corr_dist)\n",
    "\n",
    "    df = pd.DataFrame({'cophenet':hierarchy.cophenet(dist_linkage), 'corr':  flatten_upper_triangular_excluding_diagonal(corr_matrix)})\n",
    "\n",
    "    low_quantile_model = quantreg('corr ~ cophenet', df).fit(q=0.1)\n",
    "\n",
    "    # np.sort is used to ensure the line is drawn smoothly from left to right\n",
    "    x_sorted = np.sort(df['cophenet'])\n",
    "    y_predicted = low_quantile_model.predict({'cophenet': x_sorted})\n",
    "\n",
    "    return dist_linkage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fit_regression_clusterings(corr_matrix, dist_linkage, threshold=2):\n",
    "\n",
    "    df = pd.DataFrame({'cophenet':hierarchy.cophenet(dist_linkage), 'corr':  flatten_upper_triangular_excluding_diagonal(corr_matrix)})\n",
    "    df =df[df.cophenet<threshold]\n",
    "    low_quantile_model = quantreg('corr ~ cophenet', df).fit(q=0.1)\n",
    "\n",
    "    # np.sort is used to ensure the line is drawn smoothly from left to right\n",
    "    df = df.sort_values('cophenet')\n",
    "    x_sorted = df['cophenet']\n",
    "\n",
    "    # model line\n",
    "    y_predicted = low_quantile_model.predict({'cophenet': x_sorted})\n",
    "    df['linear_model'] = y_predicted\n",
    "    return df\n",
    "\n",
    "def plot_regression(df):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.scatter(df['cophenet'], df['corr'], alpha=0.7, label='Data Points')\n",
    "\n",
    "    ax.plot(df['cophenet'], df['linear_model'], color='red', linewidth=2, label='10th Percentile Quantile Regression Line')\n",
    "    ax.axhline(y=0.6, color='r', linestyle='--', label='y = 0.6')\n",
    "    # Add labels and a legend for clarity\n",
    "    plt.title('Quantile Regression with correlation threshold')\n",
    "    plt.xlabel('Cophenet')\n",
    "    plt.ylabel('Correlation')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cut_clustering_and_gene_mapping(corr_matrix, dist_linkage, threshold, variable_names):\n",
    "    clusters = hierarchy.fcluster(dist_linkage, t=threshold, criterion='distance')\n",
    "\n",
    "    cluster_to_genes = {}\n",
    "    for gene_name, cluster_id in zip(variable_names, clusters):\n",
    "        if cluster_id not in cluster_to_genes:\n",
    "            cluster_to_genes[cluster_id] = []\n",
    "        cluster_to_genes[cluster_id].append(gene_name)\n",
    "\n",
    "    high_average_similarity = {}\n",
    "    high_average_similarity_idx = {}\n",
    "\n",
    "    for cluster_id, gene_list in cluster_to_genes.items():\n",
    "        if len(gene_list) > 1:\n",
    "            # Get the sub-matrix of the correlation matrix for the genes in the cluster\n",
    "            gene_indices = [variable_names.get_loc(g) for g in gene_list]\n",
    "            cluster_corr_matrix = corr_matrix[np.ix_(gene_indices, gene_indices)]\n",
    "\n",
    "            # Calculate the average of the upper triangle (excluding the diagonal)\n",
    "            upper_triangle_indices = np.triu_indices_from(cluster_corr_matrix, k=1)\n",
    "            average_similarity = np.mean(cluster_corr_matrix[upper_triangle_indices])\n",
    "\n",
    "            #if average_similarity >= req_sim:\n",
    "            print(f\"Cluster {cluster_id}: {gene_list}\")\n",
    "            print(f\"  Average Similarity: {average_similarity:.4f}\")\n",
    "            print(\"-\" * 45)\n",
    "            high_average_similarity[cluster_id] = gene_list\n",
    "            high_average_similarity_idx[cluster_id] = gene_indices\n",
    "    return high_average_similarity_idx, high_average_similarity\n",
    "\n",
    "\n",
    "corr_matrix = np.corrcoef(adata.X.T)\n",
    "corr_dist = 1 - corr_matrix\n",
    "dist_linkage = hierarchy.average(corr_dist)\n",
    "df = fit_regression_clusterings(corr_matrix, dist_linkage, threshold=2)\n",
    "plot_regression(df)\n",
    "cluster_mapping, cluster_mapping_genes = cut_clustering_and_gene_mapping(corr_matrix, dist_linkage, 0.75, adata.var_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32eacff",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_to_position_mapper = {}\n",
    "max_un = (len(variables_selected[0])-len(representatives[0]))\n",
    "for i in range(len(gene_names)):\n",
    "    if i < (len(variables_selected[0])-len(representatives[0])):\n",
    "        gene_to_position_mapper[i] = i\n",
    "\n",
    "start_idx = max_un\n",
    "current_idx = max_un\n",
    "for k in cluster_mapping:\n",
    "    for elem in cluster_mapping[k]:\n",
    "        gene_to_position_mapper[current_idx] =start_idx \n",
    "        current_idx = current_idx+1\n",
    "    start_idx = start_idx+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0286723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribution_one_target( \n",
    "        target_gene,\n",
    "        lrp_model,\n",
    "        input_data,\n",
    "        background,\n",
    "        selected_variables,\n",
    "        xai_type='lrp-like',\n",
    "        randomize_background = False):\n",
    "    \n",
    "    attributions_list = []\n",
    "    for m in range(len(lrp_model)):\n",
    "\n",
    "        # Randomize backgorund for each round\n",
    "        if randomize_background:\n",
    "            current_background = background[:, selected_variables[m]]\n",
    "            current_background = shuffle_each_column_independently(current_background)\n",
    "\n",
    "        current_data = input_data[:, selected_variables[m]]\n",
    "        model = lrp_model[m]\n",
    "        #for _ in range(num_iterations):\n",
    "        if xai_type == 'lrp-like':\n",
    "            attribution = model.attribute(current_data, target=target_gene)\n",
    "                \n",
    "        elif xai_type == 'shap-like':\n",
    "            attribution = model.attribute(current_data, baselines = current_background, target = target_gene)\n",
    "\n",
    "        attributions_list.append(attribution.detach().cpu().numpy())\n",
    "    return attributions_list\n",
    "\n",
    "\n",
    "def attribution_one_target_one_model( \n",
    "        target_gene,\n",
    "        lrp_model,\n",
    "        input_data,\n",
    "        background,\n",
    "        selected_variables,\n",
    "        xai_type='lrp-like',\n",
    "        randomize_background = False):\n",
    "    \n",
    "    # Randomize backgorund for each round\n",
    "    if randomize_background:\n",
    "        background = shuffle_each_column_independently(background)\n",
    "\n",
    "    model = lrp_model\n",
    "    #for _ in range(num_iterations):\n",
    "    if xai_type == 'lrp-like':\n",
    "        #print(input_data)\n",
    "        #print(target_gene)\n",
    "        attribution = model.attribute(input_data, target=target_gene)\n",
    "            \n",
    "    elif xai_type == 'shap-like':\n",
    "        attribution = model.attribute(input_data, baselines = background, target = target_gene)\n",
    "\n",
    "    return attribution.detach().cpu().numpy()\n",
    "\n",
    "def get_top_edges_per_cell(grn_adata, top_edges):\n",
    "    \n",
    "    counters = np.zeros((grn_adata.shape))\n",
    "\n",
    "    idex = grn_adata.shape[1]-top_edges\n",
    "    b = np.argpartition(grn_adata, idex, axis=1)[:, idex:]\n",
    "\n",
    "    np.put_along_axis(counters, b, 1, axis=1)\n",
    "    return counters\n",
    "\n",
    "    \n",
    "def assemble_attributions(current_attr, variables_selected, a_shape):\n",
    "    assembled_attributions = np.zeros(a_shape)\n",
    "    column_counter = np.zeros(a_shape[1])\n",
    "    for i in range(len(variables_selected)):\n",
    "        for j in range(len(variables_selected[i])):\n",
    "            assembled_attributions[:, variables_selected[i][j]] += current_attr[i][:, j]\n",
    "            column_counter[variables_selected[i][j]] +=1\n",
    "    for i in range(len(column_counter)):\n",
    "        if column_counter[i] == 0:\n",
    "            column_counter[i] =1\n",
    "    assembled_attributions = assembled_attributions/column_counter\n",
    "    return assembled_attributions, column_counter\n",
    "\n",
    "def wrapper(models, data_train_full_tensor, gene_names, variables_selected, config):\n",
    "\n",
    "    data = data_train_full_tensor.detach().cpu().numpy()\n",
    "    tms = []\n",
    "    name_list = []\n",
    "    target_names = []\n",
    "\n",
    "\n",
    "    for trained_model in models:        \n",
    "        trained_model.forward_mu_only = True\n",
    "        explainer, xai_type = get_explainer(trained_model, config.xai_method, config.raw_attribution)\n",
    "        tms.append(explainer)\n",
    "\n",
    "    attributions = []\n",
    "    all_counter = []\n",
    "    ## ATTRIBUTIONS\n",
    "    for g in tqdm(range(data_train_full_tensor.shape[1])):\n",
    "    #for g in tqdm(range(len(variables_selected[0]))):\n",
    "        current_gene = gene_to_position_mapper[g]\n",
    "        current_attributions = []\n",
    "        counter_list = []\n",
    "        for m in range(len(tms)):\n",
    "\n",
    "            attributions_list = attribution_one_target_one_model(\n",
    "                current_gene,\n",
    "                tms[m], # Select the correct model\n",
    "                data_train_full_tensor[:, variables_selected[m]],\n",
    "                data_train_full_tensor[:, variables_selected[m]],\n",
    "                variables_selected, \n",
    "                xai_type=xai_type,\n",
    "                randomize_background = True)\n",
    "            counters = get_top_edges_per_cell(attributions_list, 100)\n",
    "            counter_list.append(counters)\n",
    "            current_attributions.append(attributions_list)\n",
    "\n",
    "        current_attributions, column_counter = assemble_attributions(current_attributions, variables_selected, data_train_full_tensor.shape)\n",
    "        counter_list, col = assemble_attributions(counter_list, variables_selected, data_train_full_tensor.shape)\n",
    "        attributions.append(current_attributions)\n",
    "        all_counter.append(counter_list)\n",
    "\n",
    "\n",
    "\n",
    "    ## AGGREGATION: REPLACE LIST BY AGGREGATED DATA\n",
    "    # for i in range(len(attributions)):\n",
    "    #     # CURRENTLY MEAN\n",
    "    #     attributions[i] = aggregate_attributions(attributions[i], strategy=config.aggregation_strategy )\n",
    "    \n",
    "\n",
    "    ## PENALIZE:\n",
    "    if config.penalty != 'None':\n",
    "        penalty_matrix = compute_correlation_metric(data, cor_type=config.penalty)\n",
    "        for i in range(len(attributions)):\n",
    "            # CURRENTLY MEAN\n",
    "            attributions[i] = np.dot(attributions[i], (1-penalty_matrix))\n",
    "\n",
    "    \n",
    "    ## CLUSTERING: CLUSTER EACH TARGET INDVIDUALLY\n",
    "    for i in range(len(attributions)):\n",
    " \n",
    "        attributions[i] = ad.AnnData(attributions[i])\n",
    "        print(attributions[i])\n",
    "        sc.pp.scale(attributions[i])\n",
    "        try:\n",
    "            sc.pp.pca(attributions[i],n_comps=50)\n",
    "        except:\n",
    "            try:\n",
    "                sc.pp.pca(attributions[i],n_comps=50 )\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "        sc.pp.neighbors(attributions[i], n_neighbors=15)\n",
    "        sc.tl.leiden(attributions[i], resolution=0.1)\n",
    "\n",
    "        #clusterings[f'T_{gene_names[i]}'] = np.array(attributions[i].obs['leiden'])\n",
    "\n",
    "    #EDGE SELECTION:\n",
    "    for i in range(len(attributions)):\n",
    "        edge_indices = get_edges(attributions[i], use_differential=config.use_differential, percentile=config.percentile)\n",
    "        name_list = name_list + list(gene_names[edge_indices])\n",
    "        target_names = target_names+[gene_names[i]]* len(edge_indices)\n",
    "        attributions[i] = attributions[i][:,edge_indices].X\n",
    "        all_counter[i] = all_counter[i][:, edge_indices]\n",
    "\n",
    "    attributions = np.hstack(attributions)\n",
    "    all_counter = np.hstack(all_counter)\n",
    "\n",
    "    \n",
    "    index_list = [f\"{s}_{t}\" for (s, t) in zip(name_list, target_names)]\n",
    "    cou = pd.DataFrame({'index': index_list, 'source':name_list, 'target':target_names})\n",
    "    cou = cou.set_index('index')\n",
    "\n",
    "    #clusterings = pd.DataFrame(clusterings)\n",
    "\n",
    "    #grn_adata = attribution_to_anndata(attributions, var=cou, obs = clusterings)\n",
    "    grn_adata = attribution_to_anndata(attributions, var=cou)\n",
    "    grn_adata.layers['counter'] = all_counter\n",
    "    return grn_adata\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefd0052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_edges(mapping, top):\n",
    "    expanded = []\n",
    "    for i, edges in top.iterrows():\n",
    "        # Add combinations all all mapped edges in the lists\n",
    "        if edges['source'] in mapping and edges['target'] in mapping:\n",
    "            for se in mapping[edges['source']]:\n",
    "                for st in mapping[edges['target']]:\n",
    "                    expanded.append([se, st])\n",
    "        if edges['source'] in mapping:\n",
    "            for se in mapping[edges['source']]:\n",
    "                expanded.append([se, edges['target']])\n",
    "        if edges['target'] in mapping:\n",
    "            for se in mapping[edges['target']]:\n",
    "                expanded.append([edges['source'], se])\n",
    "\n",
    "    expanded = pd.DataFrame(expanded)\n",
    "    expanded.columns = ['source', 'target']\n",
    "    all_edges = pd.concat([top, expanded])\n",
    "    return all_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_zoo, variables_selected, representatives = create_model_zoo(data_tensor, id_class_dictionary, keys, other_genes, n_models=50, n_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e31bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.raw_attribution = True\n",
    "config.percentile = 55\n",
    "grn_adata = wrapper(model_zoo, data_tensor.cuda(), gene_names, variables_selected,  config)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
