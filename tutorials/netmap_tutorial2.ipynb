{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45190821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data_nfs/og86asub/netmap/netmap-evaluation/')\n",
    "\n",
    "import scanpy as sc\n",
    "import time \n",
    "import os.path as op\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "import scipy.sparse as scs\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "\n",
    "from netmap.src.utils.misc import write_config\n",
    "\n",
    "\n",
    "from netmap.src.utils.data_utils import *\n",
    "from netmap.src.utils.tf_utils import *\n",
    "from netmap.src.utils.netmap_config import NetmapConfig\n",
    "\n",
    "from netmap.src.model.train_model import create_model_zoo\n",
    "from netmap.src.grn.inferrence import inferrence\n",
    "from src.data_simulation.data_simulation_config import DataSimulationConfig\n",
    "from netmap.src.masking.internal import *\n",
    "from netmap.src.masking.external import *\n",
    "\n",
    "def read_config(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d6a549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_data': '/data_nfs/og86asub/netmap/netmap-evaluation/data/simulated_data/config_easy/net_172_54892_net_131_54992_net_158_55084/data.h5ad', 'layer': 'X', 'output_directory': '/data_nfs/og86asub/netmap/netmap-evaluation/results/netmap/config_22/config_easy/net_172_54892_net_131_54992_net_158_55084', 'transcription_factors': '/data_nfs/datasets/SCENIC_DB/tf_lists/allTFs_hg38.txt', 'tf_only': False, 'penalize_error': True, 'adata_filename': 'grn_lrp.h5ad', 'grn': 'grn_lrp.tsv', 'masking_percentage': 0.1, 'masking_value': 0, 'print_every': 100, 'optimizer': 'Adam', 'learning_rate': 0.005, 'epochs': 150, 'overwrite': True, 'n_models': 10, 'n_top_edges': 100, 'test_size': 0.3, 'edge_count': 10000, 'model': 'negbin', 'loss_fn': 'LogCoshLoss', 'xai_method': 'GuidedBackprop', 'top_perc': False, 'percentile': 50, 'penalty': 'None', 'raw_attribution': False, 'aggregation_strategy': 'median', 'use_differential': False}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#config = NetmapConfig.read_yaml(\"/data_nfs/og86asub/netmap/netmap-evaluation/results/configurations/netmap/config/perturb_seq/\")\n",
    "dada = \"/data_nfs/og86asub/netmap/netmap-evaluation/results/configurations/data_simulation/config_easy/net_172_54892_net_131_54992_net_158_55084.config.yaml\"\n",
    "dataset_config = read_config(\"/data_nfs/og86asub/netmap/netmap-evaluation/results/configurations/data_simulation/config_easy/net_172_54892_net_131_54992_net_158_55084.config.yaml\")\n",
    "\n",
    "nets = [pd.read_csv(op.join(\"/data_nfs/og86asub/netmap/netmap-evaluation/data/clustered_network/\", filename), sep='\\t') for filename in dataset_config['edgelist']]\n",
    "common = [pd.read_csv(op.join(\"/data_nfs/og86asub/netmap/netmap-evaluation/data/clustered_network/\", filename), sep='\\t') for filename in dataset_config['common_edges']]\n",
    "\n",
    "    \n",
    "config = NetmapConfig.read_yaml('/data_nfs/og86asub/netmap/netmap-evaluation/results/netmap/config_22/config_easy/net_172_54892_net_131_54992_net_158_55084/config.yaml')\n",
    "dataset_config = DataSimulationConfig.read_yaml(dada)\n",
    "\n",
    "\n",
    "\n",
    "start_total = time.monotonic()\n",
    "\n",
    "## Load config and setup outputs\n",
    "os.makedirs(config.output_directory, exist_ok=True)\n",
    "sc.settings.figdir = config.output_directory\n",
    "config.write_yaml(yaml_file=op.join(config.output_directory, 'config.yaml'))\n",
    "\n",
    "## load data\n",
    "adata = sc.read_h5ad(config.input_data)\n",
    "\n",
    "\n",
    "## Get the data matrix from the CustumAnndata obeject\n",
    "gene_names = np.array(adata.var.index)\n",
    "model_start = time.monotonic()\n",
    "\n",
    "if config.layer == 'counts':\n",
    "    data_tensor = adata.layers['counts']\n",
    "else:\n",
    "    data_tensor = adata.X\n",
    "\n",
    "if scs.issparse(data_tensor):\n",
    "    data_tensor = torch.tensor(data_tensor.todense(), dtype=torch.float32)\n",
    "else:\n",
    "    data_tensor = torch.tensor(data_tensor, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de37d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150, Train Loss: 51.5219, Validation Loss: 33.8597\n",
      "Epoch 1/150, Train Loss: 51.5219\n",
      "Epoch 2/150, Train Loss: 28.5506\n",
      "Epoch 3/150, Train Loss: 17.2941\n",
      "Epoch 4/150, Train Loss: 9.8294\n",
      "Epoch 5/150, Train Loss: 7.6708\n",
      "Epoch 6/150, Train Loss: 7.0387\n",
      "Epoch 7/150, Train Loss: 6.6458\n",
      "Epoch 8/150, Train Loss: 6.4221\n",
      "Epoch 9/150, Train Loss: 6.2168\n",
      "Epoch 10/150, Train Loss: 6.0646\n",
      "Epoch 11/150, Train Loss: 5.9334, Validation Loss: 5.7267\n",
      "Epoch 11/150, Train Loss: 5.9334\n",
      "Epoch 12/150, Train Loss: 5.7994\n",
      "Epoch 13/150, Train Loss: 5.7075\n",
      "Epoch 14/150, Train Loss: 5.5892\n",
      "Epoch 15/150, Train Loss: 5.5081\n",
      "Epoch 16/150, Train Loss: 5.4111\n",
      "Epoch 17/150, Train Loss: 5.3264\n",
      "Epoch 18/150, Train Loss: 5.2373\n",
      "Epoch 19/150, Train Loss: 5.1650\n",
      "Epoch 20/150, Train Loss: 5.1050\n",
      "Epoch 21/150, Train Loss: 5.0300, Validation Loss: 4.8974\n",
      "Epoch 21/150, Train Loss: 5.0300\n",
      "Epoch 22/150, Train Loss: 4.9660\n",
      "Epoch 23/150, Train Loss: 4.9106\n",
      "Epoch 24/150, Train Loss: 4.8453\n",
      "Epoch 25/150, Train Loss: 4.8158\n",
      "Epoch 26/150, Train Loss: 4.7613\n",
      "Epoch 27/150, Train Loss: 4.7084\n",
      "Epoch 28/150, Train Loss: 4.6773\n",
      "Epoch 29/150, Train Loss: 4.6418\n",
      "Epoch 30/150, Train Loss: 4.6026\n",
      "Epoch 31/150, Train Loss: 4.5696, Validation Loss: 4.4567\n",
      "Epoch 31/150, Train Loss: 4.5696\n",
      "Epoch 32/150, Train Loss: 4.5488\n",
      "Epoch 33/150, Train Loss: 4.5266\n",
      "Epoch 34/150, Train Loss: 4.5014\n",
      "Epoch 35/150, Train Loss: 4.4813\n",
      "Epoch 36/150, Train Loss: 4.4530\n",
      "Epoch 37/150, Train Loss: 4.4280\n",
      "Epoch 38/150, Train Loss: 4.4150\n",
      "Epoch 39/150, Train Loss: 4.3927\n",
      "Epoch 40/150, Train Loss: 4.3752\n",
      "Epoch 41/150, Train Loss: 4.3675, Validation Loss: 4.2652\n",
      "Epoch 41/150, Train Loss: 4.3675\n",
      "Epoch 42/150, Train Loss: 4.3512\n",
      "Epoch 43/150, Train Loss: 4.3401\n",
      "Epoch 44/150, Train Loss: 4.3242\n",
      "Epoch 45/150, Train Loss: 4.3095\n",
      "Epoch 46/150, Train Loss: 4.3040\n",
      "Epoch 47/150, Train Loss: 4.2914\n",
      "Epoch 48/150, Train Loss: 4.2748\n",
      "Epoch 49/150, Train Loss: 4.2625\n",
      "Epoch 50/150, Train Loss: 4.2613\n",
      "Epoch 51/150, Train Loss: 4.2495, Validation Loss: 4.1708\n",
      "Epoch 51/150, Train Loss: 4.2495\n",
      "Epoch 52/150, Train Loss: 4.2410\n",
      "Epoch 53/150, Train Loss: 4.2339\n",
      "Epoch 54/150, Train Loss: 4.2368\n",
      "Epoch 55/150, Train Loss: 4.2191\n",
      "Epoch 56/150, Train Loss: 4.2114\n",
      "Epoch 57/150, Train Loss: 4.2034\n",
      "Epoch 58/150, Train Loss: 4.1964\n",
      "Epoch 59/150, Train Loss: 4.1887\n",
      "Epoch 60/150, Train Loss: 4.1842\n",
      "Epoch 61/150, Train Loss: 4.1849, Validation Loss: 4.1150\n",
      "Epoch 61/150, Train Loss: 4.1849\n",
      "Epoch 62/150, Train Loss: 4.1786\n",
      "Epoch 63/150, Train Loss: 4.1698\n",
      "Epoch 64/150, Train Loss: 4.1598\n",
      "Epoch 65/150, Train Loss: 4.1613\n",
      "Epoch 66/150, Train Loss: 4.1494\n",
      "Epoch 67/150, Train Loss: 4.1458\n",
      "Epoch 68/150, Train Loss: 4.1406\n",
      "Epoch 69/150, Train Loss: 4.1372\n",
      "Epoch 70/150, Train Loss: 4.1343\n",
      "Epoch 71/150, Train Loss: 4.1318, Validation Loss: 4.0736\n",
      "Epoch 71/150, Train Loss: 4.1318\n",
      "Epoch 72/150, Train Loss: 4.1265\n",
      "Epoch 73/150, Train Loss: 4.1183\n",
      "Epoch 74/150, Train Loss: 4.1174\n",
      "Epoch 75/150, Train Loss: 4.1109\n",
      "Epoch 76/150, Train Loss: 4.1129\n",
      "Epoch 77/150, Train Loss: 4.1037\n",
      "Epoch 78/150, Train Loss: 4.0980\n",
      "Epoch 79/150, Train Loss: 4.0938\n",
      "Epoch 80/150, Train Loss: 4.0965\n",
      "Epoch 81/150, Train Loss: 4.0946, Validation Loss: 4.0401\n",
      "Epoch 81/150, Train Loss: 4.0946\n",
      "Epoch 82/150, Train Loss: 4.0856\n",
      "Epoch 83/150, Train Loss: 4.0795\n",
      "Epoch 84/150, Train Loss: 4.0770\n",
      "Epoch 85/150, Train Loss: 4.0694\n",
      "Epoch 86/150, Train Loss: 4.0707\n",
      "Epoch 87/150, Train Loss: 4.0695\n",
      "Epoch 88/150, Train Loss: 4.0614\n",
      "Epoch 89/150, Train Loss: 4.0588\n",
      "Epoch 90/150, Train Loss: 4.0532\n",
      "Epoch 91/150, Train Loss: 4.0506, Validation Loss: 4.0042\n",
      "Epoch 91/150, Train Loss: 4.0506\n",
      "Epoch 92/150, Train Loss: 4.0507\n",
      "Epoch 93/150, Train Loss: 4.0428\n",
      "Epoch 94/150, Train Loss: 4.0455\n",
      "Epoch 95/150, Train Loss: 4.0409\n",
      "Epoch 96/150, Train Loss: 4.0317\n",
      "Epoch 97/150, Train Loss: 4.0283\n",
      "Epoch 98/150, Train Loss: 4.0286\n",
      "Epoch 99/150, Train Loss: 4.0236\n",
      "Epoch 100/150, Train Loss: 4.0196\n",
      "Epoch 101/150, Train Loss: 4.0183, Validation Loss: 3.9717\n",
      "Epoch 101/150, Train Loss: 4.0183\n",
      "Epoch 102/150, Train Loss: 4.0171\n",
      "Epoch 103/150, Train Loss: 4.0095\n",
      "Epoch 104/150, Train Loss: 4.0143\n",
      "Epoch 105/150, Train Loss: 4.0060\n",
      "Epoch 106/150, Train Loss: 3.9978\n",
      "Epoch 107/150, Train Loss: 4.0019\n",
      "Epoch 108/150, Train Loss: 3.9904\n",
      "Epoch 109/150, Train Loss: 3.9874\n",
      "Epoch 110/150, Train Loss: 3.9914\n",
      "Epoch 111/150, Train Loss: 3.9877, Validation Loss: 3.9431\n",
      "Epoch 111/150, Train Loss: 3.9877\n",
      "Epoch 112/150, Train Loss: 3.9777\n",
      "Epoch 113/150, Train Loss: 3.9769\n",
      "Epoch 114/150, Train Loss: 3.9748\n",
      "Epoch 115/150, Train Loss: 3.9686\n",
      "Epoch 116/150, Train Loss: 3.9684\n",
      "Epoch 117/150, Train Loss: 3.9630\n",
      "Epoch 118/150, Train Loss: 3.9604\n",
      "Epoch 119/150, Train Loss: 3.9580\n",
      "Epoch 120/150, Train Loss: 3.9531\n",
      "Epoch 121/150, Train Loss: 3.9474, Validation Loss: 3.9051\n",
      "Epoch 121/150, Train Loss: 3.9474\n",
      "Epoch 122/150, Train Loss: 3.9486\n",
      "Epoch 123/150, Train Loss: 3.9439\n",
      "Epoch 124/150, Train Loss: 3.9430\n",
      "Epoch 125/150, Train Loss: 3.9397\n",
      "Epoch 126/150, Train Loss: 3.9366\n",
      "Epoch 127/150, Train Loss: 3.9349\n",
      "Epoch 128/150, Train Loss: 3.9294\n",
      "Epoch 129/150, Train Loss: 3.9271\n",
      "Epoch 130/150, Train Loss: 3.9230\n",
      "Epoch 131/150, Train Loss: 3.9197, Validation Loss: 3.8745\n",
      "Epoch 131/150, Train Loss: 3.9197\n",
      "Epoch 132/150, Train Loss: 3.9167\n",
      "Epoch 133/150, Train Loss: 3.9139\n",
      "Epoch 134/150, Train Loss: 3.9046\n",
      "Epoch 135/150, Train Loss: 3.9048\n",
      "Epoch 136/150, Train Loss: 3.9002\n",
      "Epoch 137/150, Train Loss: 3.8981\n",
      "Epoch 138/150, Train Loss: 3.8978\n",
      "Epoch 139/150, Train Loss: 3.8927\n",
      "Epoch 140/150, Train Loss: 3.8911\n",
      "Epoch 141/150, Train Loss: 3.8894, Validation Loss: 3.8422\n",
      "Epoch 141/150, Train Loss: 3.8894\n",
      "Epoch 142/150, Train Loss: 3.8835\n",
      "Epoch 143/150, Train Loss: 3.8755\n",
      "Epoch 144/150, Train Loss: 3.8741\n",
      "Epoch 145/150, Train Loss: 3.8755\n",
      "Epoch 146/150, Train Loss: 3.8718\n",
      "Epoch 147/150, Train Loss: 3.8734\n",
      "Epoch 148/150, Train Loss: 3.8711\n",
      "Epoch 149/150, Train Loss: 3.8665\n",
      "Epoch 150/150, Train Loss: 3.8616\n",
      "Epoch 1/150, Train Loss: 53.5686, Validation Loss: 34.4480\n",
      "Epoch 1/150, Train Loss: 53.5686\n",
      "Epoch 2/150, Train Loss: 29.4062\n",
      "Epoch 3/150, Train Loss: 19.2611\n",
      "Epoch 4/150, Train Loss: 11.7533\n",
      "Epoch 5/150, Train Loss: 8.3149\n",
      "Epoch 6/150, Train Loss: 7.2822\n",
      "Epoch 7/150, Train Loss: 6.8834\n",
      "Epoch 8/150, Train Loss: 6.6135\n",
      "Epoch 9/150, Train Loss: 6.4249\n",
      "Epoch 10/150, Train Loss: 6.2727\n",
      "Epoch 11/150, Train Loss: 6.1594, Validation Loss: 5.9131\n",
      "Epoch 11/150, Train Loss: 6.1594\n",
      "Epoch 12/150, Train Loss: 6.0416\n",
      "Epoch 13/150, Train Loss: 5.9312\n",
      "Epoch 14/150, Train Loss: 5.8289\n",
      "Epoch 15/150, Train Loss: 5.7499\n",
      "Epoch 16/150, Train Loss: 5.6550\n",
      "Epoch 17/150, Train Loss: 5.5747\n",
      "Epoch 18/150, Train Loss: 5.4971\n",
      "Epoch 19/150, Train Loss: 5.4261\n",
      "Epoch 20/150, Train Loss: 5.3596\n",
      "Epoch 21/150, Train Loss: 5.2837, Validation Loss: 5.1310\n",
      "Epoch 21/150, Train Loss: 5.2837\n",
      "Epoch 22/150, Train Loss: 5.2186\n",
      "Epoch 23/150, Train Loss: 5.1560\n",
      "Epoch 24/150, Train Loss: 5.0896\n",
      "Epoch 25/150, Train Loss: 5.0365\n",
      "Epoch 26/150, Train Loss: 4.9784\n",
      "Epoch 27/150, Train Loss: 4.9247\n",
      "Epoch 28/150, Train Loss: 4.8736\n",
      "Epoch 29/150, Train Loss: 4.8162\n",
      "Epoch 30/150, Train Loss: 4.7738\n",
      "Epoch 31/150, Train Loss: 4.7198, Validation Loss: 4.5899\n",
      "Epoch 31/150, Train Loss: 4.7198\n",
      "Epoch 32/150, Train Loss: 4.6764\n",
      "Epoch 33/150, Train Loss: 4.6365\n",
      "Epoch 34/150, Train Loss: 4.5976\n",
      "Epoch 35/150, Train Loss: 4.5577\n",
      "Epoch 36/150, Train Loss: 4.5190\n",
      "Epoch 37/150, Train Loss: 4.4871\n",
      "Epoch 38/150, Train Loss: 4.4706\n",
      "Epoch 39/150, Train Loss: 4.4396\n",
      "Epoch 40/150, Train Loss: 4.4172\n",
      "Epoch 41/150, Train Loss: 4.3964, Validation Loss: 4.2814\n",
      "Epoch 41/150, Train Loss: 4.3964\n",
      "Epoch 42/150, Train Loss: 4.3859\n",
      "Epoch 43/150, Train Loss: 4.3543\n",
      "Epoch 44/150, Train Loss: 4.3427\n",
      "Epoch 45/150, Train Loss: 4.3229\n",
      "Epoch 46/150, Train Loss: 4.3136\n",
      "Epoch 47/150, Train Loss: 4.2979\n",
      "Epoch 48/150, Train Loss: 4.2855\n",
      "Epoch 49/150, Train Loss: 4.2805\n",
      "Epoch 50/150, Train Loss: 4.2657\n",
      "Epoch 51/150, Train Loss: 4.2606, Validation Loss: 4.1627\n",
      "Epoch 51/150, Train Loss: 4.2606\n",
      "Epoch 52/150, Train Loss: 4.2510\n",
      "Epoch 53/150, Train Loss: 4.2339\n",
      "Epoch 54/150, Train Loss: 4.2306\n",
      "Epoch 55/150, Train Loss: 4.2222\n",
      "Epoch 56/150, Train Loss: 4.2144\n",
      "Epoch 57/150, Train Loss: 4.2046\n",
      "Epoch 58/150, Train Loss: 4.1981\n",
      "Epoch 59/150, Train Loss: 4.1936\n",
      "Epoch 60/150, Train Loss: 4.1881\n",
      "Epoch 61/150, Train Loss: 4.1812, Validation Loss: 4.1033\n",
      "Epoch 61/150, Train Loss: 4.1812\n",
      "Epoch 62/150, Train Loss: 4.1714\n",
      "Epoch 63/150, Train Loss: 4.1700\n",
      "Epoch 64/150, Train Loss: 4.1630\n",
      "Epoch 65/150, Train Loss: 4.1569\n",
      "Epoch 66/150, Train Loss: 4.1531\n",
      "Epoch 67/150, Train Loss: 4.1462\n",
      "Epoch 68/150, Train Loss: 4.1366\n",
      "Epoch 69/150, Train Loss: 4.1341\n",
      "Epoch 70/150, Train Loss: 4.1345\n",
      "Epoch 71/150, Train Loss: 4.1287, Validation Loss: 4.0581\n",
      "Epoch 71/150, Train Loss: 4.1287\n",
      "Epoch 72/150, Train Loss: 4.1218\n",
      "Epoch 73/150, Train Loss: 4.1206\n",
      "Epoch 74/150, Train Loss: 4.1102\n",
      "Epoch 75/150, Train Loss: 4.1066\n",
      "Epoch 76/150, Train Loss: 4.1048\n",
      "Epoch 77/150, Train Loss: 4.0941\n",
      "Epoch 78/150, Train Loss: 4.0906\n",
      "Epoch 79/150, Train Loss: 4.0903\n",
      "Epoch 80/150, Train Loss: 4.0875\n",
      "Epoch 81/150, Train Loss: 4.0826, Validation Loss: 4.0206\n",
      "Epoch 81/150, Train Loss: 4.0826\n",
      "Epoch 82/150, Train Loss: 4.0820\n",
      "Epoch 83/150, Train Loss: 4.0757\n",
      "Epoch 84/150, Train Loss: 4.0673\n",
      "Epoch 85/150, Train Loss: 4.0647\n",
      "Epoch 86/150, Train Loss: 4.0596\n",
      "Epoch 87/150, Train Loss: 4.0566\n",
      "Epoch 88/150, Train Loss: 4.0539\n",
      "Epoch 89/150, Train Loss: 4.0516\n",
      "Epoch 90/150, Train Loss: 4.0478\n",
      "Epoch 91/150, Train Loss: 4.0439, Validation Loss: 3.9890\n",
      "Epoch 91/150, Train Loss: 4.0439\n",
      "Epoch 92/150, Train Loss: 4.0377\n",
      "Epoch 93/150, Train Loss: 4.0356\n",
      "Epoch 94/150, Train Loss: 4.0289\n",
      "Epoch 95/150, Train Loss: 4.0230\n",
      "Epoch 96/150, Train Loss: 4.0257\n",
      "Epoch 97/150, Train Loss: 4.0191\n",
      "Epoch 98/150, Train Loss: 4.0122\n",
      "Epoch 99/150, Train Loss: 4.0102\n",
      "Epoch 100/150, Train Loss: 4.0077\n",
      "Epoch 101/150, Train Loss: 4.0034, Validation Loss: 3.9514\n",
      "Epoch 101/150, Train Loss: 4.0034\n",
      "Epoch 102/150, Train Loss: 3.9982\n",
      "Epoch 103/150, Train Loss: 3.9959\n",
      "Epoch 104/150, Train Loss: 3.9915\n",
      "Epoch 105/150, Train Loss: 3.9885\n",
      "Epoch 106/150, Train Loss: 3.9868\n",
      "Epoch 107/150, Train Loss: 3.9816\n",
      "Epoch 108/150, Train Loss: 3.9765\n",
      "Epoch 109/150, Train Loss: 3.9774\n",
      "Epoch 110/150, Train Loss: 3.9700\n",
      "Epoch 111/150, Train Loss: 3.9658, Validation Loss: 3.9168\n",
      "Epoch 111/150, Train Loss: 3.9658\n",
      "Epoch 112/150, Train Loss: 3.9626\n",
      "Epoch 113/150, Train Loss: 3.9621\n",
      "Epoch 114/150, Train Loss: 3.9578\n",
      "Epoch 115/150, Train Loss: 3.9544\n",
      "Epoch 116/150, Train Loss: 3.9535\n",
      "Epoch 117/150, Train Loss: 3.9446\n",
      "Epoch 118/150, Train Loss: 3.9454\n",
      "Epoch 119/150, Train Loss: 3.9367\n",
      "Epoch 120/150, Train Loss: 3.9375\n",
      "Epoch 121/150, Train Loss: 3.9308, Validation Loss: 3.8843\n",
      "Epoch 121/150, Train Loss: 3.9308\n",
      "Epoch 122/150, Train Loss: 3.9325\n",
      "Epoch 123/150, Train Loss: 3.9281\n",
      "Epoch 124/150, Train Loss: 3.9240\n",
      "Epoch 125/150, Train Loss: 3.9201\n",
      "Epoch 126/150, Train Loss: 3.9160\n",
      "Epoch 127/150, Train Loss: 3.9137\n",
      "Epoch 128/150, Train Loss: 3.9043\n",
      "Epoch 129/150, Train Loss: 3.9056\n",
      "Epoch 130/150, Train Loss: 3.9015\n",
      "Epoch 131/150, Train Loss: 3.8998, Validation Loss: 3.8499\n",
      "Epoch 131/150, Train Loss: 3.8998\n",
      "Epoch 132/150, Train Loss: 3.8948\n",
      "Epoch 133/150, Train Loss: 3.8862\n",
      "Epoch 134/150, Train Loss: 3.8875\n",
      "Epoch 135/150, Train Loss: 3.8842\n",
      "Epoch 136/150, Train Loss: 3.8772\n",
      "Epoch 137/150, Train Loss: 3.8752\n",
      "Epoch 138/150, Train Loss: 3.8730\n",
      "Epoch 139/150, Train Loss: 3.8670\n",
      "Epoch 140/150, Train Loss: 3.8651\n",
      "Epoch 141/150, Train Loss: 3.8645, Validation Loss: 3.8164\n",
      "Epoch 141/150, Train Loss: 3.8645\n",
      "Epoch 142/150, Train Loss: 3.8599\n",
      "Epoch 143/150, Train Loss: 3.8582\n",
      "Epoch 144/150, Train Loss: 3.8586\n",
      "Epoch 145/150, Train Loss: 3.8506\n",
      "Epoch 146/150, Train Loss: 3.8500\n",
      "Epoch 147/150, Train Loss: 3.8471\n",
      "Epoch 148/150, Train Loss: 3.8442\n",
      "Epoch 149/150, Train Loss: 3.8430\n",
      "Epoch 150/150, Train Loss: 3.8378\n",
      "Epoch 1/150, Train Loss: 69.4912, Validation Loss: 37.5449\n",
      "Epoch 1/150, Train Loss: 69.4912\n",
      "Epoch 2/150, Train Loss: 29.6968\n",
      "Epoch 3/150, Train Loss: 17.0434\n",
      "Epoch 4/150, Train Loss: 9.6073\n",
      "Epoch 5/150, Train Loss: 7.6821\n",
      "Epoch 6/150, Train Loss: 7.0482\n",
      "Epoch 7/150, Train Loss: 6.7161\n",
      "Epoch 8/150, Train Loss: 6.4900\n",
      "Epoch 9/150, Train Loss: 6.3236\n",
      "Epoch 10/150, Train Loss: 6.1801\n",
      "Epoch 11/150, Train Loss: 6.0319, Validation Loss: 5.8725\n",
      "Epoch 11/150, Train Loss: 6.0319\n",
      "Epoch 12/150, Train Loss: 5.9149\n",
      "Epoch 13/150, Train Loss: 5.8030\n",
      "Epoch 14/150, Train Loss: 5.7000\n",
      "Epoch 15/150, Train Loss: 5.6022\n",
      "Epoch 16/150, Train Loss: 5.4883\n",
      "Epoch 17/150, Train Loss: 5.3835\n",
      "Epoch 18/150, Train Loss: 5.2881\n",
      "Epoch 19/150, Train Loss: 5.1866\n",
      "Epoch 20/150, Train Loss: 5.0917\n",
      "Epoch 21/150, Train Loss: 5.0043, Validation Loss: 4.8593\n",
      "Epoch 21/150, Train Loss: 5.0043\n",
      "Epoch 22/150, Train Loss: 4.9314\n",
      "Epoch 23/150, Train Loss: 4.8664\n",
      "Epoch 24/150, Train Loss: 4.7863\n",
      "Epoch 25/150, Train Loss: 4.7283\n",
      "Epoch 26/150, Train Loss: 4.6654\n",
      "Epoch 27/150, Train Loss: 4.6194\n",
      "Epoch 28/150, Train Loss: 4.5780\n",
      "Epoch 29/150, Train Loss: 4.5484\n",
      "Epoch 30/150, Train Loss: 4.5174\n",
      "Epoch 31/150, Train Loss: 4.4775, Validation Loss: 4.3409\n",
      "Epoch 31/150, Train Loss: 4.4775\n",
      "Epoch 32/150, Train Loss: 4.4496\n",
      "Epoch 33/150, Train Loss: 4.4223\n",
      "Epoch 34/150, Train Loss: 4.4095\n",
      "Epoch 35/150, Train Loss: 4.3880\n",
      "Epoch 36/150, Train Loss: 4.3611\n",
      "Epoch 37/150, Train Loss: 4.3384\n",
      "Epoch 38/150, Train Loss: 4.3272\n",
      "Epoch 39/150, Train Loss: 4.3145\n",
      "Epoch 40/150, Train Loss: 4.3003\n",
      "Epoch 41/150, Train Loss: 4.2743, Validation Loss: 4.1698\n",
      "Epoch 41/150, Train Loss: 4.2743\n",
      "Epoch 42/150, Train Loss: 4.2680\n",
      "Epoch 43/150, Train Loss: 4.2636\n",
      "Epoch 44/150, Train Loss: 4.2520\n",
      "Epoch 45/150, Train Loss: 4.2289\n",
      "Epoch 46/150, Train Loss: 4.2263\n",
      "Epoch 47/150, Train Loss: 4.2154\n",
      "Epoch 48/150, Train Loss: 4.2089\n",
      "Epoch 49/150, Train Loss: 4.2043\n",
      "Epoch 50/150, Train Loss: 4.1879\n",
      "Epoch 51/150, Train Loss: 4.1791, Validation Loss: 4.0870\n",
      "Epoch 51/150, Train Loss: 4.1791\n",
      "Epoch 52/150, Train Loss: 4.1731\n",
      "Epoch 53/150, Train Loss: 4.1636\n",
      "Epoch 54/150, Train Loss: 4.1604\n",
      "Epoch 55/150, Train Loss: 4.1496\n",
      "Epoch 56/150, Train Loss: 4.1442\n",
      "Epoch 57/150, Train Loss: 4.1449\n",
      "Epoch 58/150, Train Loss: 4.1275\n",
      "Epoch 59/150, Train Loss: 4.1275\n",
      "Epoch 60/150, Train Loss: 4.1167\n",
      "Epoch 61/150, Train Loss: 4.1121, Validation Loss: 4.0316\n",
      "Epoch 61/150, Train Loss: 4.1121\n",
      "Epoch 62/150, Train Loss: 4.1069\n",
      "Epoch 63/150, Train Loss: 4.1005\n",
      "Epoch 64/150, Train Loss: 4.0990\n",
      "Epoch 65/150, Train Loss: 4.0905\n",
      "Epoch 66/150, Train Loss: 4.0850\n",
      "Epoch 67/150, Train Loss: 4.0752\n",
      "Epoch 68/150, Train Loss: 4.0760\n",
      "Epoch 69/150, Train Loss: 4.0656\n",
      "Epoch 70/150, Train Loss: 4.0685\n",
      "Epoch 71/150, Train Loss: 4.0645, Validation Loss: 3.9869\n",
      "Epoch 71/150, Train Loss: 4.0645\n",
      "Epoch 72/150, Train Loss: 4.0492\n",
      "Epoch 73/150, Train Loss: 4.0484\n",
      "Epoch 74/150, Train Loss: 4.0442\n",
      "Epoch 75/150, Train Loss: 4.0322\n",
      "Epoch 76/150, Train Loss: 4.0350\n",
      "Epoch 77/150, Train Loss: 4.0240\n",
      "Epoch 78/150, Train Loss: 4.0221\n",
      "Epoch 79/150, Train Loss: 4.0207\n",
      "Epoch 80/150, Train Loss: 4.0155\n",
      "Epoch 81/150, Train Loss: 4.0102, Validation Loss: 3.9474\n",
      "Epoch 81/150, Train Loss: 4.0102\n",
      "Epoch 82/150, Train Loss: 4.0049\n",
      "Epoch 83/150, Train Loss: 4.0006\n",
      "Epoch 84/150, Train Loss: 3.9977\n",
      "Epoch 85/150, Train Loss: 3.9969\n",
      "Epoch 86/150, Train Loss: 3.9882\n",
      "Epoch 87/150, Train Loss: 3.9853\n",
      "Epoch 88/150, Train Loss: 3.9824\n",
      "Epoch 89/150, Train Loss: 3.9786\n",
      "Epoch 90/150, Train Loss: 3.9703\n",
      "Epoch 91/150, Train Loss: 3.9638, Validation Loss: 3.9104\n",
      "Epoch 91/150, Train Loss: 3.9638\n",
      "Epoch 92/150, Train Loss: 3.9657\n",
      "Epoch 93/150, Train Loss: 3.9629\n",
      "Epoch 94/150, Train Loss: 3.9520\n",
      "Epoch 95/150, Train Loss: 3.9515\n",
      "Epoch 96/150, Train Loss: 3.9518\n",
      "Epoch 97/150, Train Loss: 3.9475\n",
      "Epoch 98/150, Train Loss: 3.9401\n",
      "Epoch 99/150, Train Loss: 3.9403\n",
      "Epoch 100/150, Train Loss: 3.9313\n",
      "Epoch 101/150, Train Loss: 3.9297, Validation Loss: 3.8727\n",
      "Epoch 101/150, Train Loss: 3.9297\n",
      "Epoch 102/150, Train Loss: 3.9238\n",
      "Epoch 103/150, Train Loss: 3.9237\n",
      "Epoch 104/150, Train Loss: 3.9138\n",
      "Epoch 105/150, Train Loss: 3.9087\n",
      "Epoch 106/150, Train Loss: 3.9096\n",
      "Epoch 107/150, Train Loss: 3.9011\n",
      "Epoch 108/150, Train Loss: 3.8957\n",
      "Epoch 109/150, Train Loss: 3.8951\n",
      "Epoch 110/150, Train Loss: 3.8886\n",
      "Epoch 111/150, Train Loss: 3.8852, Validation Loss: 3.8354\n",
      "Epoch 111/150, Train Loss: 3.8852\n",
      "Epoch 112/150, Train Loss: 3.8890\n",
      "Epoch 113/150, Train Loss: 3.8818\n",
      "Epoch 114/150, Train Loss: 3.8784\n",
      "Epoch 115/150, Train Loss: 3.8760\n",
      "Epoch 116/150, Train Loss: 3.8749\n",
      "Epoch 117/150, Train Loss: 3.8696\n",
      "Epoch 118/150, Train Loss: 3.8593\n",
      "Epoch 119/150, Train Loss: 3.8579\n",
      "Epoch 120/150, Train Loss: 3.8537\n",
      "Epoch 121/150, Train Loss: 3.8521, Validation Loss: 3.8003\n",
      "Epoch 121/150, Train Loss: 3.8521\n",
      "Epoch 122/150, Train Loss: 3.8511\n",
      "Epoch 123/150, Train Loss: 3.8465\n",
      "Epoch 124/150, Train Loss: 3.8428\n",
      "Epoch 125/150, Train Loss: 3.8398\n",
      "Epoch 126/150, Train Loss: 3.8373\n",
      "Epoch 127/150, Train Loss: 3.8350\n",
      "Epoch 128/150, Train Loss: 3.8281\n",
      "Epoch 129/150, Train Loss: 3.8327\n",
      "Epoch 130/150, Train Loss: 3.8248\n",
      "Epoch 131/150, Train Loss: 3.8190, Validation Loss: 3.7729\n",
      "Epoch 131/150, Train Loss: 3.8190\n",
      "Epoch 132/150, Train Loss: 3.8166\n",
      "Epoch 133/150, Train Loss: 3.8170\n",
      "Epoch 134/150, Train Loss: 3.8148\n",
      "Epoch 135/150, Train Loss: 3.8096\n",
      "Epoch 136/150, Train Loss: 3.8083\n",
      "Epoch 137/150, Train Loss: 3.8026\n",
      "Epoch 138/150, Train Loss: 3.8050\n",
      "Epoch 139/150, Train Loss: 3.7987\n",
      "Epoch 140/150, Train Loss: 3.7971\n",
      "Epoch 141/150, Train Loss: 3.7973, Validation Loss: 3.7488\n",
      "Epoch 141/150, Train Loss: 3.7973\n",
      "Epoch 142/150, Train Loss: 3.7867\n",
      "Epoch 143/150, Train Loss: 3.7857\n",
      "Epoch 144/150, Train Loss: 3.7829\n",
      "Epoch 145/150, Train Loss: 3.7831\n",
      "Epoch 146/150, Train Loss: 3.7778\n",
      "Epoch 147/150, Train Loss: 3.7772\n",
      "Epoch 148/150, Train Loss: 3.7792\n",
      "Epoch 149/150, Train Loss: 3.7706\n",
      "Epoch 150/150, Train Loss: 3.7701\n",
      "Epoch 1/150, Train Loss: 56.4785, Validation Loss: 34.8829\n",
      "Epoch 1/150, Train Loss: 56.4785\n",
      "Epoch 2/150, Train Loss: 28.1781\n",
      "Epoch 3/150, Train Loss: 17.2899\n",
      "Epoch 4/150, Train Loss: 11.3500\n",
      "Epoch 5/150, Train Loss: 8.9122\n",
      "Epoch 6/150, Train Loss: 7.8195\n",
      "Epoch 7/150, Train Loss: 7.3895\n",
      "Epoch 8/150, Train Loss: 7.1397\n",
      "Epoch 9/150, Train Loss: 6.9625\n",
      "Epoch 10/150, Train Loss: 6.8314\n",
      "Epoch 11/150, Train Loss: 6.6946, Validation Loss: 6.3806\n",
      "Epoch 11/150, Train Loss: 6.6946\n",
      "Epoch 12/150, Train Loss: 6.5660\n",
      "Epoch 13/150, Train Loss: 6.4704\n",
      "Epoch 14/150, Train Loss: 6.3826\n",
      "Epoch 15/150, Train Loss: 6.2966\n",
      "Epoch 16/150, Train Loss: 6.2186\n",
      "Epoch 17/150, Train Loss: 6.1388\n",
      "Epoch 18/150, Train Loss: 6.0436\n",
      "Epoch 19/150, Train Loss: 5.9651\n",
      "Epoch 20/150, Train Loss: 5.9121\n",
      "Epoch 21/150, Train Loss: 5.8209, Validation Loss: 5.6183\n",
      "Epoch 21/150, Train Loss: 5.8209\n",
      "Epoch 22/150, Train Loss: 5.7387\n",
      "Epoch 23/150, Train Loss: 5.6718\n",
      "Epoch 24/150, Train Loss: 5.6181\n",
      "Epoch 25/150, Train Loss: 5.5660\n",
      "Epoch 26/150, Train Loss: 5.4920\n",
      "Epoch 27/150, Train Loss: 5.4193\n",
      "Epoch 28/150, Train Loss: 5.3651\n",
      "Epoch 29/150, Train Loss: 5.3098\n",
      "Epoch 30/150, Train Loss: 5.2441\n",
      "Epoch 31/150, Train Loss: 5.1913, Validation Loss: 5.0097\n",
      "Epoch 31/150, Train Loss: 5.1913\n",
      "Epoch 32/150, Train Loss: 5.1360\n",
      "Epoch 33/150, Train Loss: 5.0879\n",
      "Epoch 34/150, Train Loss: 5.0361\n",
      "Epoch 35/150, Train Loss: 4.9743\n",
      "Epoch 36/150, Train Loss: 4.9486\n",
      "Epoch 37/150, Train Loss: 4.9065\n",
      "Epoch 38/150, Train Loss: 4.8616\n",
      "Epoch 39/150, Train Loss: 4.8202\n",
      "Epoch 40/150, Train Loss: 4.7896\n",
      "Epoch 41/150, Train Loss: 4.7536, Validation Loss: 4.5581\n",
      "Epoch 41/150, Train Loss: 4.7536\n",
      "Epoch 42/150, Train Loss: 4.7110\n",
      "Epoch 43/150, Train Loss: 4.6825\n",
      "Epoch 44/150, Train Loss: 4.6694\n",
      "Epoch 45/150, Train Loss: 4.6241\n",
      "Epoch 46/150, Train Loss: 4.6178\n",
      "Epoch 47/150, Train Loss: 4.5766\n",
      "Epoch 48/150, Train Loss: 4.5718\n",
      "Epoch 49/150, Train Loss: 4.5455\n",
      "Epoch 50/150, Train Loss: 4.5182\n",
      "Epoch 51/150, Train Loss: 4.5069, Validation Loss: 4.3404\n",
      "Epoch 51/150, Train Loss: 4.5069\n",
      "Epoch 52/150, Train Loss: 4.4791\n",
      "Epoch 53/150, Train Loss: 4.4598\n",
      "Epoch 54/150, Train Loss: 4.4541\n",
      "Epoch 55/150, Train Loss: 4.4301\n",
      "Epoch 56/150, Train Loss: 4.4141\n",
      "Epoch 57/150, Train Loss: 4.4059\n",
      "Epoch 58/150, Train Loss: 4.4020\n",
      "Epoch 59/150, Train Loss: 4.3739\n",
      "Epoch 60/150, Train Loss: 4.3779\n",
      "Epoch 61/150, Train Loss: 4.3518, Validation Loss: 4.2282\n",
      "Epoch 61/150, Train Loss: 4.3518\n",
      "Epoch 62/150, Train Loss: 4.3424\n",
      "Epoch 63/150, Train Loss: 4.3318\n",
      "Epoch 64/150, Train Loss: 4.3191\n",
      "Epoch 65/150, Train Loss: 4.3100\n",
      "Epoch 66/150, Train Loss: 4.3009\n",
      "Epoch 67/150, Train Loss: 4.2911\n",
      "Epoch 68/150, Train Loss: 4.2841\n",
      "Epoch 69/150, Train Loss: 4.2687\n",
      "Epoch 70/150, Train Loss: 4.2542\n",
      "Epoch 71/150, Train Loss: 4.2610, Validation Loss: 4.1543\n",
      "Epoch 71/150, Train Loss: 4.2610\n",
      "Epoch 72/150, Train Loss: 4.2467\n",
      "Epoch 73/150, Train Loss: 4.2409\n",
      "Epoch 74/150, Train Loss: 4.2298\n",
      "Epoch 75/150, Train Loss: 4.2178\n",
      "Epoch 76/150, Train Loss: 4.2197\n",
      "Epoch 77/150, Train Loss: 4.2083\n",
      "Epoch 78/150, Train Loss: 4.2011\n",
      "Epoch 79/150, Train Loss: 4.1992\n",
      "Epoch 80/150, Train Loss: 4.1936\n",
      "Epoch 81/150, Train Loss: 4.1864, Validation Loss: 4.1016\n",
      "Epoch 81/150, Train Loss: 4.1864\n",
      "Epoch 82/150, Train Loss: 4.1799\n",
      "Epoch 83/150, Train Loss: 4.1782\n",
      "Epoch 84/150, Train Loss: 4.1699\n",
      "Epoch 85/150, Train Loss: 4.1574\n",
      "Epoch 86/150, Train Loss: 4.1573\n",
      "Epoch 87/150, Train Loss: 4.1501\n",
      "Epoch 88/150, Train Loss: 4.1434\n",
      "Epoch 89/150, Train Loss: 4.1350\n",
      "Epoch 90/150, Train Loss: 4.1274\n",
      "Epoch 91/150, Train Loss: 4.1263, Validation Loss: 4.0577\n",
      "Epoch 91/150, Train Loss: 4.1263\n",
      "Epoch 92/150, Train Loss: 4.1202\n",
      "Epoch 93/150, Train Loss: 4.1205\n",
      "Epoch 94/150, Train Loss: 4.1101\n",
      "Epoch 95/150, Train Loss: 4.1058\n",
      "Epoch 96/150, Train Loss: 4.1063\n",
      "Epoch 97/150, Train Loss: 4.0997\n",
      "Epoch 98/150, Train Loss: 4.0912\n",
      "Epoch 99/150, Train Loss: 4.0880\n",
      "Epoch 100/150, Train Loss: 4.0839\n",
      "Epoch 101/150, Train Loss: 4.0766, Validation Loss: 4.0175\n",
      "Epoch 101/150, Train Loss: 4.0766\n",
      "Epoch 102/150, Train Loss: 4.0693\n",
      "Epoch 103/150, Train Loss: 4.0710\n",
      "Epoch 104/150, Train Loss: 4.0672\n",
      "Epoch 105/150, Train Loss: 4.0646\n",
      "Epoch 106/150, Train Loss: 4.0621\n",
      "Epoch 107/150, Train Loss: 4.0565\n",
      "Epoch 108/150, Train Loss: 4.0444\n",
      "Epoch 109/150, Train Loss: 4.0468\n",
      "Epoch 110/150, Train Loss: 4.0378\n",
      "Epoch 111/150, Train Loss: 4.0334, Validation Loss: 3.9813\n",
      "Epoch 111/150, Train Loss: 4.0334\n",
      "Epoch 112/150, Train Loss: 4.0295\n",
      "Epoch 113/150, Train Loss: 4.0292\n",
      "Epoch 114/150, Train Loss: 4.0247\n",
      "Epoch 115/150, Train Loss: 4.0177\n",
      "Epoch 116/150, Train Loss: 4.0113\n",
      "Epoch 117/150, Train Loss: 4.0106\n",
      "Epoch 118/150, Train Loss: 4.0056\n",
      "Epoch 119/150, Train Loss: 4.0113\n",
      "Epoch 120/150, Train Loss: 3.9966\n",
      "Epoch 121/150, Train Loss: 3.9942, Validation Loss: 3.9450\n",
      "Epoch 121/150, Train Loss: 3.9942\n",
      "Epoch 122/150, Train Loss: 3.9918\n",
      "Epoch 123/150, Train Loss: 3.9860\n",
      "Epoch 124/150, Train Loss: 3.9797\n",
      "Epoch 125/150, Train Loss: 3.9747\n",
      "Epoch 126/150, Train Loss: 3.9755\n",
      "Epoch 127/150, Train Loss: 3.9672\n",
      "Epoch 128/150, Train Loss: 3.9721\n",
      "Epoch 129/150, Train Loss: 3.9590\n",
      "Epoch 130/150, Train Loss: 3.9590\n",
      "Epoch 131/150, Train Loss: 3.9488, Validation Loss: 3.8995\n",
      "Epoch 131/150, Train Loss: 3.9488\n",
      "Epoch 132/150, Train Loss: 3.9462\n",
      "Epoch 133/150, Train Loss: 3.9410\n",
      "Epoch 134/150, Train Loss: 3.9328\n",
      "Epoch 135/150, Train Loss: 3.9336\n",
      "Epoch 136/150, Train Loss: 3.9285\n",
      "Epoch 137/150, Train Loss: 3.9273\n",
      "Epoch 138/150, Train Loss: 3.9290\n",
      "Epoch 139/150, Train Loss: 3.9181\n",
      "Epoch 140/150, Train Loss: 3.9111\n",
      "Epoch 141/150, Train Loss: 3.9113, Validation Loss: 3.8618\n",
      "Epoch 141/150, Train Loss: 3.9113\n",
      "Epoch 142/150, Train Loss: 3.9042\n",
      "Epoch 143/150, Train Loss: 3.9021\n",
      "Epoch 144/150, Train Loss: 3.8957\n",
      "Epoch 145/150, Train Loss: 3.8949\n",
      "Epoch 146/150, Train Loss: 3.8941\n",
      "Epoch 147/150, Train Loss: 3.8909\n",
      "Epoch 148/150, Train Loss: 3.8838\n",
      "Epoch 149/150, Train Loss: 3.8770\n",
      "Epoch 150/150, Train Loss: 3.8764\n",
      "Epoch 1/150, Train Loss: 49.1898, Validation Loss: 34.4600\n",
      "Epoch 1/150, Train Loss: 49.1898\n",
      "Epoch 2/150, Train Loss: 28.9540\n",
      "Epoch 3/150, Train Loss: 17.2114\n",
      "Epoch 4/150, Train Loss: 10.2587\n",
      "Epoch 5/150, Train Loss: 8.1064\n",
      "Epoch 6/150, Train Loss: 7.2933\n",
      "Epoch 7/150, Train Loss: 6.8671\n",
      "Epoch 8/150, Train Loss: 6.5497\n",
      "Epoch 9/150, Train Loss: 6.3169\n",
      "Epoch 10/150, Train Loss: 6.1299\n",
      "Epoch 11/150, Train Loss: 5.9934, Validation Loss: 5.7578\n",
      "Epoch 11/150, Train Loss: 5.9934\n",
      "Epoch 12/150, Train Loss: 5.8702\n",
      "Epoch 13/150, Train Loss: 5.7940\n",
      "Epoch 14/150, Train Loss: 5.6675\n",
      "Epoch 15/150, Train Loss: 5.6124\n",
      "Epoch 16/150, Train Loss: 5.5072\n",
      "Epoch 17/150, Train Loss: 5.4419\n",
      "Epoch 18/150, Train Loss: 5.3820\n",
      "Epoch 19/150, Train Loss: 5.2905\n",
      "Epoch 20/150, Train Loss: 5.2270\n",
      "Epoch 21/150, Train Loss: 5.1738, Validation Loss: 4.9825\n",
      "Epoch 21/150, Train Loss: 5.1738\n",
      "Epoch 22/150, Train Loss: 5.1102\n",
      "Epoch 23/150, Train Loss: 5.0544\n",
      "Epoch 24/150, Train Loss: 4.9877\n",
      "Epoch 25/150, Train Loss: 4.9483\n",
      "Epoch 26/150, Train Loss: 4.9029\n",
      "Epoch 27/150, Train Loss: 4.8562\n",
      "Epoch 28/150, Train Loss: 4.8004\n",
      "Epoch 29/150, Train Loss: 4.7485\n",
      "Epoch 30/150, Train Loss: 4.7071\n",
      "Epoch 31/150, Train Loss: 4.6795, Validation Loss: 4.5022\n",
      "Epoch 31/150, Train Loss: 4.6795\n",
      "Epoch 32/150, Train Loss: 4.6355\n",
      "Epoch 33/150, Train Loss: 4.6094\n",
      "Epoch 34/150, Train Loss: 4.5988\n",
      "Epoch 35/150, Train Loss: 4.5556\n",
      "Epoch 36/150, Train Loss: 4.5257\n",
      "Epoch 37/150, Train Loss: 4.5027\n",
      "Epoch 38/150, Train Loss: 4.4798\n",
      "Epoch 39/150, Train Loss: 4.4695\n",
      "Epoch 40/150, Train Loss: 4.4512\n",
      "Epoch 41/150, Train Loss: 4.4359, Validation Loss: 4.2781\n",
      "Epoch 41/150, Train Loss: 4.4359\n",
      "Epoch 42/150, Train Loss: 4.3995\n",
      "Epoch 43/150, Train Loss: 4.3821\n",
      "Epoch 44/150, Train Loss: 4.3874\n",
      "Epoch 45/150, Train Loss: 4.3700\n",
      "Epoch 46/150, Train Loss: 4.3543\n",
      "Epoch 47/150, Train Loss: 4.3424\n",
      "Epoch 48/150, Train Loss: 4.3352\n",
      "Epoch 49/150, Train Loss: 4.3222\n",
      "Epoch 50/150, Train Loss: 4.3057\n",
      "Epoch 51/150, Train Loss: 4.3062, Validation Loss: 4.1788\n",
      "Epoch 51/150, Train Loss: 4.3062\n",
      "Epoch 52/150, Train Loss: 4.2947\n",
      "Epoch 53/150, Train Loss: 4.2811\n",
      "Epoch 54/150, Train Loss: 4.2677\n",
      "Epoch 55/150, Train Loss: 4.2657\n",
      "Epoch 56/150, Train Loss: 4.2525\n",
      "Epoch 57/150, Train Loss: 4.2415\n",
      "Epoch 58/150, Train Loss: 4.2344\n",
      "Epoch 59/150, Train Loss: 4.2245\n",
      "Epoch 60/150, Train Loss: 4.2174\n",
      "Epoch 61/150, Train Loss: 4.2068, Validation Loss: 4.1125\n",
      "Epoch 61/150, Train Loss: 4.2068\n",
      "Epoch 62/150, Train Loss: 4.2023\n",
      "Epoch 63/150, Train Loss: 4.1974\n",
      "Epoch 64/150, Train Loss: 4.1887\n",
      "Epoch 65/150, Train Loss: 4.1851\n",
      "Epoch 66/150, Train Loss: 4.1714\n",
      "Epoch 67/150, Train Loss: 4.1712\n",
      "Epoch 68/150, Train Loss: 4.1614\n",
      "Epoch 69/150, Train Loss: 4.1556\n",
      "Epoch 70/150, Train Loss: 4.1447\n",
      "Epoch 71/150, Train Loss: 4.1431, Validation Loss: 4.0642\n",
      "Epoch 71/150, Train Loss: 4.1431\n",
      "Epoch 72/150, Train Loss: 4.1429\n",
      "Epoch 73/150, Train Loss: 4.1361\n",
      "Epoch 74/150, Train Loss: 4.1205\n",
      "Epoch 75/150, Train Loss: 4.1214\n",
      "Epoch 76/150, Train Loss: 4.1200\n",
      "Epoch 77/150, Train Loss: 4.1159\n",
      "Epoch 78/150, Train Loss: 4.1078\n",
      "Epoch 79/150, Train Loss: 4.1062\n",
      "Epoch 80/150, Train Loss: 4.0987\n",
      "Epoch 81/150, Train Loss: 4.0947, Validation Loss: 4.0234\n",
      "Epoch 81/150, Train Loss: 4.0947\n",
      "Epoch 82/150, Train Loss: 4.0860\n",
      "Epoch 83/150, Train Loss: 4.0841\n",
      "Epoch 84/150, Train Loss: 4.0685\n",
      "Epoch 85/150, Train Loss: 4.0750\n",
      "Epoch 86/150, Train Loss: 4.0688\n",
      "Epoch 87/150, Train Loss: 4.0643\n",
      "Epoch 88/150, Train Loss: 4.0625\n",
      "Epoch 89/150, Train Loss: 4.0534\n",
      "Epoch 90/150, Train Loss: 4.0504\n",
      "Epoch 91/150, Train Loss: 4.0460, Validation Loss: 3.9849\n",
      "Epoch 91/150, Train Loss: 4.0460\n",
      "Epoch 92/150, Train Loss: 4.0380\n",
      "Epoch 93/150, Train Loss: 4.0356\n",
      "Epoch 94/150, Train Loss: 4.0317\n",
      "Epoch 95/150, Train Loss: 4.0297\n",
      "Epoch 96/150, Train Loss: 4.0236\n",
      "Epoch 97/150, Train Loss: 4.0217\n",
      "Epoch 98/150, Train Loss: 4.0191\n",
      "Epoch 99/150, Train Loss: 4.0110\n",
      "Epoch 100/150, Train Loss: 4.0065\n",
      "Epoch 101/150, Train Loss: 3.9996, Validation Loss: 3.9459\n",
      "Epoch 101/150, Train Loss: 3.9996\n",
      "Epoch 102/150, Train Loss: 3.9959\n",
      "Epoch 103/150, Train Loss: 3.9943\n",
      "Epoch 104/150, Train Loss: 3.9870\n",
      "Epoch 105/150, Train Loss: 3.9877\n",
      "Epoch 106/150, Train Loss: 3.9808\n",
      "Epoch 107/150, Train Loss: 3.9773\n",
      "Epoch 108/150, Train Loss: 3.9742\n",
      "Epoch 109/150, Train Loss: 3.9651\n",
      "Epoch 110/150, Train Loss: 3.9656\n",
      "Epoch 111/150, Train Loss: 3.9625, Validation Loss: 3.9106\n",
      "Epoch 111/150, Train Loss: 3.9625\n",
      "Epoch 112/150, Train Loss: 3.9619\n",
      "Epoch 113/150, Train Loss: 3.9560\n",
      "Epoch 114/150, Train Loss: 3.9510\n",
      "Epoch 115/150, Train Loss: 3.9468\n",
      "Epoch 116/150, Train Loss: 3.9488\n",
      "Epoch 117/150, Train Loss: 3.9401\n",
      "Epoch 118/150, Train Loss: 3.9389\n",
      "Epoch 119/150, Train Loss: 3.9290\n",
      "Epoch 120/150, Train Loss: 3.9275\n",
      "Epoch 121/150, Train Loss: 3.9262, Validation Loss: 3.8773\n",
      "Epoch 121/150, Train Loss: 3.9262\n",
      "Epoch 122/150, Train Loss: 3.9232\n",
      "Epoch 123/150, Train Loss: 3.9194\n",
      "Epoch 124/150, Train Loss: 3.9154\n",
      "Epoch 125/150, Train Loss: 3.9065\n",
      "Epoch 126/150, Train Loss: 3.9044\n",
      "Epoch 127/150, Train Loss: 3.9042\n",
      "Epoch 128/150, Train Loss: 3.8957\n",
      "Epoch 129/150, Train Loss: 3.8931\n",
      "Epoch 130/150, Train Loss: 3.8895\n",
      "Epoch 131/150, Train Loss: 3.8900, Validation Loss: 3.8444\n",
      "Epoch 131/150, Train Loss: 3.8900\n",
      "Epoch 132/150, Train Loss: 3.8839\n",
      "Epoch 133/150, Train Loss: 3.8830\n",
      "Epoch 134/150, Train Loss: 3.8771\n",
      "Epoch 135/150, Train Loss: 3.8721\n",
      "Epoch 136/150, Train Loss: 3.8681\n",
      "Epoch 137/150, Train Loss: 3.8683\n",
      "Epoch 138/150, Train Loss: 3.8616\n",
      "Epoch 139/150, Train Loss: 3.8651\n",
      "Epoch 140/150, Train Loss: 3.8609\n",
      "Epoch 141/150, Train Loss: 3.8556, Validation Loss: 3.8146\n",
      "Epoch 141/150, Train Loss: 3.8556\n",
      "Epoch 142/150, Train Loss: 3.8516\n",
      "Epoch 143/150, Train Loss: 3.8460\n",
      "Epoch 144/150, Train Loss: 3.8466\n",
      "Epoch 145/150, Train Loss: 3.8425\n",
      "Epoch 146/150, Train Loss: 3.8410\n",
      "Epoch 147/150, Train Loss: 3.8395\n",
      "Epoch 148/150, Train Loss: 3.8394\n",
      "Epoch 149/150, Train Loss: 3.8345\n",
      "Epoch 150/150, Train Loss: 3.8293\n",
      "Epoch 1/150, Train Loss: 75.0442, Validation Loss: 39.0573\n",
      "Epoch 1/150, Train Loss: 75.0442\n",
      "Epoch 2/150, Train Loss: 33.5039\n",
      "Epoch 3/150, Train Loss: 19.4557\n",
      "Epoch 4/150, Train Loss: 10.5420\n",
      "Epoch 5/150, Train Loss: 7.9638\n",
      "Epoch 6/150, Train Loss: 7.1919\n",
      "Epoch 7/150, Train Loss: 6.7359\n",
      "Epoch 8/150, Train Loss: 6.5042\n",
      "Epoch 9/150, Train Loss: 6.2966\n",
      "Epoch 10/150, Train Loss: 6.1237\n",
      "Epoch 11/150, Train Loss: 6.0056, Validation Loss: 5.7281\n",
      "Epoch 11/150, Train Loss: 6.0056\n",
      "Epoch 12/150, Train Loss: 5.8736\n",
      "Epoch 13/150, Train Loss: 5.7677\n",
      "Epoch 14/150, Train Loss: 5.6669\n",
      "Epoch 15/150, Train Loss: 5.5802\n",
      "Epoch 16/150, Train Loss: 5.4980\n",
      "Epoch 17/150, Train Loss: 5.4198\n",
      "Epoch 18/150, Train Loss: 5.3490\n",
      "Epoch 19/150, Train Loss: 5.2723\n",
      "Epoch 20/150, Train Loss: 5.2162\n",
      "Epoch 21/150, Train Loss: 5.1328, Validation Loss: 4.9776\n",
      "Epoch 21/150, Train Loss: 5.1328\n",
      "Epoch 22/150, Train Loss: 5.0928\n",
      "Epoch 23/150, Train Loss: 5.0330\n",
      "Epoch 24/150, Train Loss: 4.9832\n",
      "Epoch 25/150, Train Loss: 4.9317\n",
      "Epoch 26/150, Train Loss: 4.8772\n",
      "Epoch 27/150, Train Loss: 4.8282\n",
      "Epoch 28/150, Train Loss: 4.7906\n",
      "Epoch 29/150, Train Loss: 4.7488\n",
      "Epoch 30/150, Train Loss: 4.7230\n",
      "Epoch 31/150, Train Loss: 4.6902, Validation Loss: 4.5313\n",
      "Epoch 31/150, Train Loss: 4.6902\n",
      "Epoch 32/150, Train Loss: 4.6379\n",
      "Epoch 33/150, Train Loss: 4.6464\n",
      "Epoch 34/150, Train Loss: 4.6011\n",
      "Epoch 35/150, Train Loss: 4.5735\n",
      "Epoch 36/150, Train Loss: 4.5472\n",
      "Epoch 37/150, Train Loss: 4.5366\n",
      "Epoch 38/150, Train Loss: 4.5104\n",
      "Epoch 39/150, Train Loss: 4.4855\n",
      "Epoch 40/150, Train Loss: 4.4721\n",
      "Epoch 41/150, Train Loss: 4.4434, Validation Loss: 4.3091\n",
      "Epoch 41/150, Train Loss: 4.4434\n",
      "Epoch 42/150, Train Loss: 4.4414\n",
      "Epoch 43/150, Train Loss: 4.4206\n",
      "Epoch 44/150, Train Loss: 4.4841\n",
      "Epoch 45/150, Train Loss: 4.3974\n",
      "Epoch 46/150, Train Loss: 4.3778\n",
      "Epoch 47/150, Train Loss: 4.3685\n",
      "Epoch 48/150, Train Loss: 4.3475\n",
      "Epoch 49/150, Train Loss: 4.3431\n",
      "Epoch 50/150, Train Loss: 4.3302\n",
      "Epoch 51/150, Train Loss: 4.3279, Validation Loss: 4.1991\n",
      "Epoch 51/150, Train Loss: 4.3279\n",
      "Epoch 52/150, Train Loss: 4.3157\n",
      "Epoch 53/150, Train Loss: 4.3103\n",
      "Epoch 54/150, Train Loss: 4.2974\n",
      "Epoch 55/150, Train Loss: 4.2910\n",
      "Epoch 56/150, Train Loss: 4.2811\n",
      "Epoch 57/150, Train Loss: 4.2694\n",
      "Epoch 58/150, Train Loss: 4.2585\n",
      "Epoch 59/150, Train Loss: 4.2561\n",
      "Epoch 60/150, Train Loss: 4.2527\n",
      "Epoch 61/150, Train Loss: 4.2443, Validation Loss: 4.1323\n",
      "Epoch 61/150, Train Loss: 4.2443\n",
      "Epoch 62/150, Train Loss: 4.2375\n",
      "Epoch 63/150, Train Loss: 4.2260\n",
      "Epoch 64/150, Train Loss: 4.2208\n",
      "Epoch 65/150, Train Loss: 4.2231\n",
      "Epoch 66/150, Train Loss: 4.2068\n",
      "Epoch 67/150, Train Loss: 4.2033\n",
      "Epoch 68/150, Train Loss: 4.1939\n",
      "Epoch 69/150, Train Loss: 4.1944\n",
      "Epoch 70/150, Train Loss: 4.1935\n",
      "Epoch 71/150, Train Loss: 4.1920, Validation Loss: 4.0862\n",
      "Epoch 71/150, Train Loss: 4.1920\n",
      "Epoch 72/150, Train Loss: 4.1766\n",
      "Epoch 73/150, Train Loss: 4.1697\n",
      "Epoch 74/150, Train Loss: 4.1633\n",
      "Epoch 75/150, Train Loss: 4.1594\n",
      "Epoch 76/150, Train Loss: 4.1510\n",
      "Epoch 77/150, Train Loss: 4.1540\n",
      "Epoch 78/150, Train Loss: 4.1498\n",
      "Epoch 79/150, Train Loss: 4.1435\n",
      "Epoch 80/150, Train Loss: 4.1371\n",
      "Epoch 81/150, Train Loss: 4.1363, Validation Loss: 4.0487\n",
      "Epoch 81/150, Train Loss: 4.1363\n",
      "Epoch 82/150, Train Loss: 4.1266\n",
      "Epoch 83/150, Train Loss: 4.1234\n",
      "Epoch 84/150, Train Loss: 4.1224\n",
      "Epoch 85/150, Train Loss: 4.1181\n",
      "Epoch 86/150, Train Loss: 4.1144\n",
      "Epoch 87/150, Train Loss: 4.1096\n",
      "Epoch 88/150, Train Loss: 4.1062\n",
      "Epoch 89/150, Train Loss: 4.0991\n",
      "Epoch 90/150, Train Loss: 4.0962\n",
      "Epoch 91/150, Train Loss: 4.0914, Validation Loss: 4.0135\n",
      "Epoch 91/150, Train Loss: 4.0914\n",
      "Epoch 92/150, Train Loss: 4.0863\n",
      "Epoch 93/150, Train Loss: 4.0841\n",
      "Epoch 94/150, Train Loss: 4.0858\n",
      "Epoch 95/150, Train Loss: 4.0773\n",
      "Epoch 96/150, Train Loss: 4.0753\n",
      "Epoch 97/150, Train Loss: 4.0720\n",
      "Epoch 98/150, Train Loss: 4.0625\n",
      "Epoch 99/150, Train Loss: 4.0570\n",
      "Epoch 100/150, Train Loss: 4.0547\n",
      "Epoch 101/150, Train Loss: 4.0554, Validation Loss: 3.9816\n",
      "Epoch 101/150, Train Loss: 4.0554\n",
      "Epoch 102/150, Train Loss: 4.0478\n",
      "Epoch 103/150, Train Loss: 4.0450\n",
      "Epoch 104/150, Train Loss: 4.0472\n",
      "Epoch 105/150, Train Loss: 4.0425\n",
      "Epoch 106/150, Train Loss: 4.0381\n",
      "Epoch 107/150, Train Loss: 4.0340\n",
      "Epoch 108/150, Train Loss: 4.0258\n",
      "Epoch 109/150, Train Loss: 4.0268\n",
      "Epoch 110/150, Train Loss: 4.0205\n",
      "Epoch 111/150, Train Loss: 4.0178, Validation Loss: 3.9507\n",
      "Epoch 111/150, Train Loss: 4.0178\n",
      "Epoch 112/150, Train Loss: 4.0139\n",
      "Epoch 113/150, Train Loss: 4.0144\n",
      "Epoch 114/150, Train Loss: 4.0069\n",
      "Epoch 115/150, Train Loss: 4.0028\n",
      "Epoch 116/150, Train Loss: 4.0028\n",
      "Epoch 117/150, Train Loss: 3.9983\n",
      "Epoch 118/150, Train Loss: 3.9939\n",
      "Epoch 119/150, Train Loss: 3.9866\n",
      "Epoch 120/150, Train Loss: 3.9900\n",
      "Epoch 121/150, Train Loss: 3.9835, Validation Loss: 3.9195\n",
      "Epoch 121/150, Train Loss: 3.9835\n",
      "Epoch 122/150, Train Loss: 3.9774\n",
      "Epoch 123/150, Train Loss: 3.9763\n",
      "Epoch 124/150, Train Loss: 3.9730\n",
      "Epoch 125/150, Train Loss: 3.9689\n",
      "Epoch 126/150, Train Loss: 3.9689\n",
      "Epoch 127/150, Train Loss: 3.9657\n",
      "Epoch 128/150, Train Loss: 3.9637\n",
      "Epoch 129/150, Train Loss: 3.9599\n",
      "Epoch 130/150, Train Loss: 3.9518\n",
      "Epoch 131/150, Train Loss: 3.9544, Validation Loss: 3.8909\n",
      "Epoch 131/150, Train Loss: 3.9544\n",
      "Epoch 132/150, Train Loss: 3.9501\n",
      "Epoch 133/150, Train Loss: 3.9515\n",
      "Epoch 134/150, Train Loss: 3.9410\n",
      "Epoch 135/150, Train Loss: 3.9386\n",
      "Epoch 136/150, Train Loss: 3.9311\n",
      "Epoch 137/150, Train Loss: 3.9354\n",
      "Epoch 138/150, Train Loss: 3.9271\n",
      "Epoch 139/150, Train Loss: 3.9217\n",
      "Epoch 140/150, Train Loss: 3.9204\n",
      "Epoch 141/150, Train Loss: 3.9223, Validation Loss: 3.8614\n",
      "Epoch 141/150, Train Loss: 3.9223\n",
      "Epoch 142/150, Train Loss: 3.9199\n",
      "Epoch 143/150, Train Loss: 3.9122\n",
      "Epoch 144/150, Train Loss: 3.9114\n",
      "Epoch 145/150, Train Loss: 3.9056\n",
      "Epoch 146/150, Train Loss: 3.9070\n",
      "Epoch 147/150, Train Loss: 3.9019\n",
      "Epoch 148/150, Train Loss: 3.8981\n",
      "Epoch 149/150, Train Loss: 3.8929\n",
      "Epoch 150/150, Train Loss: 3.8939\n",
      "Epoch 1/150, Train Loss: 52.0868, Validation Loss: 36.4559\n",
      "Epoch 1/150, Train Loss: 52.0868\n",
      "Epoch 2/150, Train Loss: 30.6872\n",
      "Epoch 3/150, Train Loss: 19.2871\n",
      "Epoch 4/150, Train Loss: 10.8355\n",
      "Epoch 5/150, Train Loss: 8.1432\n",
      "Epoch 6/150, Train Loss: 7.1920\n",
      "Epoch 7/150, Train Loss: 6.7182\n",
      "Epoch 8/150, Train Loss: 6.4331\n",
      "Epoch 9/150, Train Loss: 6.2561\n",
      "Epoch 10/150, Train Loss: 6.1291\n",
      "Epoch 11/150, Train Loss: 6.0052, Validation Loss: 5.7910\n",
      "Epoch 11/150, Train Loss: 6.0052\n",
      "Epoch 12/150, Train Loss: 5.8944\n",
      "Epoch 13/150, Train Loss: 5.7920\n",
      "Epoch 14/150, Train Loss: 5.7125\n",
      "Epoch 15/150, Train Loss: 5.6466\n",
      "Epoch 16/150, Train Loss: 5.5641\n",
      "Epoch 17/150, Train Loss: 5.4810\n",
      "Epoch 18/150, Train Loss: 5.4136\n",
      "Epoch 19/150, Train Loss: 5.3347\n",
      "Epoch 20/150, Train Loss: 5.2794\n",
      "Epoch 21/150, Train Loss: 5.2152, Validation Loss: 5.0720\n",
      "Epoch 21/150, Train Loss: 5.2152\n",
      "Epoch 22/150, Train Loss: 5.1586\n",
      "Epoch 23/150, Train Loss: 5.1011\n",
      "Epoch 24/150, Train Loss: 5.0437\n",
      "Epoch 25/150, Train Loss: 4.9988\n",
      "Epoch 26/150, Train Loss: 4.9414\n",
      "Epoch 27/150, Train Loss: 4.8932\n",
      "Epoch 28/150, Train Loss: 4.8624\n",
      "Epoch 29/150, Train Loss: 4.8247\n",
      "Epoch 30/150, Train Loss: 4.7759\n",
      "Epoch 31/150, Train Loss: 4.7445, Validation Loss: 4.6074\n",
      "Epoch 31/150, Train Loss: 4.7445\n",
      "Epoch 32/150, Train Loss: 4.7079\n",
      "Epoch 33/150, Train Loss: 4.6768\n",
      "Epoch 34/150, Train Loss: 4.6455\n",
      "Epoch 35/150, Train Loss: 4.6129\n",
      "Epoch 36/150, Train Loss: 4.5889\n",
      "Epoch 37/150, Train Loss: 4.5815\n",
      "Epoch 38/150, Train Loss: 4.5393\n",
      "Epoch 39/150, Train Loss: 4.5303\n",
      "Epoch 40/150, Train Loss: 4.5067\n",
      "Epoch 41/150, Train Loss: 4.4870, Validation Loss: 4.3646\n",
      "Epoch 41/150, Train Loss: 4.4870\n",
      "Epoch 42/150, Train Loss: 4.4571\n",
      "Epoch 43/150, Train Loss: 4.4460\n",
      "Epoch 44/150, Train Loss: 4.4205\n",
      "Epoch 45/150, Train Loss: 4.4123\n",
      "Epoch 46/150, Train Loss: 4.3941\n",
      "Epoch 47/150, Train Loss: 4.3898\n",
      "Epoch 48/150, Train Loss: 4.3673\n",
      "Epoch 49/150, Train Loss: 4.3561\n",
      "Epoch 50/150, Train Loss: 4.3462\n",
      "Epoch 51/150, Train Loss: 4.3348, Validation Loss: 4.2358\n",
      "Epoch 51/150, Train Loss: 4.3348\n",
      "Epoch 52/150, Train Loss: 4.3109\n",
      "Epoch 53/150, Train Loss: 4.2995\n",
      "Epoch 54/150, Train Loss: 4.2952\n",
      "Epoch 55/150, Train Loss: 4.2752\n",
      "Epoch 56/150, Train Loss: 4.2794\n",
      "Epoch 57/150, Train Loss: 4.2719\n",
      "Epoch 58/150, Train Loss: 4.2610\n",
      "Epoch 59/150, Train Loss: 4.2471\n",
      "Epoch 60/150, Train Loss: 4.2403\n",
      "Epoch 61/150, Train Loss: 4.2341, Validation Loss: 4.1550\n",
      "Epoch 61/150, Train Loss: 4.2341\n",
      "Epoch 62/150, Train Loss: 4.2185\n",
      "Epoch 63/150, Train Loss: 4.2055\n",
      "Epoch 64/150, Train Loss: 4.2015\n",
      "Epoch 65/150, Train Loss: 4.2034\n",
      "Epoch 66/150, Train Loss: 4.1969\n",
      "Epoch 67/150, Train Loss: 4.1861\n",
      "Epoch 68/150, Train Loss: 4.1732\n",
      "Epoch 69/150, Train Loss: 4.1712\n",
      "Epoch 70/150, Train Loss: 4.1606\n",
      "Epoch 71/150, Train Loss: 4.1554, Validation Loss: 4.0940\n",
      "Epoch 71/150, Train Loss: 4.1554\n",
      "Epoch 72/150, Train Loss: 4.1502\n",
      "Epoch 73/150, Train Loss: 4.1494\n",
      "Epoch 74/150, Train Loss: 4.1413\n",
      "Epoch 75/150, Train Loss: 4.1298\n",
      "Epoch 76/150, Train Loss: 4.1259\n",
      "Epoch 77/150, Train Loss: 4.1186\n",
      "Epoch 78/150, Train Loss: 4.1074\n",
      "Epoch 79/150, Train Loss: 4.1086\n",
      "Epoch 80/150, Train Loss: 4.1041\n",
      "Epoch 81/150, Train Loss: 4.0970, Validation Loss: 4.0478\n",
      "Epoch 81/150, Train Loss: 4.0970\n",
      "Epoch 82/150, Train Loss: 4.0931\n",
      "Epoch 83/150, Train Loss: 4.0946\n",
      "Epoch 84/150, Train Loss: 4.0789\n",
      "Epoch 85/150, Train Loss: 4.0816\n",
      "Epoch 86/150, Train Loss: 4.0732\n",
      "Epoch 87/150, Train Loss: 4.0697\n",
      "Epoch 88/150, Train Loss: 4.0675\n",
      "Epoch 89/150, Train Loss: 4.0547\n",
      "Epoch 90/150, Train Loss: 4.0517\n",
      "Epoch 91/150, Train Loss: 4.0540, Validation Loss: 4.0064\n",
      "Epoch 91/150, Train Loss: 4.0540\n",
      "Epoch 92/150, Train Loss: 4.0430\n",
      "Epoch 93/150, Train Loss: 4.0339\n",
      "Epoch 94/150, Train Loss: 4.0323\n",
      "Epoch 95/150, Train Loss: 4.0233\n",
      "Epoch 96/150, Train Loss: 4.0244\n",
      "Epoch 97/150, Train Loss: 4.0159\n",
      "Epoch 98/150, Train Loss: 4.0150\n",
      "Epoch 99/150, Train Loss: 4.0154\n",
      "Epoch 100/150, Train Loss: 4.0084\n",
      "Epoch 101/150, Train Loss: 3.9992, Validation Loss: 3.9626\n",
      "Epoch 101/150, Train Loss: 3.9992\n",
      "Epoch 102/150, Train Loss: 3.9961\n",
      "Epoch 103/150, Train Loss: 3.9896\n",
      "Epoch 104/150, Train Loss: 3.9859\n",
      "Epoch 105/150, Train Loss: 3.9834\n",
      "Epoch 106/150, Train Loss: 3.9804\n",
      "Epoch 107/150, Train Loss: 3.9699\n",
      "Epoch 108/150, Train Loss: 3.9666\n",
      "Epoch 109/150, Train Loss: 3.9702\n",
      "Epoch 110/150, Train Loss: 3.9624\n",
      "Epoch 111/150, Train Loss: 3.9586, Validation Loss: 3.9214\n",
      "Epoch 111/150, Train Loss: 3.9586\n",
      "Epoch 112/150, Train Loss: 3.9579\n",
      "Epoch 113/150, Train Loss: 3.9512\n",
      "Epoch 114/150, Train Loss: 3.9481\n",
      "Epoch 115/150, Train Loss: 3.9409\n",
      "Epoch 116/150, Train Loss: 3.9415\n",
      "Epoch 117/150, Train Loss: 3.9346\n",
      "Epoch 118/150, Train Loss: 3.9299\n",
      "Epoch 119/150, Train Loss: 3.9306\n",
      "Epoch 120/150, Train Loss: 3.9208\n",
      "Epoch 121/150, Train Loss: 3.9225, Validation Loss: 3.8893\n",
      "Epoch 121/150, Train Loss: 3.9225\n",
      "Epoch 122/150, Train Loss: 3.9132\n",
      "Epoch 123/150, Train Loss: 3.9103\n",
      "Epoch 124/150, Train Loss: 3.9094\n",
      "Epoch 125/150, Train Loss: 3.9046\n",
      "Epoch 126/150, Train Loss: 3.9037\n",
      "Epoch 127/150, Train Loss: 3.9019\n",
      "Epoch 128/150, Train Loss: 3.8946\n",
      "Epoch 129/150, Train Loss: 3.8887\n",
      "Epoch 130/150, Train Loss: 3.8872\n",
      "Epoch 131/150, Train Loss: 3.8856, Validation Loss: 3.8543\n",
      "Epoch 131/150, Train Loss: 3.8856\n",
      "Epoch 132/150, Train Loss: 3.8851\n",
      "Epoch 133/150, Train Loss: 3.8825\n",
      "Epoch 134/150, Train Loss: 3.8733\n",
      "Epoch 135/150, Train Loss: 3.8750\n",
      "Epoch 136/150, Train Loss: 3.8744\n",
      "Epoch 137/150, Train Loss: 3.8680\n",
      "Epoch 138/150, Train Loss: 3.8650\n",
      "Epoch 139/150, Train Loss: 3.8661\n",
      "Epoch 140/150, Train Loss: 3.8640\n",
      "Epoch 141/150, Train Loss: 3.8519, Validation Loss: 3.8260\n",
      "Epoch 141/150, Train Loss: 3.8519\n",
      "Epoch 142/150, Train Loss: 3.8557\n",
      "Epoch 143/150, Train Loss: 3.8565\n",
      "Epoch 144/150, Train Loss: 3.8521\n",
      "Epoch 145/150, Train Loss: 3.8458\n",
      "Epoch 146/150, Train Loss: 3.8392\n",
      "Epoch 147/150, Train Loss: 3.8454\n",
      "Epoch 148/150, Train Loss: 3.8424\n",
      "Epoch 149/150, Train Loss: 3.8400\n",
      "Epoch 150/150, Train Loss: 3.8396\n",
      "Epoch 1/150, Train Loss: 67.9573, Validation Loss: 35.5784\n",
      "Epoch 1/150, Train Loss: 67.9573\n",
      "Epoch 2/150, Train Loss: 31.3107\n",
      "Epoch 3/150, Train Loss: 18.8944\n",
      "Epoch 4/150, Train Loss: 11.3879\n",
      "Epoch 5/150, Train Loss: 8.5979\n",
      "Epoch 6/150, Train Loss: 7.7049\n",
      "Epoch 7/150, Train Loss: 7.2635\n",
      "Epoch 8/150, Train Loss: 6.9924\n",
      "Epoch 9/150, Train Loss: 6.7651\n",
      "Epoch 10/150, Train Loss: 6.6273\n",
      "Epoch 11/150, Train Loss: 6.5107, Validation Loss: 6.1290\n",
      "Epoch 11/150, Train Loss: 6.5107\n",
      "Epoch 12/150, Train Loss: 6.3603\n",
      "Epoch 13/150, Train Loss: 6.2649\n",
      "Epoch 14/150, Train Loss: 6.1664\n",
      "Epoch 15/150, Train Loss: 6.0405\n",
      "Epoch 16/150, Train Loss: 5.9854\n",
      "Epoch 17/150, Train Loss: 5.8807\n",
      "Epoch 18/150, Train Loss: 5.8099\n",
      "Epoch 19/150, Train Loss: 5.7430\n",
      "Epoch 20/150, Train Loss: 5.6505\n",
      "Epoch 21/150, Train Loss: 5.5944, Validation Loss: 5.3445\n",
      "Epoch 21/150, Train Loss: 5.5944\n",
      "Epoch 22/150, Train Loss: 5.5235\n",
      "Epoch 23/150, Train Loss: 5.4556\n",
      "Epoch 24/150, Train Loss: 5.3981\n",
      "Epoch 25/150, Train Loss: 5.3237\n",
      "Epoch 26/150, Train Loss: 5.2720\n",
      "Epoch 27/150, Train Loss: 5.2170\n",
      "Epoch 28/150, Train Loss: 5.1609\n",
      "Epoch 29/150, Train Loss: 5.1251\n",
      "Epoch 30/150, Train Loss: 5.0682\n",
      "Epoch 31/150, Train Loss: 5.0101, Validation Loss: 4.7991\n",
      "Epoch 31/150, Train Loss: 5.0101\n",
      "Epoch 32/150, Train Loss: 4.9812\n",
      "Epoch 33/150, Train Loss: 4.9319\n",
      "Epoch 34/150, Train Loss: 4.8919\n",
      "Epoch 35/150, Train Loss: 4.8544\n",
      "Epoch 36/150, Train Loss: 4.8205\n",
      "Epoch 37/150, Train Loss: 4.7904\n",
      "Epoch 38/150, Train Loss: 4.7534\n",
      "Epoch 39/150, Train Loss: 4.7309\n",
      "Epoch 40/150, Train Loss: 4.6975\n",
      "Epoch 41/150, Train Loss: 4.6611, Validation Loss: 4.4661\n",
      "Epoch 41/150, Train Loss: 4.6611\n",
      "Epoch 42/150, Train Loss: 4.6391\n",
      "Epoch 43/150, Train Loss: 4.6330\n",
      "Epoch 44/150, Train Loss: 4.5974\n",
      "Epoch 45/150, Train Loss: 4.5735\n",
      "Epoch 46/150, Train Loss: 4.5550\n",
      "Epoch 47/150, Train Loss: 4.5455\n",
      "Epoch 48/150, Train Loss: 4.5130\n",
      "Epoch 49/150, Train Loss: 4.4977\n",
      "Epoch 50/150, Train Loss: 4.4814\n",
      "Epoch 51/150, Train Loss: 4.4622, Validation Loss: 4.2706\n",
      "Epoch 51/150, Train Loss: 4.4622\n",
      "Epoch 52/150, Train Loss: 4.4346\n",
      "Epoch 53/150, Train Loss: 4.4235\n",
      "Epoch 54/150, Train Loss: 4.3974\n",
      "Epoch 55/150, Train Loss: 4.4016\n",
      "Epoch 56/150, Train Loss: 4.3777\n",
      "Epoch 57/150, Train Loss: 4.3708\n",
      "Epoch 58/150, Train Loss: 4.3548\n",
      "Epoch 59/150, Train Loss: 4.3264\n",
      "Epoch 60/150, Train Loss: 4.3289\n",
      "Epoch 61/150, Train Loss: 4.3127, Validation Loss: 4.1521\n",
      "Epoch 61/150, Train Loss: 4.3127\n",
      "Epoch 62/150, Train Loss: 4.3035\n",
      "Epoch 63/150, Train Loss: 4.2976\n",
      "Epoch 64/150, Train Loss: 4.2866\n",
      "Epoch 65/150, Train Loss: 4.2749\n",
      "Epoch 66/150, Train Loss: 4.2680\n",
      "Epoch 67/150, Train Loss: 4.2560\n",
      "Epoch 68/150, Train Loss: 4.2526\n",
      "Epoch 69/150, Train Loss: 4.2407\n",
      "Epoch 70/150, Train Loss: 4.2268\n",
      "Epoch 71/150, Train Loss: 4.2206, Validation Loss: 4.0833\n",
      "Epoch 71/150, Train Loss: 4.2206\n",
      "Epoch 72/150, Train Loss: 4.2230\n",
      "Epoch 73/150, Train Loss: 4.2158\n",
      "Epoch 74/150, Train Loss: 4.2116\n",
      "Epoch 75/150, Train Loss: 4.2017\n",
      "Epoch 76/150, Train Loss: 4.1869\n",
      "Epoch 77/150, Train Loss: 4.1857\n",
      "Epoch 78/150, Train Loss: 4.1849\n",
      "Epoch 79/150, Train Loss: 4.1737\n",
      "Epoch 80/150, Train Loss: 4.1667\n",
      "Epoch 81/150, Train Loss: 4.1568, Validation Loss: 4.0364\n",
      "Epoch 81/150, Train Loss: 4.1568\n",
      "Epoch 82/150, Train Loss: 4.1530\n",
      "Epoch 83/150, Train Loss: 4.1571\n",
      "Epoch 84/150, Train Loss: 4.1529\n",
      "Epoch 85/150, Train Loss: 4.1450\n",
      "Epoch 86/150, Train Loss: 4.1412\n",
      "Epoch 87/150, Train Loss: 4.1273\n",
      "Epoch 88/150, Train Loss: 4.1290\n",
      "Epoch 89/150, Train Loss: 4.1228\n",
      "Epoch 90/150, Train Loss: 4.1173\n",
      "Epoch 91/150, Train Loss: 4.1143, Validation Loss: 4.0002\n",
      "Epoch 91/150, Train Loss: 4.1143\n",
      "Epoch 92/150, Train Loss: 4.1075\n",
      "Epoch 93/150, Train Loss: 4.0997\n",
      "Epoch 94/150, Train Loss: 4.0978\n",
      "Epoch 95/150, Train Loss: 4.0956\n",
      "Epoch 96/150, Train Loss: 4.0941\n",
      "Epoch 97/150, Train Loss: 4.0888\n",
      "Epoch 98/150, Train Loss: 4.0805\n",
      "Epoch 99/150, Train Loss: 4.0749\n",
      "Epoch 100/150, Train Loss: 4.0780\n",
      "Epoch 101/150, Train Loss: 4.0678, Validation Loss: 3.9667\n",
      "Epoch 101/150, Train Loss: 4.0678\n",
      "Epoch 102/150, Train Loss: 4.0636\n",
      "Epoch 103/150, Train Loss: 4.0580\n",
      "Epoch 104/150, Train Loss: 4.0573\n",
      "Epoch 105/150, Train Loss: 4.0472\n",
      "Epoch 106/150, Train Loss: 4.0499\n",
      "Epoch 107/150, Train Loss: 4.0447\n",
      "Epoch 108/150, Train Loss: 4.0410\n",
      "Epoch 109/150, Train Loss: 4.0351\n",
      "Epoch 110/150, Train Loss: 4.0283\n",
      "Epoch 111/150, Train Loss: 4.0361, Validation Loss: 3.9355\n",
      "Epoch 111/150, Train Loss: 4.0361\n",
      "Epoch 112/150, Train Loss: 4.0240\n",
      "Epoch 113/150, Train Loss: 4.0184\n",
      "Epoch 114/150, Train Loss: 4.0120\n",
      "Epoch 115/150, Train Loss: 4.0081\n",
      "Epoch 116/150, Train Loss: 4.0077\n",
      "Epoch 117/150, Train Loss: 4.0095\n",
      "Epoch 118/150, Train Loss: 3.9968\n",
      "Epoch 119/150, Train Loss: 3.9986\n",
      "Epoch 120/150, Train Loss: 3.9869\n",
      "Epoch 121/150, Train Loss: 3.9882, Validation Loss: 3.8977\n",
      "Epoch 121/150, Train Loss: 3.9882\n",
      "Epoch 122/150, Train Loss: 3.9840\n",
      "Epoch 123/150, Train Loss: 3.9739\n",
      "Epoch 124/150, Train Loss: 3.9729\n",
      "Epoch 125/150, Train Loss: 3.9728\n",
      "Epoch 126/150, Train Loss: 3.9676\n",
      "Epoch 127/150, Train Loss: 3.9622\n",
      "Epoch 128/150, Train Loss: 3.9633\n",
      "Epoch 129/150, Train Loss: 3.9581\n",
      "Epoch 130/150, Train Loss: 3.9552\n",
      "Epoch 131/150, Train Loss: 3.9505, Validation Loss: 3.8634\n",
      "Epoch 131/150, Train Loss: 3.9505\n",
      "Epoch 132/150, Train Loss: 3.9505\n",
      "Epoch 133/150, Train Loss: 3.9458\n",
      "Epoch 134/150, Train Loss: 3.9416\n",
      "Epoch 135/150, Train Loss: 3.9356\n",
      "Epoch 136/150, Train Loss: 3.9346\n",
      "Epoch 137/150, Train Loss: 3.9347\n",
      "Epoch 138/150, Train Loss: 3.9220\n",
      "Epoch 139/150, Train Loss: 3.9285\n",
      "Epoch 140/150, Train Loss: 3.9193\n",
      "Epoch 141/150, Train Loss: 3.9170, Validation Loss: 3.8344\n",
      "Epoch 141/150, Train Loss: 3.9170\n",
      "Epoch 142/150, Train Loss: 3.9163\n",
      "Epoch 143/150, Train Loss: 3.9120\n",
      "Epoch 144/150, Train Loss: 3.9126\n",
      "Epoch 145/150, Train Loss: 3.9028\n",
      "Epoch 146/150, Train Loss: 3.9041\n",
      "Epoch 147/150, Train Loss: 3.8982\n",
      "Epoch 148/150, Train Loss: 3.8975\n",
      "Epoch 149/150, Train Loss: 3.8917\n",
      "Epoch 150/150, Train Loss: 3.8909\n",
      "Epoch 1/150, Train Loss: 49.3215, Validation Loss: 29.5630\n",
      "Epoch 1/150, Train Loss: 49.3215\n",
      "Epoch 2/150, Train Loss: 23.8943\n",
      "Epoch 3/150, Train Loss: 12.9671\n",
      "Epoch 4/150, Train Loss: 8.8741\n",
      "Epoch 5/150, Train Loss: 7.6328\n",
      "Epoch 6/150, Train Loss: 7.1599\n",
      "Epoch 7/150, Train Loss: 6.8704\n",
      "Epoch 8/150, Train Loss: 6.6583\n",
      "Epoch 9/150, Train Loss: 6.5131\n",
      "Epoch 10/150, Train Loss: 6.3838\n",
      "Epoch 11/150, Train Loss: 6.2533, Validation Loss: 5.9911\n",
      "Epoch 11/150, Train Loss: 6.2533\n",
      "Epoch 12/150, Train Loss: 6.1330\n",
      "Epoch 13/150, Train Loss: 6.0367\n",
      "Epoch 14/150, Train Loss: 5.9548\n",
      "Epoch 15/150, Train Loss: 5.8586\n",
      "Epoch 16/150, Train Loss: 5.7661\n",
      "Epoch 17/150, Train Loss: 5.6965\n",
      "Epoch 18/150, Train Loss: 5.6071\n",
      "Epoch 19/150, Train Loss: 5.5433\n",
      "Epoch 20/150, Train Loss: 5.4473\n",
      "Epoch 21/150, Train Loss: 5.3760, Validation Loss: 5.1776\n",
      "Epoch 21/150, Train Loss: 5.3760\n",
      "Epoch 22/150, Train Loss: 5.2977\n",
      "Epoch 23/150, Train Loss: 5.2340\n",
      "Epoch 24/150, Train Loss: 5.1652\n",
      "Epoch 25/150, Train Loss: 5.1053\n",
      "Epoch 26/150, Train Loss: 5.0491\n",
      "Epoch 27/150, Train Loss: 4.9859\n",
      "Epoch 28/150, Train Loss: 4.9134\n",
      "Epoch 29/150, Train Loss: 4.8708\n",
      "Epoch 30/150, Train Loss: 4.8468\n",
      "Epoch 31/150, Train Loss: 4.7862, Validation Loss: 4.5842\n",
      "Epoch 31/150, Train Loss: 4.7862\n",
      "Epoch 32/150, Train Loss: 4.7568\n",
      "Epoch 33/150, Train Loss: 4.7009\n",
      "Epoch 34/150, Train Loss: 4.6736\n",
      "Epoch 35/150, Train Loss: 4.6276\n",
      "Epoch 36/150, Train Loss: 4.6114\n",
      "Epoch 37/150, Train Loss: 4.5826\n",
      "Epoch 38/150, Train Loss: 4.5460\n",
      "Epoch 39/150, Train Loss: 4.5176\n",
      "Epoch 40/150, Train Loss: 4.4869\n",
      "Epoch 41/150, Train Loss: 4.4791, Validation Loss: 4.3072\n",
      "Epoch 41/150, Train Loss: 4.4791\n",
      "Epoch 42/150, Train Loss: 4.4589\n",
      "Epoch 43/150, Train Loss: 4.4334\n",
      "Epoch 44/150, Train Loss: 4.4156\n",
      "Epoch 45/150, Train Loss: 4.4003\n",
      "Epoch 46/150, Train Loss: 4.3785\n",
      "Epoch 47/150, Train Loss: 4.3624\n",
      "Epoch 48/150, Train Loss: 4.3517\n",
      "Epoch 49/150, Train Loss: 4.3313\n",
      "Epoch 50/150, Train Loss: 4.3199\n",
      "Epoch 51/150, Train Loss: 4.3019, Validation Loss: 4.1756\n",
      "Epoch 51/150, Train Loss: 4.3019\n",
      "Epoch 52/150, Train Loss: 4.3003\n",
      "Epoch 53/150, Train Loss: 4.2956\n",
      "Epoch 54/150, Train Loss: 4.2818\n",
      "Epoch 55/150, Train Loss: 4.2662\n",
      "Epoch 56/150, Train Loss: 4.2551\n",
      "Epoch 57/150, Train Loss: 4.2554\n",
      "Epoch 58/150, Train Loss: 4.2406\n",
      "Epoch 59/150, Train Loss: 4.2348\n",
      "Epoch 60/150, Train Loss: 4.2252\n",
      "Epoch 61/150, Train Loss: 4.2223, Validation Loss: 4.1054\n",
      "Epoch 61/150, Train Loss: 4.2223\n",
      "Epoch 62/150, Train Loss: 4.2026\n",
      "Epoch 63/150, Train Loss: 4.2059\n",
      "Epoch 64/150, Train Loss: 4.1930\n",
      "Epoch 65/150, Train Loss: 4.1861\n",
      "Epoch 66/150, Train Loss: 4.1792\n",
      "Epoch 67/150, Train Loss: 4.1747\n",
      "Epoch 68/150, Train Loss: 4.1643\n",
      "Epoch 69/150, Train Loss: 4.1563\n",
      "Epoch 70/150, Train Loss: 4.1576\n",
      "Epoch 71/150, Train Loss: 4.1529, Validation Loss: 4.0594\n",
      "Epoch 71/150, Train Loss: 4.1529\n",
      "Epoch 72/150, Train Loss: 4.1417\n",
      "Epoch 73/150, Train Loss: 4.1396\n",
      "Epoch 74/150, Train Loss: 4.1327\n",
      "Epoch 75/150, Train Loss: 4.1347\n",
      "Epoch 76/150, Train Loss: 4.1207\n",
      "Epoch 77/150, Train Loss: 4.1154\n",
      "Epoch 78/150, Train Loss: 4.1135\n",
      "Epoch 79/150, Train Loss: 4.1099\n",
      "Epoch 80/150, Train Loss: 4.1063\n",
      "Epoch 81/150, Train Loss: 4.1028, Validation Loss: 4.0216\n",
      "Epoch 81/150, Train Loss: 4.1028\n",
      "Epoch 82/150, Train Loss: 4.0943\n",
      "Epoch 83/150, Train Loss: 4.0910\n",
      "Epoch 84/150, Train Loss: 4.0839\n",
      "Epoch 85/150, Train Loss: 4.0815\n",
      "Epoch 86/150, Train Loss: 4.0746\n",
      "Epoch 87/150, Train Loss: 4.0734\n",
      "Epoch 88/150, Train Loss: 4.0675\n",
      "Epoch 89/150, Train Loss: 4.0623\n",
      "Epoch 90/150, Train Loss: 4.0585\n",
      "Epoch 91/150, Train Loss: 4.0552, Validation Loss: 3.9843\n",
      "Epoch 91/150, Train Loss: 4.0552\n",
      "Epoch 92/150, Train Loss: 4.0539\n",
      "Epoch 93/150, Train Loss: 4.0516\n",
      "Epoch 94/150, Train Loss: 4.0436\n",
      "Epoch 95/150, Train Loss: 4.0391\n",
      "Epoch 96/150, Train Loss: 4.0347\n",
      "Epoch 97/150, Train Loss: 4.0294\n",
      "Epoch 98/150, Train Loss: 4.0298\n",
      "Epoch 99/150, Train Loss: 4.0252\n",
      "Epoch 100/150, Train Loss: 4.0184\n",
      "Epoch 101/150, Train Loss: 4.0127, Validation Loss: 3.9479\n",
      "Epoch 101/150, Train Loss: 4.0127\n",
      "Epoch 102/150, Train Loss: 4.0101\n",
      "Epoch 103/150, Train Loss: 4.0025\n",
      "Epoch 104/150, Train Loss: 4.0078\n",
      "Epoch 105/150, Train Loss: 4.0006\n",
      "Epoch 106/150, Train Loss: 3.9950\n",
      "Epoch 107/150, Train Loss: 3.9932\n",
      "Epoch 108/150, Train Loss: 3.9899\n",
      "Epoch 109/150, Train Loss: 3.9822\n",
      "Epoch 110/150, Train Loss: 3.9766\n",
      "Epoch 111/150, Train Loss: 3.9768, Validation Loss: 3.9131\n",
      "Epoch 111/150, Train Loss: 3.9768\n",
      "Epoch 112/150, Train Loss: 3.9738\n",
      "Epoch 113/150, Train Loss: 3.9695\n",
      "Epoch 114/150, Train Loss: 3.9644\n",
      "Epoch 115/150, Train Loss: 3.9646\n",
      "Epoch 116/150, Train Loss: 3.9582\n",
      "Epoch 117/150, Train Loss: 3.9514\n",
      "Epoch 118/150, Train Loss: 3.9521\n",
      "Epoch 119/150, Train Loss: 3.9461\n",
      "Epoch 120/150, Train Loss: 3.9480\n",
      "Epoch 121/150, Train Loss: 3.9414, Validation Loss: 3.8769\n",
      "Epoch 121/150, Train Loss: 3.9414\n",
      "Epoch 122/150, Train Loss: 3.9330\n",
      "Epoch 123/150, Train Loss: 3.9295\n",
      "Epoch 124/150, Train Loss: 3.9320\n",
      "Epoch 125/150, Train Loss: 3.9237\n",
      "Epoch 126/150, Train Loss: 3.9216\n",
      "Epoch 127/150, Train Loss: 3.9189\n",
      "Epoch 128/150, Train Loss: 3.9121\n",
      "Epoch 129/150, Train Loss: 3.9125\n",
      "Epoch 130/150, Train Loss: 3.9056\n",
      "Epoch 131/150, Train Loss: 3.9024, Validation Loss: 3.8414\n",
      "Epoch 131/150, Train Loss: 3.9024\n",
      "Epoch 132/150, Train Loss: 3.8941\n",
      "Epoch 133/150, Train Loss: 3.8981\n",
      "Epoch 134/150, Train Loss: 3.8892\n",
      "Epoch 135/150, Train Loss: 3.8886\n",
      "Epoch 136/150, Train Loss: 3.8824\n",
      "Epoch 137/150, Train Loss: 3.8817\n",
      "Epoch 138/150, Train Loss: 3.8798\n",
      "Epoch 139/150, Train Loss: 3.8726\n",
      "Epoch 140/150, Train Loss: 3.8679\n",
      "Epoch 141/150, Train Loss: 3.8694, Validation Loss: 3.8078\n",
      "Epoch 141/150, Train Loss: 3.8694\n",
      "Epoch 142/150, Train Loss: 3.8647\n",
      "Epoch 143/150, Train Loss: 3.8601\n",
      "Epoch 144/150, Train Loss: 3.8616\n",
      "Epoch 145/150, Train Loss: 3.8571\n",
      "Epoch 146/150, Train Loss: 3.8479\n",
      "Epoch 147/150, Train Loss: 3.8455\n",
      "Epoch 148/150, Train Loss: 3.8441\n",
      "Epoch 149/150, Train Loss: 3.8410\n",
      "Epoch 150/150, Train Loss: 3.8437\n",
      "Epoch 1/150, Train Loss: 50.9795, Validation Loss: 34.6013\n",
      "Epoch 1/150, Train Loss: 50.9795\n",
      "Epoch 2/150, Train Loss: 29.1288\n",
      "Epoch 3/150, Train Loss: 18.1909\n",
      "Epoch 4/150, Train Loss: 11.2320\n",
      "Epoch 5/150, Train Loss: 8.6668\n",
      "Epoch 6/150, Train Loss: 7.6561\n",
      "Epoch 7/150, Train Loss: 7.0719\n",
      "Epoch 8/150, Train Loss: 6.7879\n",
      "Epoch 9/150, Train Loss: 6.5331\n",
      "Epoch 10/150, Train Loss: 6.3846\n",
      "Epoch 11/150, Train Loss: 6.2187, Validation Loss: 6.0263\n",
      "Epoch 11/150, Train Loss: 6.2187\n",
      "Epoch 12/150, Train Loss: 6.0936\n",
      "Epoch 13/150, Train Loss: 5.9859\n",
      "Epoch 14/150, Train Loss: 5.8835\n",
      "Epoch 15/150, Train Loss: 5.7830\n",
      "Epoch 16/150, Train Loss: 5.6820\n",
      "Epoch 17/150, Train Loss: 5.6010\n",
      "Epoch 18/150, Train Loss: 5.5209\n",
      "Epoch 19/150, Train Loss: 5.4356\n",
      "Epoch 20/150, Train Loss: 5.3592\n",
      "Epoch 21/150, Train Loss: 5.2988, Validation Loss: 5.1444\n",
      "Epoch 21/150, Train Loss: 5.2988\n",
      "Epoch 22/150, Train Loss: 5.2067\n",
      "Epoch 23/150, Train Loss: 5.1515\n",
      "Epoch 24/150, Train Loss: 5.0760\n",
      "Epoch 25/150, Train Loss: 5.0079\n",
      "Epoch 26/150, Train Loss: 4.9644\n",
      "Epoch 27/150, Train Loss: 4.9043\n",
      "Epoch 28/150, Train Loss: 4.8425\n",
      "Epoch 29/150, Train Loss: 4.7930\n",
      "Epoch 30/150, Train Loss: 4.7544\n",
      "Epoch 31/150, Train Loss: 4.7068, Validation Loss: 4.5543\n",
      "Epoch 31/150, Train Loss: 4.7068\n",
      "Epoch 32/150, Train Loss: 4.6696\n",
      "Epoch 33/150, Train Loss: 4.6374\n",
      "Epoch 34/150, Train Loss: 4.6114\n",
      "Epoch 35/150, Train Loss: 4.5847\n",
      "Epoch 36/150, Train Loss: 4.5317\n",
      "Epoch 37/150, Train Loss: 4.5180\n",
      "Epoch 38/150, Train Loss: 4.4937\n",
      "Epoch 39/150, Train Loss: 4.4801\n",
      "Epoch 40/150, Train Loss: 4.4523\n",
      "Epoch 41/150, Train Loss: 4.4272, Validation Loss: 4.3020\n",
      "Epoch 41/150, Train Loss: 4.4272\n",
      "Epoch 42/150, Train Loss: 4.4164\n",
      "Epoch 43/150, Train Loss: 4.4069\n",
      "Epoch 44/150, Train Loss: 4.3900\n",
      "Epoch 45/150, Train Loss: 4.3617\n",
      "Epoch 46/150, Train Loss: 4.3450\n",
      "Epoch 47/150, Train Loss: 4.3334\n",
      "Epoch 48/150, Train Loss: 4.3356\n",
      "Epoch 49/150, Train Loss: 4.3089\n",
      "Epoch 50/150, Train Loss: 4.3013\n",
      "Epoch 51/150, Train Loss: 4.2932, Validation Loss: 4.1851\n",
      "Epoch 51/150, Train Loss: 4.2932\n",
      "Epoch 52/150, Train Loss: 4.2706\n",
      "Epoch 53/150, Train Loss: 4.2724\n",
      "Epoch 54/150, Train Loss: 4.2645\n",
      "Epoch 55/150, Train Loss: 4.2493\n",
      "Epoch 56/150, Train Loss: 4.2344\n",
      "Epoch 57/150, Train Loss: 4.2361\n",
      "Epoch 58/150, Train Loss: 4.2219\n",
      "Epoch 59/150, Train Loss: 4.2129\n",
      "Epoch 60/150, Train Loss: 4.2039\n",
      "Epoch 61/150, Train Loss: 4.1989, Validation Loss: 4.1115\n",
      "Epoch 61/150, Train Loss: 4.1989\n",
      "Epoch 62/150, Train Loss: 4.1961\n",
      "Epoch 63/150, Train Loss: 4.1808\n",
      "Epoch 64/150, Train Loss: 4.1723\n",
      "Epoch 65/150, Train Loss: 4.1708\n",
      "Epoch 66/150, Train Loss: 4.1637\n",
      "Epoch 67/150, Train Loss: 4.1613\n",
      "Epoch 68/150, Train Loss: 4.1537\n",
      "Epoch 69/150, Train Loss: 4.1510\n",
      "Epoch 70/150, Train Loss: 4.1409\n",
      "Epoch 71/150, Train Loss: 4.1310, Validation Loss: 4.0585\n",
      "Epoch 71/150, Train Loss: 4.1310\n",
      "Epoch 72/150, Train Loss: 4.1298\n",
      "Epoch 73/150, Train Loss: 4.1160\n",
      "Epoch 74/150, Train Loss: 4.1155\n",
      "Epoch 75/150, Train Loss: 4.1101\n",
      "Epoch 76/150, Train Loss: 4.1086\n",
      "Epoch 77/150, Train Loss: 4.1042\n",
      "Epoch 78/150, Train Loss: 4.0949\n",
      "Epoch 79/150, Train Loss: 4.0876\n",
      "Epoch 80/150, Train Loss: 4.0857\n",
      "Epoch 81/150, Train Loss: 4.0790, Validation Loss: 4.0167\n",
      "Epoch 81/150, Train Loss: 4.0790\n",
      "Epoch 82/150, Train Loss: 4.0752\n",
      "Epoch 83/150, Train Loss: 4.0720\n",
      "Epoch 84/150, Train Loss: 4.0658\n",
      "Epoch 85/150, Train Loss: 4.0597\n",
      "Epoch 86/150, Train Loss: 4.0570\n",
      "Epoch 87/150, Train Loss: 4.0526\n",
      "Epoch 88/150, Train Loss: 4.0443\n",
      "Epoch 89/150, Train Loss: 4.0452\n",
      "Epoch 90/150, Train Loss: 4.0419\n",
      "Epoch 91/150, Train Loss: 4.0287, Validation Loss: 3.9724\n",
      "Epoch 91/150, Train Loss: 4.0287\n",
      "Epoch 92/150, Train Loss: 4.0253\n",
      "Epoch 93/150, Train Loss: 4.0250\n",
      "Epoch 94/150, Train Loss: 4.0179\n",
      "Epoch 95/150, Train Loss: 4.0188\n",
      "Epoch 96/150, Train Loss: 4.0125\n",
      "Epoch 97/150, Train Loss: 4.0035\n",
      "Epoch 98/150, Train Loss: 4.0042\n",
      "Epoch 99/150, Train Loss: 3.9988\n",
      "Epoch 100/150, Train Loss: 3.9904\n",
      "Epoch 101/150, Train Loss: 3.9853, Validation Loss: 3.9312\n",
      "Epoch 101/150, Train Loss: 3.9853\n",
      "Epoch 102/150, Train Loss: 3.9802\n",
      "Epoch 103/150, Train Loss: 3.9797\n",
      "Epoch 104/150, Train Loss: 3.9760\n",
      "Epoch 105/150, Train Loss: 3.9692\n",
      "Epoch 106/150, Train Loss: 3.9699\n",
      "Epoch 107/150, Train Loss: 3.9614\n",
      "Epoch 108/150, Train Loss: 3.9591\n",
      "Epoch 109/150, Train Loss: 3.9515\n",
      "Epoch 110/150, Train Loss: 3.9456\n",
      "Epoch 111/150, Train Loss: 3.9400, Validation Loss: 3.8872\n",
      "Epoch 111/150, Train Loss: 3.9400\n",
      "Epoch 112/150, Train Loss: 3.9399\n",
      "Epoch 113/150, Train Loss: 3.9397\n",
      "Epoch 114/150, Train Loss: 3.9257\n",
      "Epoch 115/150, Train Loss: 3.9300\n",
      "Epoch 116/150, Train Loss: 3.9259\n",
      "Epoch 117/150, Train Loss: 3.9214\n",
      "Epoch 118/150, Train Loss: 3.9131\n",
      "Epoch 119/150, Train Loss: 3.9126\n",
      "Epoch 120/150, Train Loss: 3.9079\n",
      "Epoch 121/150, Train Loss: 3.9081, Validation Loss: 3.8557\n",
      "Epoch 121/150, Train Loss: 3.9081\n",
      "Epoch 122/150, Train Loss: 3.9008\n",
      "Epoch 123/150, Train Loss: 3.8979\n",
      "Epoch 124/150, Train Loss: 3.9006\n",
      "Epoch 125/150, Train Loss: 3.8901\n",
      "Epoch 126/150, Train Loss: 3.8916\n",
      "Epoch 127/150, Train Loss: 3.8852\n",
      "Epoch 128/150, Train Loss: 3.8753\n",
      "Epoch 129/150, Train Loss: 3.8718\n",
      "Epoch 130/150, Train Loss: 3.8760\n",
      "Epoch 131/150, Train Loss: 3.8688, Validation Loss: 3.8216\n",
      "Epoch 131/150, Train Loss: 3.8688\n",
      "Epoch 132/150, Train Loss: 3.8643\n",
      "Epoch 133/150, Train Loss: 3.8675\n",
      "Epoch 134/150, Train Loss: 3.8580\n",
      "Epoch 135/150, Train Loss: 3.8548\n",
      "Epoch 136/150, Train Loss: 3.8538\n",
      "Epoch 137/150, Train Loss: 3.8523\n",
      "Epoch 138/150, Train Loss: 3.8397\n",
      "Epoch 139/150, Train Loss: 3.8463\n",
      "Epoch 140/150, Train Loss: 3.8427\n",
      "Epoch 141/150, Train Loss: 3.8368, Validation Loss: 3.7930\n",
      "Epoch 141/150, Train Loss: 3.8368\n",
      "Epoch 142/150, Train Loss: 3.8364\n",
      "Epoch 143/150, Train Loss: 3.8305\n",
      "Epoch 144/150, Train Loss: 3.8275\n",
      "Epoch 145/150, Train Loss: 3.8294\n",
      "Epoch 146/150, Train Loss: 3.8231\n",
      "Epoch 147/150, Train Loss: 3.8228\n",
      "Epoch 148/150, Train Loss: 3.8184\n",
      "Epoch 149/150, Train Loss: 3.8184\n",
      "Epoch 150/150, Train Loss: 3.8153\n"
     ]
    }
   ],
   "source": [
    "model_zoo = create_model_zoo(data_tensor,  n_models=config.n_models, n_epochs=config.epochs, model_type=config.model)\n",
    "\n",
    "grn_adata = inferrence(model_zoo, data_tensor.cuda(), gene_names,  config, use_raw_attribution=False)\n",
    "grn_adata_raw = inferrence(model_zoo, data_tensor.cuda(), gene_names,  config, use_raw_attribution=True)\n",
    "grn_adata.layers['raw_attribution'] = grn_adata_raw.X\n",
    "grn_adata.layers['raw_attribution_quantile_count'] = grn_adata_raw.layers['quantile_count']\n",
    "\n",
    "grn_adata = add_neighbourhood_expression_mask(adata,grn_adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e3b9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 461/461 [02:06<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating anndata\n",
      "Setting vars\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0d98329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pariwise_consistency_1 = find_consistent_pairs(grn_adata[grn_adata.obs['grn'] == 1.0, :].copy(), gene_names)\n",
    "pariwise_consistency_2 = find_consistent_pairs(grn_adata[grn_adata.obs['grn'] == 2.0, :].copy(), gene_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "790f2638",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_consistency_1 = pd.DataFrame(pariwise_consistency_1.items())\n",
    "pairwise_consistency_2 = pd.DataFrame(pariwise_consistency_2.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ed78b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_consistency_1.columns = ['edge', 'spearman']\n",
    "pairwise_consistency_2.columns = ['edge', 'spearman']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f9f70c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edge</th>\n",
       "      <th>spearman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AATF_ABCA1</td>\n",
       "      <td>0.282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABCA1_AATF</td>\n",
       "      <td>0.282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AATF_ABCA12</td>\n",
       "      <td>-0.404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABCA12_AATF</td>\n",
       "      <td>-0.404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AATF_ABCB1</td>\n",
       "      <td>-0.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212055</th>\n",
       "      <td>ZNF692_ZNF513</td>\n",
       "      <td>0.787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212056</th>\n",
       "      <td>ZNF513_ZNF746</td>\n",
       "      <td>-0.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212057</th>\n",
       "      <td>ZNF746_ZNF513</td>\n",
       "      <td>-0.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212058</th>\n",
       "      <td>ZNF692_ZNF746</td>\n",
       "      <td>-0.221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212059</th>\n",
       "      <td>ZNF746_ZNF692</td>\n",
       "      <td>-0.221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212060 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 edge  spearman\n",
       "0          AATF_ABCA1     0.282\n",
       "1          ABCA1_AATF     0.282\n",
       "2         AATF_ABCA12    -0.404\n",
       "3         ABCA12_AATF    -0.404\n",
       "4          AATF_ABCB1    -0.591\n",
       "...               ...       ...\n",
       "212055  ZNF692_ZNF513     0.787\n",
       "212056  ZNF513_ZNF746    -0.230\n",
       "212057  ZNF746_ZNF513    -0.230\n",
       "212058  ZNF692_ZNF746    -0.221\n",
       "212059  ZNF746_ZNF692    -0.221\n",
       "\n",
       "[212060 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_consistency_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c661693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_edges_per_cell(grn_adata, top_edges):\n",
    "    \n",
    "    idex = grn_adata.shape[1]-top_edges\n",
    "    b = np.argpartition(grn_adata.X, idex, axis=1)    # top 3 values from each row\n",
    "    top_idx = b[:,idex:]\n",
    "\n",
    "\n",
    "    scgenerai_var_index_np = np.array(grn_adata.var)\n",
    "    top_edges_per_cell = pd.DataFrame(np.concat(scgenerai_var_index_np[top_idx]))\n",
    "    top_edges_per_cell.columns = ['source', 'target', 'n_cells']\n",
    "    top_edges_per_cell['cell_barcode'] = np.repeat(grn_adata.obs.index, repeats=top_edges)\n",
    "    return top_edges_per_cell\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixi_netmap",
   "language": "python",
   "name": "pixi_netmap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
